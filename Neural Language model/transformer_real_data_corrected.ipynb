{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8527adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a6ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count() >= i+1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "device = try_gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba983f7",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18adf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "#Data path to ne_dedup.txt\n",
    "datapath = '../data/ne_dedup.txt'\n",
    "\n",
    "with open(datapath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "    \n",
    "    #Add space to the right places\n",
    "    text = re.sub(r'\\s*[\\u0964]\\s*', r'\\u0020\\u0964\\u0020', text)\n",
    "    text = re.sub(r'\\s*[\\u003f]\\s*', r'\\u0020\\u003f\\u0020', text)\n",
    "    text = re.sub(r'\\s*[\\u002c]\\s*', r'\\u0020\\u003f\\u0020', text)\n",
    "    text = re.sub(r'\\s*\\n\\s*','\\n', text)\n",
    "    #text = re.sub(r'\\s*[\\u0966-\\u0976]+\\s*','\\u0020[\\u0966-\\u0976]\\u0020', text)\n",
    "    #text = re.sub(r'\\s+?\\s+', r'\\u0020?\\u0020', text)\n",
    "    text = re.sub(r'[^\\u0900-\\u097F,?\\s+]','', text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2ec7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?नमस्ते,'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = re.sub(r'[^\\u0900-\\u097F,?]','','?नमस्तेnishant,')\n",
    "#c = re.sub(r'\\s*[\\u002c]\\s*','\\u0020\\u002c\\u0020','?नमस्तेnishant,')\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8492ecaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x966'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(ord('०'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b02d3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341961"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_iter_first = text.split('\\n')[:30000]\n",
    "train_iter_second = text.split('\\n')[:35000]\n",
    "test_iter = text.split('\\n')[35000:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f050e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['बर्दिबास नगरपालिकाको तेस्रो नगर परिषदबाट पारित आव२०७३ । ७४ को संशोधित र २०७४ । ७५ को प्रस्तावित नीति ? कार्यक्रम तथा बजेट',\n",
       " 'अार्थिक वर्ष २०७५७६ काे नदिजन्य पदार्थकाे उत्खनन् गरी बिक्रि वितरण तथा अान्तरिक निकासी गर्ने कार्यकाे बाेलपत्र सम्बन्धी सुचना',\n",
       " 'सक्षार सप्तरी अभियानमा सप्तरीबासी सम्पूर्ण सरोकारवालाहरुको सहयोग र सहभागिताकाो लागि अनुराोध छ ।  सामुदायिक अध्ययन केन्द्रहरूको नविकरण सम्बन्धमा । ',\n",
       " 'काठमाडौं ? १२ कातिक । राष्ट्रपति विद्यादेवी भण्डारी मित्रराष्ट्र कतारको चार दिवसीय औपचारिक भ्रमणमा आज त्यसतर्फ प्रस्थान गरेकी छन् । राष्ट्रपति विद्यादेवी भण्डारी कतारका अमिर शेख हमाद बीन खालिदा अल थानीको मैत्रीपूर्ण निमन्त्रणामा चार दिवसीय औपचारिक',\n",
       " 'काठमाडौँ ? २६ कात्तिक । सरकारले सङ्घ ? प्रदेश र स्थानीय तहमा कर्मचारी समायोजन गर्नका लागि कर्मचारी समायोजन अध्यादेश२०७५ ल्याउने तयारी गरेको छ । सरकारले यसअघि ल्याएको',\n",
       " 'काठमाडौं ? २६ कातिक । महानायक राजेश हमाल अहिले चलचित्र क्षेत्रमा पातलिए पनि उनको सिने जगतमा नामै काफी छ । कुनै समय बलिउड सुपरस्टार अमिताभ वच्चनसँग',\n",
       " 'काठमाडौं ? २६ कातिक । यमनको प्रमुख शहर होडेडामा सरकार समर्थक र विद्रोहीबीच भएको पछिल्लो युद्धमा एक सय ४९ जनाको मृत्यु भएको चिकित्सक र सैनिक स्रोतले',\n",
       " 'काठमाडौं ? २६ कातिक । निजी क्षेत्रबाट निर्माण भएको पहिलो चलचित्र माइतीघरका छाँयाकार गेहेन्द्रप्रसाद धिमालको ७२',\n",
       " 'नेपाल कम्युनिष्ट पार्टी नेकपाको एकीकरण प्रक्रिया जारी छ । तर ? ३ जेठ २०७५ मा पार्टी एकताको घोषणा हुदै गर्दा जुन कार्यतालिका बनाइएको थियो । त्यो',\n",
       " 'सुर्खेत ? १ कात्तिक । गत शुक्रबार आठबीस नगरपालिका१ डाव दैलेखका कर्णबहादुर रावत र उनकी श्रीमतीलाई चिया चाउचाउ नास्ता खुवाउन भ्याइ नभ्याइ थियो । खासै यसअघि दुईचार स्थानीय ग्राहक आउने त्यस चिया पसलमा उक्त दिनको भीडले भने रावत दम्पती निकै दङ्ग थिए । त्यहाँ कर्णाली प्रदेश सरकार ? आन्तरिक तथा विदेशी पाहुना पुगेका थिए । आजभोलि रावतझैँ',\n",
       " 'काठमाडौं ? २२ कात्तिक । सशस्त्र प्रहरी बलको मुख्यालयमा मंगलबार कुकुर तिहारका दिन देउसीभैली कार्यक्रम गरिएको थियो । सशस्त्र प्रहरी परिवार महिला संघको ब्यानरमा गरिएको उक्त दिनको देउसी भैली कार्यक्रम रमाइलोका लागि र स्वेच्छिक रुपमा दान दिने उद्देश्यले गरिएको नभई जबर्जस्ती असुलीका लागि गरिएको सन्देश गएको',\n",
       " 'काठमाडौं ? २६ कातिक । नेपाल स्टक एक्सचेञ्जले कारोवार रकम तलमाथि परेको भन्दै आएको गुनासो हल्ला मात्रै भएको जनाएको छ । कारोवार रकमलगायतका विषयमा अन्योलता बढेपछि केही लगानीकर्ताले एउटा दलालको देखिरहेको मूल्य र अर्को दलालमा देखिरहेको',\n",
       " 'काठमाडौं ? २२ कात्तिक । सशस्त्र प्रहरी बलको मुख्यालयमा मंगलबार कुकुर तिहारका दिन देउसीभैली कार्यक्रम गरिएको थियो । सशस्त्र प्रहरी परिवार महिला संघको ब्यानरमा गरिएको उक्त दिनको देउसी भैली कार्यक्रम रमाइलोका लागि र स्वेच्छिक रुपमा दान दिने उद्देश्यले गरिएको नभई जबर्जस्ती असुलीका लागि गरिएको सन्देश गएको छ । पदको आडमा रकम उठाउन गरिएको थियो भन्ने कुरा',\n",
       " 'पेरिस ? २५ कार्तिक । पहिलो विश्वयुद्ध समाप्तिको एक सय वर्ष अवसर पारेर फ्रान्सको पेरिसमा विशेष कार्यक्रम आयोजना गरिएको छ । प्रथम विश्वयुद्ध अन्त्य भएको सम्झौतामा हस्ताक्षर गरिएको एकसय वर्ष पुगेको सम्झनामा पेरिसमा आयोजित एउटा समारोहमा विश्वका',\n",
       " 'हङकङ  चीनले संसारलाई चकित तुल्याउने गरी छोटो समयमै विश्वकै लामो समुन्द्री पुलको निर्माण कार्य सम्पन्न गरेको छ । हङकङमकाउचुहाई जोड्न ७ किलोमिटर सामुद्रिक सुरङसहित ५५ किलोमिटर लामो सामुद्रिक पुल चीनले निर्माण गरेर उद्घाटन गरेको हो । तस्बिरहरु ऐजेन्सी चिनियाँ सहर जुहाईको हुँदै मकाउ जोड्ने',\n",
       " 'काठमाडौं ? १३ कात्तिक । नेपाल एयर होस्टेस एकाडमीद्धारा सञ्चालित एयर होस्टेस तालिम लिएका विद्यार्थी दीक्षित भएका छन् । मकवानपुरमा एक कार्यक्रमको विच मंगलवार १८ जना एयरहोस्टेस दीक्षित भए । तालिम लिएका युवायुवतीहरुलाई उक्त एयरहोस्टेस एकाडमीका शिक्षकहरु नितामनि',\n",
       " 'दमौली । तनँहुको पर्यटकीय नगरी बन्दीपुरमा कृत्रिम ताल बनाइने भएको छ । पहाडकी रानी नामले परिचित बन्दीपुरको पर्यटन प्रवद्र्धन गर्न मानवनिर्मित ताल बनाउन लागिएको हो । सोमबार प्रसिद्ध अर्थशास्त्री कार्लो कोटेरेलीलाई नयाँ सरकारको नेतृत्व गर्न आग्रह गर्दै तत्काल यहाँको मन्त्रीमण्डल विस्तार गर्ने जनादेश प्रदान गरेका छन् राष्ट्रपतिको कार्यालयबाट जारी एक विज्ञप्तिमा जनाइएको छ । प्रधानमन्त्रीमा आफ्नो नाम प्रस्तावित भएपछि कोटेरेलीले देशवासीलाई सम्बोधन गर्दै भने ? यदि मेरो नेतृत्वको सरकारले संसदबाट विश्वासको मत पाएमा सार्वजनिक वित्त सुधारका लागि सावधानीपूर्वक नेतृत्व प्रदान गर्नेछ । ',\n",
       " 'उहाँले कुशल नेतृत्व प्रदान गर्दै आगामी वर्ष सन् २०१९ को बजेटलाई अनुमोदन गराउँदै इटालीलाई यूरो समूहमा नै रहने सुनिश्चितता गर्ने पनि सम्बोधनमा बताए । लण्डन । उपसभामुख डा शिवमाया तुम्बाहाङ्फेले लण्डनमा सम्पन्न विश्वभरका महिला सांसदहरुको एक अन्तर्राष्ट्रिय सम्मेलनलाई सम्बोधन गरेकी छन् । आइतबार लण्डनको फेल्थाममा १६ औँ राष्ट्रिय विभुती ? समाज सुधारक महागुरु फल्गुनन्दको १३४ औँ जन्म जयन्ती मनाइएको छ । हनोई । भियतनामले यसै वर्षको अन्त्यदेखि लागू हुने बहुचर्चित प्रशान्त व्यापार सम्झौतालाई अनुमोदन गरेको छ । अमेरिकी राष्ट्रपति डोनाल्ड ट्रम्पले यो सम्झौताबाट अलग हुने घोषणा गरे पनि सम्झौता अनुमोदन गर्ने',\n",
       " 'काठमाडौँ । उमेर पुगेका हरेक व्यक्तिलाई यौनको चाहाना हुनु स्वाभाविक नै मानिन्छ तर यदि कोहि महिला वा पुरुषलाई यौन चाहानामा गडबढी हुनु पनि सामान्य हुँदै गएको छ । धेरै विद्यार्थीहरुको मनमा ग्य्राजुएट गरेपछि पढु कि नपढु भन्ने दुविधा हुन्छ । यो विषय पढौं कि त्यो विषय पढौं भन्ने प्रश्न पनि धेरै ग्य्राजुएटहरुको दिमागमा खेलिरहेको हुन्छ । यो फाइल   बाट हो र अन्य परियोजनाहरू द्वारा पनि प्रयोग गर्न सकिन्छ । त्यहाँ नेर यसको फाइल विवरण पृष्ठमा रहेको विवरण तल दिइएको छ । उजुरी कर्ताहरुलाई आइतबार सिंहदरबार वोलाइएको छ । उजुरीका लागि दिइएको १० दिन मध्ये वुधवार अन्तिम समयसम्म मिश्रलाई अनुमोदन गर्न नहुने दाबीसहित चारवटा उजुरी दर्ता भएको थियो । मिश्र बिरुद्ध परेका चार वटा उजुरी मध्ये एक जना उजुरीकर्ताले उजुरी फिर्ता लिएको समितीले जनाएको छ । मिश्रविरुद्ध ऐनसेलको मुद्दामा राज्यलाई करोडौं घाटा हुने गरी पैसा विदेश लैजान अन्तरिम आदेश दिएको ? सूर्य नेपालको मुद्दामा कर छली गर्न आदेश मार्फत सहयोग गरेको ? हाँडीगाउँको सार्वजनिक जग्गा व्यक्तिका नाउमा नामसारी गर्न बाटो खोलिदिएको ? विभिन्न मुद्दामा पैसा लिएर अभियुक्तलाई छाडेको लगायतका उजुरी परेका छन् । प्रधानन्यायाधीशमा प्रस्तावित दीपकराज जोशीलाई संसदीय सुनुवाई विशेष समितिले अस्विकृत गरेपछि संवैधानिक परिषद्ले सर्वोच्च अदालतका वरिष्ठ न्यायाधीश मिश्रलाई प्रधानन्यायाधीशमा सिफारिस गरेको हो । यसैबीच समितिको बैठकले संवैधानिक परिषद्बाट अख्तियार दुरुपयोग अनुसन्धान आयोगको प्रमुख आयुक्तमा सिफारिस भएका नविनकुमार घिमिरेमाथि सार्वजनिक उजुरीको आव्हान गरिएको छ । आज शनिवारदेखि लागु हुने गरी १० दिनभित्र उजुरी दर्ता गराउन सार्वजनिक सूचना प्रकाशित गर्ने निर्णय वैठकले गरेको छ । भदौ २० गते बुधबार वसेको संवैधानिक परिषदको बैठकले कार्यवाहक प्रमुख आयुक्त घिमिरेलाई प्रमुख आयुक्तमा सिफारिस गर्ने निर्णय गरेको थियो । घिमिरे २०७१ सालमा अख्तियारको आयुक्तमा नियुक्त हुनुभएको थियो ।          ?                   ',\n",
       " 'उक्त तरकारी बेच्न उनी दैनिक ६० किलोमिटर यात्रा गर्छन् । बिक्री गर्ने तरकारी अघिल्लो दिन नै तयार गरेर बुढा बिहान झिसमिसेमै हिँड्छन् । उज्यालो हुँदासम्म महेन्द्रनगर पुग्छन् । दैनिक एकडेढ क्विन्टल तरकारी महेन्द्रनगर पुर्याउँछु बुढाले भने ? महेन्द्रनगरमा तरकारीको माग पनि बढी छ ? मूल्य पनि राम्रो पाइन्छ ।  महाकाली नदीमा निर्मित झोलुंगे पुल भएर महेन्द्रनगर आउने गरेका छन् । कुतियाकभरदेखि महेन्द्रनगरसम्मको दूरी ३० किलोमिटरभन्दा बढी छ । कञ्चनपुर  कञ्चनपुरको महाकाली नगरपालिका १० कुतियाकभरमा खड्के बुढा तरकारी खेतीका लागि चिनिएका किसान हुन् । उनको बारीमा अहिले पनि मुला ? परबल ? भण्टा ? तरुल लगायतको तरकारी छ । कुतियाकभर तरकारी खेतीका लागि चिनिएको क्षेत्र हो । कुतियाकभरसँगै जोडिएको भारतको बंगाली बस्तीभरि तरकारी खेती गरिने भएकाले त्यहींको सिको नेपालीले पनि गरेका छन् । सिजन अनुसारका तरकारी लगाएर दोधारा चांँदनी र महेन्द्रनगरसम्म पुर्याउने गरिन्छ । यहाँको माटो तरकारीका लागि निकै उर्बर छ लामो समयदेखि व्यावसायिक रूपमा तरकारी खेती गर्दै आएका खड्केले भने ? तर बजारको अभावका कारण कि त आफैंले बोकेर टाढासम्म लैजानुपर्छ ? कि सस्तोमै बेच्नुपर्छ । ',\n",
       " 'उनका अनुसार अस्पतालमा उपचाररत कतिपय बिरामीले रगत प्रोसेसिङको पनि रकम तिर्न नसक्ने अवस्था हुँदा गाह्रो हुने भएकोले नगरपालिकाको सहयोगमा बिरामीले सहज रगत पाउन सक्नेछन् । रगतको जोहो रेडक्रसले नै गर्नेछ । कुतियाकभरमा करिब ४० परिवारको बसोबास छ । यहांँ अधिकांश तरकारी उत्पादनमै लागेका छन् । तर यहांँका कृषकहरूले न त कुनै बीउ बीजन पाउँछन् न कुनै प्राविधिक सुविधा नै । छिमेकी बंगालीहरूबाट बीउबीजन लिएर तरकारी लगाउने गरेको उनीहरू बताउँछन् । बंगालीबाटै खेतीपाती गर्ने तरिकासमेत सिक्नुपर्ने बाध्यता रहेको स्थानीयको गुनासो छ । टीकापुर  टीकापुर नगरपालिकाका बासिन्दाले निशुल्क रगत पाउन थालेका छन् । रगत निकाल्दा लाग्ने प्रोसेसिङ चार्ज नगरपालिकाले व्यहोरिदिने भएपछि बिरामीले रगत निशुल्क पाउन थालेका हुन् । नेपाल रेडक्रस सोसाइटी शाखा टीकापुरका सचिव धर्मराज शर्माले टीकापुर नगरपालिकाभित्रका बिरामीलाई आवश्यक रगत जाँचको खर्च रकम नगरपालिकाले व्यहोर्ने रेडक्रससित सम्झौता भएको बताए । उनले भने ?  रगत त पहिले पनि निशुल्क नै हो तर एक पिन्ट रगत प्रोसिसिङका लागि करिब ८ सय रुपैयाँ लाग्छ । त्यो रकम नगरपालिकाले दिने भएपछि बिरामीले त्यो रकम तिर्न नपर्ने भएको छ । कुतियाकभरकै मानवीर सुनारले पनि मुला ? बोडी र परबर उत्पादन गरेका छन् । उनी पनि खड्केसँगै दैनिक महेन्द्रनगर पुगेर आफ्नो तरकारी बिक्री गर्दै आएका छन् । पारि भारतीय बजारभन्दा महेन्द्रनगरमा प्रतिकिलो ५१० रुपैयाँ बढी पाइन्छ मानवीरले भने ? टाढा भए पनि तरकारीको मूल्य हामीले सोचेभन्दा राम्रो पाइन्छ । ',\n",
       " 'नेपालभारत सीमा क्षेत्रको बस्ती भएकाले बुढाका लागि भारतीय बजार नजिक छ । तर त्यहॉँ तरकारीको उचित मूल्य पाइँदैन । त्यही भएर धेरै टाढा भए पनि महेन्द्रनगर पुग्नुपरेको उनले बताए । उनले आधा बिघा आफ्नो र आधा बिघा वार्षिक ५० हजार भाडामा लिएको जमिनमा तरकारी खेती गरेका छन् । खुर्सानी ? तरुल ? मुला ? परबर ? भण्टासँगै बेसार र अदुवा समेत उनको बारीमा पाइन्छ । मानवीर पनि दैनिक एकडेढ क्विन्टल तरकारी महेन्द्रनगर पुर्याउँछन् । महेन्द्रनगरबाट फर्किंदा पनि सानो तिनो सामान लिएर आउँछन् । अहिले महेन्द्रनगरमा मुला प्रतिकिलो २० ? परबर ४० ? भण्टा ४० र बोडी ६० मा बिक्री हुने गरेको उनले बताए । भारतमा यसको आधा मूल्य पनि पाइँदैन उनले भने ? नजिक भएर मात्रै के गर्नु ? फाइदा हँुदैन । ',\n",
       " 'नगरपालिकाले यसका लागि आर्थिक वर्ष २०७५०७६ का लागि करिब १० लाख रुपैयाँ बजेट विनियोजन गरेको जनाएको छ । रेडक्रससँगको सहकार्यमा निशुल्क रगत उपलब्ध गराउने व्यवस्था मिलाएको नगरपालिकाले जनाएको छ । टीकापुर नगरपालिकाबाहेक अन्य ठाउँका बिरामीले भने टीकापुर अस्पतालमा उपचार गराउँदा आवश्यक रगत परीक्षणको शुल्क आफैं व्यहोर्नुपर्ने नेपाल रेडक्रस टीकापुर शाखाले जनाएको छ । प्रदेश १ का मुख्यमन्त्री शेरधन राई दुखेको गर्धन इलाज गर्न बैंकक गएका के थिए ? केही दिनमै न्युयोर्क पुगेको खबर आयो । सँगै आयोउनको अस्वाभाविक अमेरिकी',\n",
       " 'संघ र प्रदेशबीच अधिकारको विवादबारे चर्चापरिचर्चा सुरु भएको छ । सार्वजनिक सञ्चार माध्यम र प्रदेश तथा संघीय सरकारका मन्त्रीहरूको सार्वजनिक अभिव्यक्तिमा समेत यो विषयले प्राथमिकता पाउन',\n",
       " 'कात्तिक ८ गते राति सिकागो अमेरिका मा सुत्नै लाग्दा मेरो मोबाइल फोन बज्यो । विराटनगरमा जाग्राम बसेकी मेरी श्रीमती डाँको छाडेर रुन थालिन् । किन रोएको भनेर',\n",
       " 'संघीय संसदबाट मातृभाषा शिक्षाको पक्षमा विधेयक पारित भएको छ । अनिवार्य तथा निशुल्क शिक्षाको सम्बन्धमा व्यवस्था गर्न बनेको विधेयकको दफा २५ मा विद्यालयले प्रदान गर्ने शिक्षणको',\n",
       " 'केटाकेटीलाई अनुशासित बनाउन पिट्ने प्रवृत्ति घट्दो छ । तर कराउने ? प्राय सबैले आफ्ना बच्चालाई कहिलेकाहीं कराएर गाली गर्छन् । यसले कुनै काम गर्दैन भन्ने थाहा पाउँदापाउँदै पनि',\n",
       " 'गतवर्ष सिरहाबाट बिहानै एक साथीको फोन आयो । उनी निकै हतासिएकी थिइन् । कसैले उनकी छोरीको फोटो तोडमोड गरेर सामाजिक सञ्जालमा राखिदिएछ । पुरानो रिस साध्न कसैले उनकी',\n",
       " 'पञ्चायतकालको आखिरीतिर रानी ऐश्वर्य समाजसेवाका नाममा गैरसरकारी संस्थाहरूको संरक्षकत्व गर्दै थिइन् । गैससहरूको समन्वय गर्ने समाज कल्याण परिषद्को नेतृत्व गर्थिन् । परिषद्को लैनचौरस्थित मुख्यालय निर्माण गर्न दाताहरूसँग',\n",
       " 'लेनिनले रूसका विख्यात लेखक लियो टल्सटायलाई कति पढे थाहा छैन\\u202fतर अहिले नेपालमा राज्य सञ्चालनको जिम्मेवारीमा रहेका धेरै वामपन्थी नेताहरूले टल्सटायलाई भन्दा लेनिन र सोभियत समाजवादलाई',\n",
       " 'आज कोही निरीह छ भने त्यो हो प्रहरी । प्रहरी चौतर्फी प्रहारको सिकार हुनुपरेको छ । अपराधीलाई कठघरामा उभ्याउने प्रहरी आफैं कठघरामा शिर निहुराएर उभिएको छ । १३',\n",
       " 'युवा पुस्ता राजनीति मन पराउँदैन । राजनीतिको कुरा आउँदा उनीहरू नाक खुम्च्याउँदै भन्छन् ? यो फोहोरी खेल हो ? स्वार्थी व्यापार हो । लुटतन्त्र ? बेथिति अनि पथभ्रष्ट र भ्रष्टाचारको',\n",
       " 'यो वेबसाइट कान्तिपुर राष्ट्रिय दैनिकको आधिकारिक न्युज पोर्टल हो । नेपाली भाषाको यो पोर्टलले समाचार ? विचार ? मनोरञ्जन ? खेल ? विश्व ? सूचना प्रविधि ? भिडियो तथा जीवनका विभिन्न आयामका समाचार र विश्लेषणलाई समेट्छ । पूरा पढ्नुहोस् ',\n",
       " 'नोट हरेक शुक्रवार बेलुका ९ बजे  मा कार्यक्रम द सिनेमा टाइम्स ? हेर्न नभुल्नु होला ? हाम्रो यस आधिकारिक  मार्फत पनि   द सिनेमा टाइम्स ?  हेर्न सकिने छ  साथै एभिन्युज टेलिभिजन मा बिहान ९३० बाट बेलुकी ९३० सम्म प्रतेक घण्टा प्रोमो प्रसारण भैरेहेको छ । फ्यानले काटे आकाशको सरप्राइज बर्थ डे केक फोटो फिचर           ',\n",
       " '२६ कार्तिक २०७५ ? सोमबार १९४० ४ वर्षमा ६ हजार किलोमिटर सडक र २ हजार ८ सय किलोमिटरसम्म रेलमार्ग विस्तार गरिने',\n",
       " 'काठमाण्डौ ? २ जेष्ठ । प्रमुख प्रतिपक्षी दल नेपाली कांग्रेसले सहमतिका आधारमा राष्ट्रियसभा र प्रतिनिधिसभाको नियमावली मस्यौदा पारित गर्नुपर्ने निष्कर्ष निकालेको छ । सिंहदरबारस्थित संसदीय दलको कार्यालयमा बुधबार बसेको काँग्रेस संसदीय दलको बैठकले मतविभाजनका आधारमा नभई सहमतीय आधारमा सदनबाट नियमावली मस्यौदा पारित गर्नुपर्ने निष्कर्ष निकालेको हो । मस्यौदा समितिमा भएको सहमति अनुसार नै जिम्मेवार भएर नियमावली मस्यौदालाई अघि बढाउन कांग्रेसले सत्तारुढ दलको ध्यानाकर्षण पनि गराएको छ । कांग्रेसले नियमावलीका विषयमा सदनमा सहभागी सबै दलहरूबीच अधिकतम सहमति खोज्नुपर्नेमा समेत जोड दिएको छ । त्यस्तै बैठकले सरकारको नीति तथा कार्यक्रममा सम्बन्धमा पार्टीको धारणा ठोस रूपमा राख्नका लागि विषयगत समिति बनाउने निर्णय पनि गरेको छ । टोलीमा विज्ञहरूलाई पनि सहभागी गराइने र छोटो समयमा पनि महत्वपूर्ण धारणा राख्न सक्ने गरी सांसदहरूलाई प्रशिक्षण गराइने मुख्य सचेतक बालकृष्ण खाँणले जानकारी दिए । बैठकले बुधबार सम्पन्न संसदीय दलको निर्वाचनमा सहमति जुटाउन सफल भएकोमा निर्वाचन समितिलाई धन्यवाद दिने निर्णय पनि गरेको खाँडले जानकारी दिए । काठमाडौँ  प्रमिला गिरीको बारेमा पढेर मलाई बस्तीपुरको अनायास याद आयो । गिरी परिवारमा प्रदीप गिरीले लेखेको मलाई मन पर्छ ? वानीरा गिरीको लेखन मलाई मन पर्छ । प्रमिला गिरीको आर्ट चाहिं त्यति बुझ्ने क्षमता नभए पनि उनको व्यक्तित्वको म प्रशंसक छु । गिरी परिवार र बस्तीपुरको नाता छ भनेर दस वर्षअघि थाहा पाउँदा म छक्क परेकी थिएँ । मधेसको कुनै पनि आन्दोलनमा कुनै पनि डोम ? चमारलाई सँगै लिएर हिँड्नुपर्ने कुरालाई बल दिएका छैन । कुनै दलितका किशोरीहरू कलिलो उमेरमा आमा नबनुन् र स्कुल जाउन् भनेर कुनै मधेसी नेता प्राय बोल्दैनन् । कुनै पनि मधेस आन्दोलनले मधेसका जिल्ला अस्पताल राम्रो व्यवस्थापन होस् भनेर आवाज उठाउँदैन । तराईका केही भागको मातृ मृत्यु दर कर्णालीमा उच्च छ भनरे तथ्यांक आउँछ ? तर त्यहाँक नेताहरू प्राय यो विषयमा बोल्न रुचाउँदैनन् कि आन्दोलन जीवनसँग जोडिंदैन भन्ने नेताको विचार छ ? प्रदीप गिरीको विचार छ ? राष्ट्रिय र अन्तर्राष्ट्रिय रूपमा चिनिएको बस्तीपुरको गिरी परिवारका र प्रमिला गिरी ? वानीता गिरी र प्रदीप गिरीजस्ता हस्तीहरूले दलित जीवन ? शिक्षा ? स्वास्थ र उनीहरूको गरिबीबारे बोलिदिए र केही काम गरिदिए बस्तीपुरका चमार र पासवान का जीवनमा केही उज्यालो आउँयो कि भन्ने आशाचाहिं मलाई भइरहन्छ । यो आशा गर्नुहुन्न भन्ने थाहा पाउँदापाउँदै । मधेसका महिलाको आन्दोलनले कुनचाहिं यस्तो मानवीय मुद्दा उठाएको छ जुन मुद्दा सफल भयो भने मधेसका महिला र मुसहर मूल प्रभावमा लाग्न सक्नेछन् ? कुनचाहिं मधेसका नेताले तराईमा भएका हिंसा र बलात्कारको सिकार भएका किशोरीको न्यायका लागि मुद्दा उठाएको छ र मैले प्रदीपजीसँग आशा गर्नु । कुनचाहिं मधेस र थारूहरूको आन्दोलनले कैलाली र दाङका पूर्वकमलरीहरूको जीवनमा परिर्वतन ल्याउन कोसिस गरेको छ । मैले गिरी परिवारसँग आशा गर्नु । कागजमा हटिसकेको कमलरी र कमैया प्रथा वास्तविक जीवनमा अझै छ र १५ वर्षकी कमलरी स्कुलमा जान खोज्दा कनै सहयोग नपाएर घरेलु काममा फर्केको उदाहरण मधेसका नेता र थारू नेतालाई भए पनि मौन छन् । त्यसैले बस्तीपुरका मुसहरका लागि आवाज उठाउन गिरी परिवारले किन बोल्नुपर्यो र ? घरभित्र शौचालय नभएर राति मात्र महिला खेतमा जानुपर्ने बाध्यता कम गर्न कुन नेताले ध्यान दिएका छन् ? दिउसो खेत जान नसक्ने भएकाले आधा पेट मात्र खाएर बस्नुपर्ने मधेसी महिलाको कुपोषण के मधेसी नेताले देखेका छन् ? नेताका परिवारका आमा ? बुबा ? दिदीबहिनीलाई खेतमा जान पर्दैन ? मधेसमा रहेका अस्पताल जानु पर्दैन ? कुपोषणको सिकार भएर अकालमा मर्नुपर्दैन त्यसैले त उहाँहरूलाई किन वास्ता हुन्थ्यो त मुसहर महिला समूहको जीवन सुधार्ने वास्ता हुँदैन । मधेसको अहिलेको आन्दोलनमा आएमा नेतृत्व वर्गमा कति जना मुसहरपासवानडोम महिलाहरू छन् ? कुनचाहिं मुसहर महिलाले यो आन्दोलनमा मलाई उमेर पुगेपछि मेरो नै नागरिकता पाउनु मेरो अधिकार हो । र ? म १६ वर्ष पुगेपछि नागरिकता पाउन सक्छु मेरो खातामा मैले कमाएको पैसा राख्न सक्छु भनेर आवाज उठाउन सकेका छन् । उनीहरूले यो आवाज उठाउन सक्दैनन् किनभने उनीहरूको आवाज दबिएको छ । जसले आवाजमा शक्ति भर्ने हो ऊ चुप छ । विकासका लागि यस्ता कुरा उठाएर भनिदिए हुन्थो । कति डोमको घरमा शौचालय छ र आन्दोलनले उनीहरूलाई के दिनेर भनेर हेरे ? उनीहरूलाई सोधौँ कसरी उनीहरूले चुलोमा आगो बलेको छ बुझिदिए त हुन्थ्यो । काठमाडौँ  निर्मला  तिमी मरेकी छैनौकसले भन्छ तिमी मर्यौ ? तिमी त करोडौं करोड विवेकशील मान्छेको आाखामाअमर भएर बााचिरहेकी छ्यौ ',\n",
       " 'दार्चुला  हिमाली क्षेत्रमा बसोबास गर्दै आएका शौका समुदाय पछिल्लो पुस्तादेखि दसैंतिहार मनाउन थालेका छन् । पछिल्ला १०१५ वर्षयता अन्य समुदायको देखासिकीका कारण चाडपर्व मनाउन थालिएको उनीहरू बताउँछन् । शौका जनजाति समुदायको बसोबास नेपालको दार्चुला जिल्लामा मात्र छ । कपिलवस्तुको तौलिहवास्थित तौलेश्वरनाथ मन्दिर परिसरमा माटोका भाँडाकुँडा बेच्न बसेका व्यवसायी । तिहारमा विशेषगरी पाला बढी बिक्री हुने गरेका छन् । तस्बिर  मनोज पौडेलकान्तिपुर',\n",
       " 'एकडेढ दशक अघिसम्म दसैंतिहारको समयमा आफूहरू ब्याँसमै हुन्थ्यौं ?  तिंकर टोलका स्थानीय चमकसिंह तिंकरीले भने ? बिस्तारै अन्य समुदायसित सदरमुकाम खलंगामै बसिन थालेपछि रमाइलोका लागि दसैंतिहार मनाउँछौं ।  तिहारमा भने यमपञ्चकका पाँचै दिनमा तिहार मनाउँदैनाैं । लक्ष्मीपूजा अर्थात् औंसीको दिन घरभित्र साँझको समयमा आफ्नो कुल देवतालाई सम्झेर पूजा गरिने उनको भनाइ छ । अन्दाजी ४५ वर्ष उमेर बताउने तिंकरी दसैं त आफूले हाईस्कुल पढ्ने बेलादेखि मनाउन थालेको बताए । तिहार भने उक्त समयमा मनाउँदैनथ्यौं ?  उनले भने ? आफूभन्दा पाको पुस्तालाई सोध्दा दसैंतिहारका बारेमा जानकारी नभएको बताउँछन् । ',\n",
       " 'तिहारमा मनाइने काग ? कुकुर ? गाई ? गोरु र भाइटीकामध्ये पछिल्ला समय क्याम्पस र कलेज पढ्ने विद्यार्थी उमेरसम्मकाले बिस्तारै मनाउँदै आएका छन् । यहाँका स्थानीय अन्य समुदायमा भाइटीकालाई महत्त्वसाथ मनाउने हुनाले उनको साथी सर्कलका भाइबहिनी टीका लगाउँछन् । तर ? हाम्रो पुस्तामा भने भाइबहिनीमा तिहारको टीकासमेत लगाउने चलन छैन ?  तिंकरीले भने । उनका अनुसार आफूभन्दा पाको पुस्तालाई तिहार भनेपछि जुवातास मात्र खेल्ने सम्झन्छन् । आफ्ना बालबच्चाको अन्य समुदायसँग हिंडाइ बसाइ बाक्लिँदै गएपछि उनीहरूको संस्कार भित्रिनु स्वाभाविकै भएको महेन्द्र माध्यमिक विद्यालयका शिक्षक प्रेमसिंह बोहराको भनाइ छ । दसैंतिहार मान्ने आफ्नो परम्परागत संस्कार नभए पनि अन्य समुदायका परिवार छिमेकी हुँदा त्यसको प्रभाव बढेको छ ?  उनले भने ? वल्लोपल्लो घरका मानिसले बिस्तारै दसैंतिहार मान्दै आएपछि आफूले पनि बाध्य भएर मनाउनुपर्ने स्थिति छ । ',\n",
       " 'आफ्नो समुदायको परम्परागत रूपमा भव्यतासाथ मनाउँदै आएको चार्ड एवं मुख्य पर्व घोबला हो । उनीहरूको मुख्य चाड घोबला माघ महिनामा मनाउने उनले बताए । पहिला यो समयमा ताक्लाकोटको व्यापारमा व्यक्त भइने बेला भएको हुनाले सबै मानिस कोही ब्याँसमा त कोही व्यापारको सिलसिलामा ताक्लाकोट भइन्थ्यो ?  बोहराले भने ? परिवारका सबै सदस्य आफ्ना काममा व्यस्त हुन्थे । त्यसका कारण पनि दसैंतिहार तथा अन्य चाडको खासै मतलबै हुँदैनथ्यो ।  अहिले पनि यो समुदायका धेरैजसो मानिस व्यापारका सिलसिलामा तांलाकोट गएको उनी बताउँछन् । अधिकांशका घरमा केटाकेटी र वृद्धवृद्धा भेटिन्छन् । यो समुदाय परम्परागत रूपमा व्यापार गर्दै आएको हुनाले महिला पुरुष व्यापारमै व्यस्त हुने शिक्षक बोहराको भनाइ छ । ब्याँस क्षेत्रमा बसोबास रहेका शौका समुदायका मानिस यो महिना ताक्लाकोटको व्यापार सकेर मंसिरको सुरुमै बेंसी अर्थात् सदरमुकाम खलंगा झर्ने गर्छन् । परम्परागत आफ्ना लालाबालासहित बसाइँ सर्ने हुँदा हिउँदमा लगाइने न्यानो कपडा र ऊन ताक्लाकोटबाट ल्याएर यहाँ व्यापार गर्छन् । ',\n",
       " '६ महिनासम्मलाई व्यापार गर्नका लागि ताक्लाकोटबाट रेडिमेट लत्ताकपडा ? श्रृगारका सामग्री ? इलोक्ट्रोनिक सामान बिक्रीका लागि बेंसी ल्याउँछौं ?  ब्याँस र खलंगामा व्यापार गर्ने स्थानीय व्यापारी जितसिंह बोहराले भने ? ६ महिनापछि ब्याँस उक्लँदा यहाँबाट ताक्लाकोटका लागि गुड ? मिश्री ? शृंगारलगायत सामान लानुपर्छ । ',\n",
       " 'सिमरा  तराईको प्रसिद्ध लोक पर्व समा चकेवा मनाउन बारा सदरमुकाम कलैया लगायत जिल्लाको ग्रामिण भेकमा अहिले समा चकेवाको कलात्मक मुर्ती घर घरमा भित्रियाउन थालिएको छ । माटोले बनाईएको उक्त मुर्ती लिएर गाउ गाउमा आईपुगेका कुम्हालबाट किशोरीहरुले प्रतिगोटा नगद २५ रुपैयाँ वा सो रकम बराबर अन्न दिएर खरीद गरीरहेका छन । एका बिहानै कांधमा समा चकेवाको भरिया बोकेर कलैया उपमहानगरपालिका १६ उत्तरझिटकैयामा पत्रहट्टीबाट एक कुम्हाल आई पुगेका थिए । उनको कांधमा रहेको भरिया समा चकेवाको स साना मुर्तिले भरिएको थियो । उनी गाउमा प्रवेस गरेसंगै स साना बालबालिकाहरु समा चकेवा आयो भन्दै खुशीले हौसिएका देखिन्थे । गाउका अधिकांसलाई पायक पर्ने स्थानमा उनी भरिया राखेर पहिला सुस्ताए । आफ्नो घरबाट उनी झन्डै ३ घण्टाको पैदल दुरी तय गरेर मुर्ती बेच्न परको गाउमा आई पुगेका थिए । सिजनको बेलामा यस्तो मुर्ती निर्माण गर्छौ ?  मुर्ति बेच्न आएका विश्वनाथ कुम्हालले भने ? पर्व सुरु हुनु अगावै बिक्री गरीसक्नु पर्छ ।  समयमा बिक्री गर्न नसकिए लगानी खेर जाने उनले बताए । उनी लामो समय देखी सिजनलाई लक्षित गरी समाचकेवाको मुर्ती बनाएर बिक्री गर्दै आएको बताउछन । माटोको भाडा कुडा ? मुर्ती ? दियो लगायत निर्माण गर्ने उनको पुख्र्यौली पेसा हो । पुख्र्यौली पेसाबाट जिवन निर्वाह गर्न सक्ने अवस्था नरहे पनि माग अनुसारको सामा निर्माण गरी पुर्याउदै आएको उनले बताए । आफ्नो उत्पादनलाई छिटो भन्दा छिटो बिक्री गर्न समा चकेवाको कलात्मक माटोको मुर्ति घुमाएर बिक्री गर्न थालेको उनले बताए । उनले उत्तर झिटकैयामा मात्र ३२ वटा भन्दा बढी समा चकेवाको मुर्ति एकै छिनमा बिक्री गरेको जानकारी दिए । पहिलाको तुलनामा मुर्तिको लगायत माटोको भाडोको व्यपार घटेको उनले जानकारी दिए । प्रत्येक वर्ष गोवद्र्धन पुजा देखी कार्तिक पुर्णिमा सम्म स्थानीय किशोरीहरु समाचकेवा उक्त मुर्ति राखेर समा चकेवा लोक पर्व मनाउछन । समा चकेवा लोक पर्व मनाउन उत्तरझिटकैयाकी १४ वर्षिया किशोरी मुनिया कुमारी ठाकुरले समा चकेवाको २ वटा मुर्ति एक्लैले खरीद गरीन । उनको समुहमा अन्य ४ जना किशोरीहरु सामेल रहेका छन । उनीहरुले पनि एकएक वटा मुर्ति किनेको उनले जानकारी दिईन । पहिला जस्तो धेरै समा चकेवाको मुर्ति किन्दैनन ?  उनले भनिन ? पहिला धेरैले सामचकेवाको मुर्ती किन्थे ।  अहिले १६ वर्ष मुनिका किशोरीले मात्र समा चकेवा खेल्छन । ठुलाहरु खेल्दैनन । त्यसैले अहिले आएर मुर्तीको माग कम हुदै गएको उनले जानकारी दिईन । सोही ठाउकै अर्का किशोरी रंजिता कुमारी मण्डलले परम्परागत रुपमा मनाईदै आएका धेरै लोक पर्व लोप हुदै गएको बताईन । समा चकेवा प्रति पनि धेरैको चासो छैन ?  उनले भनिन ? पहिला बढी उमेरका किशोरी र महिलाले पनि मनाउथे ? अहिले मनाउदैनन ।  हामी कलिाला किशोरीहरुले यो मौलिक पर्वलाई धानेका छौं । गाउका युवा पुस्ताले ध्यान नदिए यो पर्व पनि अरु लोक पर्व जस्तो लोप भएर जाने उनको बुझाई छ । लोक पर्व हाम्रो पहिचान हो ? त्यसैले यसलाई जिवन्त राख्नु पर्छ ?  उनले भनिन ? पहिचानको लागी सबैले आन्दोलन गर्छन तर पहिचान झल्काउने लोक पर्वको जगेर्णा गर्दैनन ।  मधेशको पहिचान र संस्कृति झल्काउने लोक पर्वलाई जोगाउनै पर्नेमा उनको जोड छ । लोक पर्वलाई मनाउन अहिले देखी नै किशोरीहरु तयारीमा लागेको गाउका बृद्धा ७५ वर्षिय भुवन रात अहिरले बताए । पहिला पहिला त अन्नबाट नै समा चकेवाको मुर्ती किन्थे ?  उनले भने ? नगद हेर्नै मुश्किल पथ्र्यो ।  तर अहिले सबैसंग पैसा सहज उपलब्ध भएकोले अन्न भन्दा पैसाले नै मुर्ती खरीद गर्ने गरेको उनले बताए । पहिला पहिला जोडतोडका साथ श्रद्धा पुर्वक समा चकेवा मनाउने गिरिन्थ्यो । अहिले यो पर्व मनाउन कसैले चासो नदेखाएको उनले गुनासो गरे । युग परिवर्तनसंगै रिती संस्कृती समेत परिवर्त हुन थालेकोमा उनले चिन्ता व्यक्त गरे । तराई मधेशमा खासगरी मधेशी र थारु समुदायका किशोरीहरु समा चकेवा लोक पर्व मनाउने गर्दछन् । त्यहाँ तर अहिले औपचारिक रूपमा स्कुल निषेधित त छैन तर धेरै दलित बालिका स्कुल जाँदैनन् । सानैमा गौना गर्छन् । उनीहरूको स्कुल जाने सपनालाई कुनै पनि मधेसका नेताहरूले मुद्दा बनाएजस्तो लाग्दैन ? त्यसैले गिरी परिवारले किन वास्ता गरेनन् भनेर चित्त दुखाउनुपर्ने त कारण सायद छैन । तर ? मानवीय व्यवहार नै हो ? जो शक्तिशाली छ ऊसँग सायद बढी नै आशा गरिन्छ । मैले भन्ठानेकी छु सायद हामी टाढाबाट बस्तीपुर हेर्ने मानिसले गिरी जति शक्तिशाली परिवार छ ? त्योभन्दा बढी नै अपेक्षा गरेको हो कि ? ',\n",
       " 'किनभने प्रदीप गिरीले त्यहींबाट आफ्नो राजनीति बनाए ? उनी अनेक दार्शनिक लेख लेख्छन् ? गाँधीका ? अनुयायी हुन् भन्ने थाहा थियो । तर ? उनको परिवारले बनारसम गएर उच्च शिक्षा हासिल गरे पनि सायद उनले बस्तीपुर को बस्तीलाई चाहिं बिर्सेजस्तै छ । किनभने म बस्तीपुर पुग्दा कोही पनि दलित ? मुसहर ? चमार ? राय ? पासवान ? डोम । स्कुलमा आउँदैनथे । डोम त सायद अझै पनि स्कुल जाँदैनन् । बस्तीपुरको स्कुल २००४ सालमा खुलेको थियो । त्यहाँ पढ्न भारतबाट पनि आउँथे रे । तर ? वरिपरि भएका दलितहरूका लागि त्यो स्कुल प्राय निषेधित क्षेत्र नै रहेको थियो ?  एक ७० वर्षीय शिक्षकले भनेका थिए । यो त गिरीहरूको राजनितीक क्षेत्र हो । किन सानो रूपमा भए पनि यहाँका दलितहरूलाई शक्तिशाली परिवारले केही प्रेरणा नदिएको होला ।  मैले प्रमिला गिरीको बारेमा पढेपछि सम्झें । तर ? सायद उहाँहरूले धेरै काम गरे पनि ? मुसहर बस्तीसम्म पुगेको पनि हुन सक्छ । बस्तीपुर बारम्बार जाँदा मैले अनुभव गरेकी हुँ कि कति मुसहर महिला नागरिकता नपाएर सरकारले दलितलाई दिएको सुविधा पाएका छैनन् । सायद ती दलित परिवारले आफ्नो व्यथा गिरी परिवारलाई सुनाउन भ्याएका छैनन् । मधेसमा छोरीहरूलाई नागरिकता दियो भने बाबुको सम्पत्तिमा आफ्नो अधिकारको दाबा गर्छन् भन्ने विचार हुन्छ । लोग्नेले पनि विवाह गरेपछि नागरिकता दिन चाहँदैनन् ? सम्बन्धविच्छेद भए लोग्नेको सम्पत्ति माग गर्छन् भनेर ।  यो वास्तविकता त पक्कै पनि गिरी परिवारलाई थाहा नै छ होला । प्रदीप गिरीले कांग्रेसका नेताको हैसियतले यी समस्या उठाउँदा पनि कतै हामीले नसुनेका त होइनौं ? सायद स्थानीय नेताले राष्ट्रिय स्तरका नेता गिरीलाई यो समस्या भन्न नभ्याएका हुन् कि ? सायद कति नेता भन्छन् ? हामीले सम्पूर्ण मधेसको मुद्दा उठाइरहेछौं । यति जाबो एउटा बस्तीपुरको त्यो पनि मुसहर ? डोम र चमारको मुदा मात्र उठाउने हो र ?  तर मलाई लाग्छ साधारण समस्या समाधान नभए कसरी जीवन चल्छ ? द्वन्द्व विशेषज्ञ डा विण्णु उप्रेतीले एक छलफलको कार्यक्रममा भन्नुभएको थियो ? मधेसी दलले मधेसी मुद्दा उठाएका नै छैनन् । भूमिहीन मुसहर ? हलियाको जीवनको स्तर अति नराम्रो छ तर मधेसका नेताहरूले कुनै पनि नीति ? माग र आन्दोलनले भूमिसुधार गरेर भूमिहीनको जीवन बदल्नका लागि आवाज उठाउँदैनन् ? उनीहरू मधेसका महिलाप्रति गरिने मानसिक र शारीरिक हिंसाविरुद्धमा बोल्दैनन् । दाइजो प्रथा र बोक्सीको नाममा मधेसका महिलाप्रति गरिने अति अमानवीय कार्यको विरुद्धमा मधेसीमा नेताहरूले आवाज उठाउँदैन ।  द्वन्द्व विशेषज्ञको विश्लेषण र विचार पनि सायद मधेसका नेताको कानमा पुग्दैन । दर्शनको बारेमा गान्धीको विचार लेख्ने कुनै सिरहाका नेताले आफ्नो लेखमा बस्तीपुर महिलाले ज्ञानको लाठी लिएर गलत सांस्कृतिक मान्यतालाई हटाउन प्रेरित गरेको देखिएको छैन । कहिले उनीहरूले मुसहरको आवाजलाई स्थान दिएको पाइएको छैन । ना॰ खेतीपाती गर्ने वर्षायाममा वा पानी पर्नुपर्ने समयमा पानी नपरी सुक्खा लागेको अवस्था लामो समयदेखि पानी नपरेको अवस्था सुक्खा अनावृष्टि । पाठ क्रिएटिभ कमन्स एट्रिब्युसनसेयरअलाइक लाइसेन्सअन्तर्गत उपलब्ध छ अतिरिक्त सर्तहरू लागू हुन सक्छन् । अधिक जानकारीको लागि उपयोगका सर्तहरू हेर्नुहोला । गाउँमा जनप्रतिनिधि निवार्चित भएपछि स्थानीय तहहरुमा केही न केही नौला कामहरु भइरहेका छन । रोल्पा जिल्लाको सुकिदह गाउँपालिका ३ नं वडापालिका पुरै गाउँनै हरियाली देखिएको छ । रोल्पा ? १५ मंसिर । सदरदमुकाम लिबाङदेखि केही कोष टाढा रहेको सुकिदह गाउँ । पहिले पुग्दा हरेक रङले गाउँ रङगी ? बिरङगी देखिन्थ्यो । अहिले गाउँ पुरै हरियो रङले सजाइएको छ । सरसफाइको प्रतिक भनेर चिनिने हरियो रंङले गाउँ हरियाली बनेको छ । सुकिदह गाउँपालिका वडा नं ३ मा करिब ७ सय घरहरु रहेका छन । ती घरमध्ये ८७ प्रतिशत घरमा हरियो रङ लगाइसकेको वडा सचिब कमलराज वलीले जानकारी दिए । गाउँको फरक परिचान बनाउनको लागि यो अभियानको सुरुवात गरेको सुकिदह ३ का वडाअध्यक्ष जलेशकुमार केसीले बताए । पूर्ण सरसफाइमा सहयोग पुयाउने उद्देश्यका साथ यो अभियानको सुरुवात गरिएको हो । गाउँका पुरै घर एउटै रङ लगाउँदा निकै सुन्दर देखिएको स्थानीयहरुले बताएका छन । उनीहरु वडापालिकामा एउटै रङ लगाउने कुरामा सहमत भएका थिए । सरकारकै इशारामा चल्ने किसिमको कर्मचारी संयन्त्र बनाउन आफूहरुले छलफल गरिरहेको संकेत गर्दै उनले कर्मचारीहरु सिंहदरबारबाट प्रदेश र गाउँपालिकामा जान नमान्ने नियति छिट्टै अन्त्य हुने बताए । पोखरा । नेपाल कम्युनिष्ट पार्टी नेकपा अध्यक्ष पुष्पकमल दाहालले कर्मचारीकै कारण सरकारले जनता सामु गरेका बाचा पुरा गर्न नसकेको बताएका छन् । प्रेस सेन्टर र प्रेस चौतारी गण्डकी प्रदेशले पोखरामा बुधबार आयोजना गरेको पत्रकार सम्मेलनमा दाहालले राजनीतिक परिवर्तनका लागि दलहरु सफल भए पनि कर्मचारी संयन्त्रका कारण सरकार असफल भएको आरोप लगाए । पूर्वप्रधानमन्त्री समेत रहेका दाहालले अन्तर्राष्ट्रिय जगतमा राजनीतिक परिवर्तनपछि पुराना कर्मचारीको नेतृत्व तहलाई पूरै हटाइने र नयाँ प्रणालीलाई आत्मसात गर्ने कर्मचारी ल्याउने परम्परा रहेको बताए । ब्यूरोक्रेसीले राजनीतिक नेतृत्वलाई बिस्तारै आफूतर्फ लैजाँदा दलहरु असफल हुने गरेको उनले बताए । प्रदेश सरकारलाई प्रभावकारी नबनाउने हो भने परिवर्तन असफल हुने र सबै उपलब्धी चेतावनीसमेत उनले दिए । उनले भने ? प्रदेश सरकार असफल हुनु भनेको गणतन्त्र ? संघीयता ? धर्मनिरपेक्षतालगायत सबै परिवर्तनहरु असफल हुनु हो । ',\n",
       " 'अध्यक्ष दाहालले ओली सरकार अन्तर्राष्ट्रिय शक्ति सन्तुलन ? राष्ट्रियता ? ऐतिहासिक र रणनीतिक रुपमा सफल भएको बताए । जनताले देख्ने गरि विकासको काम हुने ठोकुवा गर्दै उनले सरकारले गियर बदलेर जनताका चाहना पुरा गर्ने बताए । असार ९ काठमाडौँ क्रोयसियासँग बिहीबार भएको विश्वकप खेलमा अर्जेन्टिनाले लज्जास्पद हार ब्यहोरेको र यसका कारण खेलाडीले प्रशिक्षक जर्ज सामपाओलीलाई हटाउन चाहेका भन्ने समाचार आफ्नो समूहविरुद्ध अफवाह फैलाउन रचिएको प्रपञ्चबाहेक केही नभएको अर्जेन्टिनाले जनाएको छ । यो विल्कुल गलत हो ?  अर्जेन्टिनी फुटबल समूहका अधिकारीले भने ? खेलाडीको बैठक र अन्य केहीबारे पनि आएका नकारात्मक खबरहरू भ्रम सिर्जना गर्न गरिएका अनर्गल प्रचारमात्र हुन् । ',\n",
       " 'बिहीबारको खेलपछि अर्जेन्टिनाका स्थानीय तथा अन्तर्राष्ट्रिय सञ्चारमाध्यममा अर्जेन्टिनाका खेलाडी तथा प्रशिक्षक र प्रशिक्षण समूहका अन्य सदस्यबीच तनाब बढेको र खेलाडीले प्रशिक्षक सामपाओलीलाई हटाउन चाहेको भन्ने समाचारहरू बाहिरिएका थिए । शनि ? असार ९ ? २०७५ मा प्रकाशित सुनौलो नेपाल अनलाइनद्वारा संचालित सुनौलो नेपाल टी भीसंग प्रत्यक्ष जोडिन यहाँ क्लिक गर्नुहोस',\n",
       " 'हलिउड अभिनेता फिलिप सिमोर हफम्यानको निधन भएको छ । ओस्कार अवार्ड विजेता फिलिपको ४६ बर्षको उमेरमा न्युयोर्कस्थित आफ्नै अपार्टमेन्टमा निधन भएको हो । उनी केही समयदेखि लागूऔषधीको दुव्र्यसन थिए । अपार्टमेन्टबाट लागूऔषधका सिरिन्ज र हेरोइन फेला परेको बताइएको छ । सन् १९९० पनि हलिउडमा देखिएका फिलिपको बग्गी नाइट ? द बिग लेबोस्की जस्ता चर्चित फिल्म रहेका छन् । रंगमन्चबाट अभिनय थालेका फिलिपले चलचित्रको निर्देशन समेत गरेका छन् । उनलाइ २००५ मा कोपेट फिल्मको लागि ओस्कार अबार्ड दिइएको थियो । उनले आफ्नो अन्तिम अभिनय हङ्गर गेम्समार्फत गरेका थिए । असंख्य शुभचिन्तकहरुले उनको निधनमा शोक व्यक्त गरेका छन् । उनलाई महत्वकांक्षी र निकै प्रशन्न अभिनेताका रुपमा हलिउडमा सम्झने गरिन्छ । काठमाडौं  एभिन्यूज खबरले हरेक वर्ष सञ्चालन गर्दै आएको देउसी भैलो प्रतियोगिता यस वर्ष पनि सञ्चालन गर्ने भएको छ । ',\n",
       " 'प्रधानमन्त्री शुसिल कोइराला तेस्रो बिमस्टेक शिखर सम्मेलनमा भाग लिएर स्वदेश फर्केका छन् । फर्कने क्रममा त्रिभुवन अन्तराष्ट्रिय बिमानस्थलमा संचारकर्मीहरुसंग कुरा गर्दै प्रधानमन्त्री कोइरालाले बिमस्टेक सम्मेलन नेपालका लागि फलदायी भएको दाबी गरेका छन् । उनले सम्मेलनमा भुटानी शरणार्थी समस्या समाधान लागि भुटानी पक्षसंग निकै महत्वपुर्ण कुराकानी भएको बताएका छन् । नेपालले बिमस्टेकको अध्यक्षता ग्रहण गर्न',\n",
       " 'काठमाडौं  सरकारले भारत ? संयुक्त अरब इमिरेट्स र मलेसियाका लागि राजदूत सिफारिस गरेको छ । गएराति भएको मन्त्रीपरिषद् बैठकले तीन ',\n",
       " 'पाउनु ठुलो उपलब्धी भएको भन्दै प्रधानमन्त्री कोइरालाले सबै सदस्य राष्ट्रहरु मिलेर यस क्षेत्रको गरिबी घटाउने लगायतका साझा चुनौती सामना गर्नुपर्नेमा जोड दिए । उनले भारतिय प्रधनमन्त्री डा मनमोहन सिंह र म्यानमारकी प्रजातन्त्रवादी नेत्री आङसाङ सुकीलाइ नेपाल भ्रमणको निमन्त्रणा दिएको जानकारी गराएका छन् । पत्रकार सम्मेलनमा प्रधानमन्त्री कोइरालाले प्रशंगबस आफुले उठाएको टनकपुरबाँधको बिषयमा बिबाद गर्नुपर्ने आबश्यकता नरहेको पनि बताएका छन् । उनले सम्मेलनमा नेपालमा जारी शान्ति प्रक्रियामा संबिधान निर्माण तथा लोकतन्त्र प्राप्तिका लागि भएका प्रयासका बारेमा पनि सम्मेलनमा सहभागी बिभिन्न देशका राष्ट्र प्रमुखहरुले चासो दिएको पनि बताए । आल्डरशट ? ग्रेटर रशमॊर नेपाली कम्युनिटीले यहि आगामी १ जुन २०१७ ? बेलायतकी महारानी एलिजाबेथ द्वीतियको जन्म दिन भिक्टोरिया डे मनाउने काम भब्य रुपमा सम्पन्न गर्ने भएको छ । यस वर्ष ग्रेटर रशमॊर नेपाली कम्युनिटीको सक्रिय पहलमा स्थानिय काउन्सिल एवं ब्रिटिश समुदायसंग प्रत्यक्ष रुपमा साझेदारी सहित सहकार्य गरि प्रथम नौमती बाजा लगायत प्राय सबैजसो',\n",
       " 'कोरियामा प्रवासी मजदुर स्वयमले प्रवासी मजदुरहरुकै समानता र हक अधिकार प्राप्तिको निम्ति गठन भएको एक मात्र मजदुर युनियन प्रवासी मजदुर युनियन एमटियुले निय',\n",
       " 'कोरियामा रहेको प्रवासी श्रमिकहरुको हक अधिकारको निमित्त संघर्ष गर्दै आइरहेको प्रवासी श्रमिकहरुको एक मात्र स्वतन्त्र युनियन प्रवासी मजदुर युनियन एमटियुले ',\n",
       " '२७ वषर्ीय जहाँगिरविरुद्ध वीरगन्जकै एक युवती रमा नाम परिवर्तन ले असार २ गते जिल्ला प्रहरी कार्यालय पर्सामा साइबर अपराध मुद्दाको किटानी जाहेरी दर्ता गरेकी थिइन् । ती युवतीले आफ्नो सेक्स टेपलाई जहाँगिरले इन्टरनेटका विभिन्न साइटमा राखेको तथा त्यसको सिडीसमेत बनाई बजारमा सार्वजनिक गरेको आरोप लगाएकी छिन् । गत जेठ २३ गते युट्युबलगायत विभिन्न अश्लील पोर्नसाइटमा ती युवतीसमेत सरिक सेक्सक्लिप राखिएको थियो । जहाँगिरले आफूसँग पटकपटक शारीरिक सम्पर्क राख्न धम्की र दबाब दिएको तर आफूले नमान्दा उक्त मुभीको सिडी आफ्ना श्रीमान्को घर तथा माइतीमा पठाई आफ्नो बेइज्जती गरेको दाबी जाहेरीमा गरिएको छ । उक्त सेक्स टेप प्रकरणपछि आफ्ना बुबा पक्षघात भै हिडडुल गर्न नसकी अपाहिज भैसकेको तथा बहिनीहरू घरबाट बाहिर निस्कन नसकेको उनले आफ्नो जाहेरीमा उल्लेख गरेकी छिन् । हाल श्रीमान्ले आफूलाई बहिष्कार गरी माइतीमा छाडी गएको अवस्थामा समेत जहाँगिरले आफूलाई पटकपटक धम्की दिइरहेको उनले आरोप लगाएकी छिन् । रमाले जहाँगिरसँग आफ्नो मित्रता एवं प्रेमसम्बन्ध एक कम्प्युटर इन्स्िटच्युटमा कम्प्युटर सिक्ने क्रममा भएको बताएकी छिन् । जहाँगिरले त्यस क्रममा आफूलाई नशालु पदार्थ ख्वाएर शारीरिक सम्पर्क राखेको तथा त्यसको तस्बिर खिचेको दाबी गरेकी छिन् । उक्त मुभी वीरगन्जकै छपकैयातिर खिचिएको हुन सक्ने उनको अनुमान छ । चरणबद्ध रूपमा दुई थरीका क्लिप इन्टरनेट साइटमा राखिएका थिए । तेस्रो क्लिप भने सिडीका रूपमा बजारमा आएको थियो । आगोझैं फैलिएको वीरगन्जसँग सम्बन्धित उक्त सेक्स टेप प्रकरण पछि जहाँगिर फरार भएका थिए । उनी एकैपटक अदालतमा उपस्थित भएका हुन् । मुलुकमा साइबर अपराधसम्बन्धी कानुन निर्माण भएपछि उक्त कानुनलाई टेकेर न्यायालयसमक्ष पुगेको वीरगन्जको एउटा मुद्दा अहिले चर्चाको बिषय बनेको छ । साइबर अपराधका आरोपी एक युवक भदौ २३ गते पर्सा जिल्ला अदालतमा १ लाख ३० हजार रुपैयाँ धरौटी राखेर सामान्य तारेखमा छुटेका छन् । मुलुककै नौलो खालको तथा सम्भवतः पहिलो यो मुद्दाका आरोपी वीरगन्ज उपमहानगरपालिका२ ? छपकैंयाका शैफ भनिने जहाँगिर आलम पर्सा जिल्ला न्यायाधीश विष्णुप्रसाद कोइरालाको आदेशमा धरौटी लिई तारेखमा रिहा भएका हुन् । युवतीले विवाहपूर्व आफ्नो प्रेमसम्बन्ध जहाँगिरसँग रहेको र त्यसै क्रममा उनीसँग राखिएको शारीरिक सम्बन्धको तस्बिर मोबाइलको क्यामेराबाट छायांकन गरी इन्टरनेट वेबसाइट तथा सिडीमार्फत सावर् जनिक गरिएको दाबी गरेकी छिन् । उक्त मुभीलाई जहाँगिरले एमएमएस तथा सिडी बनाई सार्वजनिक रूपमा बिक्रीवितरणसमेत गरेर आफूलाई हदैसम्मको ब्ल्याकमेलिङ गर्ने काम गरेको रमाले बताएकी छिन् । युवतीको माइती तथा घर दुवै पक्षका व्यक्तिहरूको नामै उल्लेख गरेर उक्त सेक्स प्रकरण सार्वजनिक भएपछि उनको माइती र घर दुवै पक्षले सामाजिक रूपमा ठूलो अपहेलना भोग्नुपर्यो । भारतमा उनको घर पक्ष आफू बस्दै आएको सहरबाट बसाइँ सरिसकेको छ । न्यायाधीश कोइरालाको इजलासले थुनछेक प्रयोजनको बहसपछिको आदेशमा आरोपित जहाँगिर कसुरदार देखिए पनि निजलाई अदालती बन्दोबस्त ११८२ अनुसार थुनामा राख्नुपर्ने अवस्था नदेखिएकाले पछि प्रमाण बुझ्दै जादा ठहरेबमोजिम हुने गरी निज प्रतिवादीबाट अदालती बन्दोबस्तको ११८५ समेतको आधारमा धरौटी लिई मुद्दाको पुर्पक्ष गर्ने भनी उल्लेख छ । अविवाहित जहाँगिरले अदालतमा बयानका क्रममा आफूले हालसम्म रमासँग भेटघाट नगरेको र उनीसँग आफ्नो कुनै पनि खालको मित्रता तथा सम्बन्ध नरहेको बताएका छन् । रमाले जाहेरीमा आफूउपर लगाएका आरोप पूर्ण रूपले झूटो र निराधार भएको उनको जिकिर छ । जहाँगिरले आफूले कक्षा ८ सम्म मात्र पढेको र रमासँग कुनै सम्बन्ध र सरोकार नभएकाले उनी संलग्न सिडी तथा सेक्स क्लिप आफूले सार्वजनिक नगरेको जिकिर गरेका छन् । जिल्ला सरकारी वकिलको कार्यालयले जिल्ला अदालतमा उक्त मुद्दा प्रस्तुत गर्दै जहाँगिरले विद्युतिय इलेक्ट्रोनिक कारोबार ऐन २०६३ को दफा ४७१ अनुसारको कसुर अपराध गरेकाले उनलाई हदैसम्मको सजायको माग गरेको छ । सरकारी वकिलको कार्यालयको अभियोगपत्रमा जहाँगिरले सामाजिक जातीय सद्भाव बिगार्ने ? कसैप्रति घृणा वा द्वेष फैल्याउने वा विभिन्न जातजाति तथा साम्प्रदायिक सुमधुर सम्बन्ध खलल पार्ने कार्य गरेकाले उनलाई सजायको माग गरेको छ । रमाका कानुन व्यवसायी अधिवक्ता सुधीर कर्णले अधिराज्यमै सम्भवतः कुनै पनि न्यायिक निकायमा दर्ता गरिएको सबैभन्दा पहिलो यस्तो खालको मुद्दामा आफूले चाहेर पनि बहस पैरवी गर्न नपाएको बताए । आफू कामविशेषले हेटौंडामा रहेको र रमालाई समेत उक्त मुद्दाको पेसी बुधबार परेको कुराको जानकारी नहुँदा अत्यन्त कम समयमा अदालती कारबाही सम्पन्न भएको उनले बताए । उनले सूचना र प्रविधिको वर्तमान युगमा यस्ता खाले मुद्दामा इजलासमै भिडियो ? कम्प्युटर आदि राखेर न्याय निरूपण गर्न सकिने अवस्था विद्यमान हुँदाहँुदै पनि त्यसो नगरिएको गुनासो गरे । उनले प्रहरीले यो मुद्दामा गम्भीर रूपमा अनुसन्धान गर्न सक्ने अवस्था हुँदाहँुदै पनि त्यसो नभएको बताए । आरोपी एक्लैले उक्त टेप बनाउने ? एडिटिङ गर्ने र वेबसाइटमा हाल्नेसम्मको काम गर्न नसक्ने अवस्था रहेको र उक्त कार्यमा कम्तीमा तीनचार जनाको समूह लागिपरेको निश्चित हुँदाहँुदै पनि जहाँगिर एक्लैलाई अभियुक्त बनाउनु प्रहरी अनुसन्धानमा रहेको कमजोरीको प्रमाण भएको उनको आरोप छ । प्रहरी उपरीक्षक राजेन्द्रमान श्रेष्ठ भने यो मुद्दा हालसम्मकै अत्यन्त नौलो र जटिलसमेत भएकाले प्रहरीले अत्यन्त गम्भीरतापूर्वक लिएको जिकिर गर्छन् । उनले व्यापक अनुसन्धानपछि मात्र प्रहरीले सरकारी वकिलको कार्यालयमा मुद्दा पेस गरेको बताए । यता जिल्ला सरकारी वकिलको कायर्ालय जहाँगिरलाई धरौटीमा छाडने जिल्ला अदालतको आदेशविरुद्ध पुनरावेदन अदालत हेटौंडामा पुनरावेदन गर्ने तयारीमा जुटेको कार्यालय स्रोतले बताएको छ । विवाहअघिको प्रेमसम्बन्ध र त्यस क्रममा भएका गतिविधिका कारण अनपेक्षित नतिजा बेहोरेकी रमा हाल बिस्तारै उक्त घटनाजनित सामाजिक अपहेलना ? लाञ्छनाबाट आफूलाई सम्हाल्ने क्रममा छिन् । घरबाट बहिष्कृत भैसकेकी उनले अकल्पनीय घटना बेहोरेको र आफ्ना कारण माइती पक्ष पनि ठूलो पीडाको सिकार भएकाले आफू अत्यन्त दुःखित भएको बताएकी छिन् । यद्यपि रमाले पहाडजस्तो लामो जीवनलाई त्यत्तिकै घरमा बसेर खेर नफाल्ने मनस्थिति बनाएकी छिन् । विवाहका कारण आफ्नो उच्च शिक्षा अवरुद्ध भएकाले अब उच्च शिक्षालाई निरन्तरता दिने र स्वावलम्बी बन्न चाहेको उनी बताउँछिन् ? तर यो कुरा समाजले सकारात्मक सहयोग र समर्थन गरिदिए मात्र सम्भव भएको कुरा उनलाई थाहा छ । कुनै सहृदयी संघसंस्थाले आफूलाई पुनर्जीवन दिने पहल गरे आफू त्यसका लागि तयार रहेको पनि उनको भनाइ छ । टेप प्रकरणको पात्र आफू एक्लै नभई आफूलाई बदनाम गर्ने युवक पनि उत्तिकै जिम्मेवार भएकाले पितृसत्तात्मक समाजले आफूलाई मात्र दोषी बनाइरहेको उनको गुनासो छ । समाजमा यस्ता क्रियाकलाप गर्ने पुरुष पनि उत्तिकै दोषी हुनुपर्ने र उसको सामाजिक बहिष्कार गर्न सक्नुपर्ने उनको भनाइ छ । नेपाली हिरो त्यसै बलियो भएको होइन । ऊ सुत्नअघि नियमित दूध पिउँछ । उसलाई कि आमाले दूध पिलाउँछ कि हिरोइनले । उसले दूध सधैं पारदर्शी गिलासमा पिउँछ । हिरोइन वा आमा पात्रले पारदर्शी गिलासमा दूध लिएको दृश्य छ भने ? त्यो हिरोकै लागि हो भने बुझे हुन्छ । यसअघि एक गायक तथा हास्यकलाकारको यस्तै खाले टेप सार्वजनिक हुँदा हालको ऐन बनिनसकेकाले उनलाई गाली बेइज्जती ऐनअनुसार कारबाही चलाइएको अधिवक्ता कर्ण बताउँछन् । त्यसैगरी चर्चित दुई मोडल तथा नायिका यस्तै खालका दुई फरक प्रकरणमा चर्चामा आए पनि उनीहरूले कुनै कानुनी उपचार खोजेनन् । वीरगन्जका लागि पनि यस्तो घटना पहिलो भने होइन । यसअघि पनि वीरगन्ज स्थानीय घर भएका एक अन्र्तराष्ट्रिय खेलाडीको भिडियो क्लिप्स सार्वजनिक भएको थियो । स्थानीय एक महिलासँगको उनको उक्त टेपले पनि आगाको झिल्कासरह चर्चा बटुलेको थियो । आधुनिकतासँगै बढ्दो साइबर अपराध यहाँका युवा पुस्ता र उनीहरूका अभिभावकका लागि पनि चिन्ताको विषय बन्दै गएको छ । फलस्वरूप साइबर अपराधसम्बन्धी कानुनलाई अझ बढी प्रभावकारी र बलियो बनाउनुपर्ने धारणा अधिकांश अभिभावकको छ । दुर्घटना राजमार्गमा मात्र हुँदैन । नेपाली फिल्मको कलेजमा पनि हुन्छ । त्यहाँ हिरोहिरोइनले एकआपसमा ठक्कर खाइरहन्छन् । धेरै फिल्ममा यस्तो दुर्घटना भइसकेको छ । हिरोहिरोइन ठोक्किन्छन् र हिरो वा हिरोइनका किताब भुइँमा छरपस्टिन्छ । ओह सरी भन्दै किताब टिप्ने काम हुन्छ । त्यतिबेला फेरि टाउको पनि ठोक्किनसक्छ । हिरोहिरोइन एकअर्कालाई देख्दा मुग्ध भएर टोलाउन थाल्छन् । दायाँबायाँ कोको छन् उनीहरूलाई मतलब हुँदैन । एकनाशले आँखा जुधाउँदै हराउँछन् । सँगैका साथीहरूले उनीहरूका आँखाअगाडि हत्केला हल्लाउँदा पनि ध्यान भंग हुँदैन । गाउँकी हिरोइन यति निर्धक्क हुन्छे कि नुहाउनलाई ऊ जंगलबीचको अप्ठ्यारो झरनामा पुग्छे । घरमै नुहाउन उसलाई मन लाग्दैन । किनभने त्यसो गर्दा उसको जवानी प्रदर्शन हुँदैन । जसरी हुन्छ ? सहरबाट आएको हिरोले देख्ने गरी उसलाई झरनामा गएर नुहाउनैपर्छ । हिरोइनको जवानीमाथि भिलेनको गिद्धेनजर पर्छ । उसले निकै प्रपञ्च रच्दै हिरोइनलाई कब्जामा लिन्छ । त्यतिबेला हिरोइनले सल भएको लुगा अनिवार्य लगाएकी हुन्छे । भिलेनले सललाई तानेर हावामा हुर्र्याइदिन्छ । सल हावामा तैरिँदै झर्छ र हिरोको हातमा पर्छ । त्यसपछि रामधुलाइ । कुनै जटिल रोगले वा गोली लागेर घाइते भएका पात्रले सबैजना भेला भएको बेला लामो संवाद बोल्न थाल्यो भने निश्चित भए हुन्छ ? ऊ मर्दैछ । अरू पात्रहरू पनि एम्बुलेन्स बोलाएर अस्पताल लैजानुको सट्टा त्यो संवाद सुनेर बस्छन् ? मानौ प्राणभन्दा पनि कुरा फुत्किएला भन्ने चिन्ता उनीहरूलाई छ । हिरोहिरोइनको समागमको दृश्य देखाउन नेपाली फिल्मले पुरानै विम्ब प्रयोग गर्न छोडेको छैन । ओछ्यानमा पल्टन्छन् र बत्ती निभाउँछन् । बाँकी कुरा अँध्यारोमा । नयाँ प्रतीकात्मक दृश्य सिर्जना गर्ने जाँगर किन नपलाएको हो ? बुझ्न गाह्रो छ । खोलामा खस्दा लास बेपत्ता हुनुपर्ने हो । तर ? नेपाली फिल्ममा त्यस्तो हुँदैन । खोलामा खसेको पात्र हतपत मर्दैन ? उसलाई कुनै अनजान ठाउँका मान्छेहरूले उद्धार गरिदिन्छन् । बरु ? उसको स्मृति चाहिँ गुम्नसक्छ । त्यसरी गुमेको होश फिर्ता हुन डाक्टर चाहिँदैन ? त्यस्तै कुनै ठूलो घटना घटे पुग्छ । कुनै गाउँले पात्र सहर आयो भने उसलाई सबैभन्दा बढी समस्या केमा हुन्छ ? नेपाली फिल्म हेर्नुस् ? यसको जवाफ पाइनेछ । गाउँलेलाई सहरमा बाटो काट्न निकै समस्या हुन्छ । गाउँलेहरूकै लागि भनेर सहरमा आकाशे पुल बनाइएको हो कि जस्तो लाग्छ । गरिबलाई पैसा अभावले पक्कै सताउँछ । पैसा खट्टे पर्ने कारण थुप्रै हुनसक्छन् । नेपाली फिल्ममा भने अपरेसन गर्न गरिब हिरोलाई पैसा खाँचो परेको देखाइन्छ । हिरोको कोही आफन्त इमरजेन्सीमा भर्ती हुन पुग्छ र डाक्टरले हजारौं वा लाखौं रकम तुरुन्त ल्याएमात्र अप्रेसन गर्छु भन्छ । नेपाली फिल्मका सामन्तीले ऋण असुल गर्ने तरिका अनौठो छ । भाका नाधेपछि ऊ आफ्नो दलबलसहित ऋणीको घर पुग्छ र केटाहरूलाई भाँडाकुडा बाहिर फ्याँक्न हुकुम दिन्छ । ऋणीले त्यतिबेला निकै रोइकराइ गर्छ ? खुट्टा ढोग्छ । तर ? उसको मन पग्लिदैन । सायद यस्तो बिचल्लीमा रमाउने आदत उसलाई लागिसकेको छ । आफ्नो प्रिय व्यक्तिको मृत्यु देख्दा वा त्यसबारे सुन्दा नेपाली फिल्मका पात्रहरू शिथिल बन्नपुग्छन् । त्यतिबेला उनीहरूका हातमा भएका गिलास ? किस्ती ? औषधी वा पैसा सबै खस्छ । नेपाली फिल्ममा यस्तो दृश्य खिचिँदा कति सिसी फुटे ? दूध पोखे वा औषधी बगे कुनै लेखाजोखा छैन । फिल्ममा कुनै प्रिय पात्रको निधन हुँदा रुवाबासी चल्छ । वरिपरि झुम्मिएकाहरूले आआफ्नै पाराले प्रतिक्रिया व्यक्त गर्छन् । प्रहरीले भने अफसोच मान्दै टोपी खोल्छ । श्रद्धा गर्नुपर्ने मान्छे मर्दा प्रहरीले टोपी खोल्छ भन्ने सायद धेरै जसोले नेपाली फिल्म हेरेरै थाहा पाएका होलान् । हिरोलाई आफ्नो मुठ्ठीमा ल्याउन भिलेनले प्रयोग गर्ने अन्तिम उपाय हो ? अपहरण । हिरोइन वा हिरोका परिवारलाई अपहरण गरी आफ्नो अखडामा बाँधिदिएपछि हिरो जसरी पनि आइपुग्छ भन्ने विश्वास भिलेनको हुन्छ । नभन्दै हिरो निरीह बन्दै आइपुग्छ । समाजमा भन्दा बढी अपहरण हुन्छ नेपाली फिल्ममा । विष खुवाएर हिरोलाई मार्नु छ भने फिल्ममा दूधकै सहारा लिइन्छ । षड्यन्त्रकारीले खुसुक्क दूधमा विष मिसाइदिन्छ । तर ? हिरोको भाग्य कति दह्रो हुन्छ भने त्यस्तो दूध कसोकसो गरी भुइँमा पोखिनपुग्छ । र ? अचानक बिरालो आएर खाइदिन्छ । मान्छेको विष बिरालोले खाएको दृश्य कस्तो हुन्छ ? नेपाली फिल्म खोजीखोजी हेर्नुहोला । सासु र नन्द मिलेर षड्यन्त्र बुन्दै गरेको बुहारीले सुन्छे । एउटा पात्र क्यान्सर रोगी हुन्छ र उसको केही महिनापछि मृत्यु निश्चित छ । तर ? यो कुरा उसलाई कसैले भन्दैन । अरूले अर्कै कोठामा कुरा गर्दा उसले सुन्न पुग्छ । यसरी सुन्न नहुने वा सुनाउन नखोजिएको कुरा नेपाली फिल्मका पात्रहरूले सुटुक्क सुनिहाल्छ । हिरो र भिलेन पक्षबीच फाइट भइरहँदा निर्देशकलाई हिरोइन र अरू नारी पात्र झन्झट हुनपुग्छ । यो झन्झटबाट मुक्त हुने तरिका पनि उनीहरूले जानेका छन् । भिलेनसँग झागलझुगल हुँदा हुत्तिएर ढुंगा वा भित्तामा ठोक्काई बेहोश बनाइदिने । त्यसरी बेहोश भएका नारी पात्रहरू फाइट सिद्धिएपछि मात्र ब्युँझिन्छे । हिरोलाई कसैको हत्याको गलत आरोप लागेको हुन्छ । हिरोइनलाई पनि चोरीको आरोप लाग्छ । हिरोको बुबालाई कसले हत्या गरेको हो ? थाहा हुँदैन । यो कुराले हिरोलाई पनि आपत परेको हुन्छ । तर ? अन्तिममा हिरोलाई भिलेनले नै आरोपमुक्त बनाइदिन्छ । हिरोको सबै खान्दान खत्तम हुने भो भन्दै उसले सबैका अगाडि आफ्नो अपराध एकपछि अर्को गर्दै ओकलिदिन्छ । कुनै पात्र मर्नु छ वा पश्चातापको आगोमा जलेको छ भने उसको मृत्यु बडो नाटकीय हुन्छ । हिरो ? हिरोइन वा अरू कुनै महत्वपूर्ण पात्रमाथि प्रहार गरिएको गोली आफ्नो छातीमा थाप्दै उसले मृत्युवरण गर्छ । त्रिकोणात्मक प्रेमकथा भयो भने एउटा जगेडा पात्र हिरो होस् वा हिरोइन ? उसको नियति यस्तै हुने सम्भावना बढी छ । हिरो र भिलेनबीच भीषण लडाइँ हुन्छ । दुवै पक्षका थुप्रै पात्रको निधन हुन्छ । अन्तिममा हिरोले मुख्य भिलेनलाई मार्न लागेको बेला प्रहरी आइपुग्छ । भुइँमा लडेका भुसतिघ्रेहरू र भिलेनलाई पत्रे्कर लैजान्छ । प्रहरीमाथि जनविश्वास घट्नुको कारण फिल्मको यही पाराले पनि हो कि',\n",
       " 'भद्रपुर । स्थानीय तहमा वृद्धि गरिएको करको विरोधमा बुधबार झापाको मेचीनगरमा प्रदर्शन गरिएको छ । नेपाल कम्युनिष्ट पार्टी वडा नं १० को अगुवाइमा भएको विरोध कार्यक्रममा सहभागीहरूले मेचीनगरको मुख्यद्वारमा दुई घण्टा धर्ना दिनुका साथै नाराबाजीसहित नगरपालिका र सम्बन्धित वडा कार्यालयमा ध्यानाकर्षण पत्र बुझाएका थिए । अवैज्ञानिकरूपमा नगरवासीको ढाड सेक्ने गरी नगरपालिकाले गरेको कर वृद्धि तत्काल फिर्ता  ',\n",
       " 'परिक्षामा चिट चोर्ने मामिलामा केटीहरू अति नै अगाडि ? यस्ता स्थानमा यसरी लुकाउछन् चीट हेर्नुहोस् फोटोसहित',\n",
       " 'चर्चित रियालिटी शो टिभिएस अपाचे द भ्वाइस अफ नेपाल  बाट बाहिरिएकी बर्त गन्धर्व र श्रीमानबीच यस्तो प्रेमभिडियो अन्तरवार्ता',\n",
       " 'यी हुन् विश्वका सबैभन्दा महँगा विवाह गर्ने ५ ब्यक्तिहरु ? जसले बगाए पानी सरि पैसा  अवस्य चिन्नुहोस्',\n",
       " 'क्यान्सरको उपचार गराईरहेकी अभिनेत्री सोनाली बेन्द्रेले विवाहको १६ औँ वर्षगाठमा आफ्नै पतिलाई न्यूयोर्कबाट लेखिन यस्तो भावुक चिठी',\n",
       " 'प्रधानमन्त्री ओलीले देवकोटाका परिवारसँग कुरा गरेर उपयुक्त मूल्यमा भग्नावशेषको अवस्थामा रहेको सो घर खरिद गरी रोक्टोफिटिङ गरेर देवकोटा कसरी पलेटी कसेर लेख्थे भन्नेसम्मका स्मृति झल्काउन सरकारले सङ्ग्रहालय बनाउने उद्घोष गर्दा सहभागी स्रष्टाहरुले तालीले स्वागत गरेका थिए । नेपालका प्रधानमन्त्री केपी शर्मा ओलीले महाकवि लक्ष्मीप्रसाद देवकोटाको काठमाडौँ डिल्लीबजार ? धोबीधारास्थित जन्मघर किनेर सङ्ग्रहालय बनाउने घोषणा गर्नु भएको छ । प्रधानमन्त्री ओलीको अग्रसरतामा महाकवि देवकोटाको ११०औँ जन्मजयन्तीका अवसरमा मदन भण्डारी कला ? साहित्य प्रतिष्ठानले हिजो बुधबार प्रधानमन्त्री निवास बालुवाटारमा आयोजना गरेको कार्यक्रममा उहाँले देवकोटा विलक्षण प्रतिभा र बौद्धिक व्यक्तित्व भएकाले उहाँको कृतित्व र व्यक्तित्वबारे भावी पुस्तालाई जानकारी दिनका लागि सङ्ग्रहालयको आवश्यकता रहेको बताउनु भएको राष्ट्रिय समाचार समितिले लेखेको छ । महाकविको उक्त जन्मघर हाल जीर्ण अवस्थामा छ भने नजिकैको नयाँ घरमा उनका छोरा डा पदमप्रसाद देवकोटा बस्दै आउनु भएको छ । विसं १९६६ कात्तिक २७ गते लक्ष्मीपूजाका दिन जन्मिएका र विसं २०१६ भदौ २९ गते निधन भएका महाकावि देवकोटाका मुनामदन ? सुलोचना ? शाकुन्तल पहाडी पुकार लगायत दर्जनौँ लोकप्रिय कृति प्रकाशित छन् । उहाँ पूर्व शिक्षामन्त्रीसमेत हुनुहुन्छ । गतसाता छातिमा संक्रमण भएर उपचारार्थ अस्पताल भर्ना भई निवास फर्केपछि पहिलो पटक सार्वजनिक मन्तव्य दिनु भएका प्रधानमन्त्री ओलीले महाकवि देवकोटाका विभिन्न कविता सुनेपछि भन्नुभयो ? देवकोटाजस्तो बौद्धिक ज्ञानयुक्त विलक्षण कवि मैले विश्वमै देखेको छैन । उहाँ प्राकृतिक प्रतिभा हो ? मूलबाट फुटेर खसेको झाँगोजस्तो हुनुहुन्थ्यो । ',\n",
       " '२८ जेठ ? गौर । रौतहटको कटहरीया नगरपालिका ९ बाट गत शनिवार राति अपहरणमा परेका १३ वर्षीय बालक युवराज साहको गोली हानेर हत्या गरेको अवस्थामा शव फेला परेको छ । कटहरीया ९ डुमरिया टोलका चिकित्सक दिलिप साहको छोरा युवराज शनिवार राति घरमा सुतिरहेकै अवस्थामा हराएका थिए । परिवारका सदस्यहरुसँगै सुतेका युवराजलाई अज्ञात समूहले ओछयानबाटै लगेका थिए । कटहरीया नगरपालिकाभन्दा १५ किलोमिटर टाढा गढीमाई नगरपालिका ५ को बाग्मती नदी किनारमा आज बिहान बालकको शव फेला परेको प्रहरीले जनाएको छ । जिल्लाका विभिन्न क्षेत्रमा बाढी बसेर तथा कृषिउपज डुबानमा परेर सो नोक्सानी भएको हो । सूर्यपूरा गाविसमा रहेको अस्ट्रिच फर्ममा रोहिणी नदीको बाढी पसेपछि ८० भन्दा बढी अस्ट्रिचका छ महिना नाघेका बच्चा बगाएको अस्ट्रिच नेपाल प्रालिका सञ्चालक सिपी शर्माले बताए । बाढीले फर्मको चार बिगाहा जमिन र तारबार कटान गरेको छ । दाङमा पालेका बयस्क अस्ट्रिच ३० वटाभन्दा बढीको मृत्यु भएको कम्पनीले जनाएको छ । बाढीले जिल्लाको करिब १२ हजार ५१९ हेक्टर खेतीयोग्य जमिन डुबानमा परेको छ । तिनाउ ? दानव र रोहिणी नदीमा आएको बाढीले ४० हेक्टर धान खेती नष्ट गरेको छ भने १२ हजार ४६० हेक्टरमा लगाइएको धानबाली डुबानमा परेको जिल्ला कृषि विकास कार्यालयका निमित्त प्रमुख आश्विनी शर्माले बताए । जिल्लामा रु ७५ लाख मूल्य बराबरको धानबाली ? तरकारी रु १५ लाख ? केरा रु दुई लाख ५० हजार ? माछा रु ५० लाख ? मैरीको मह र घार रु ११ लाख ७० हजार तथा बदाम रु १० लाख ८० हजार तथा पशुतर्प करिब रु ३५ लाख ८३ हजार मूल्य बराबरको बाख्रा क्षति भएको पशुसेवा कार्यालयले जनाएको छ । रासस',\n",
       " 'काठमाडौं चैत १ । नेपाली कांग्रेसको भातृ संगठन नेपाल लोकतान्त्रिक खेलकुद संघले नेपाली कांग्रेसका नवनिर्वाचत सभापति एवं पूर्व प्रधानमन्त्री शेरबहादुर देउवालाई सोमबार बधाई तथा शुभकामना प्रदान गरेको छ । ',\n",
       " 'एजेन्सी ? चैत १ । बंगलादेश विश्वकप ट्वेन्टी ट्वेन्टी क्रिकेट प्रतियोगिताको सुपर टेनमा प्रवेश गरेको छ । समुह ए अन्तर्गतको अन्तिम लिग खेलमा ओमानलाई ५४ रनले हराउँदै बंगलादेशले सुपर टेनमा आफ्नो स्थान पक्का ',\n",
       " 'एजेन्सी ? चैत १ । आर्सनललाई हराउँदै वाट्फोर्ड एफए कप फुटबल प्रतियोगिताको सेमिफाइनलमा प्रवेश गरेको छ । आइतबार राति आर्सनलको मैदानमा भएको खेलमा पाहुना टोली वाट्फोर्ड २१ ले विजयी भयो । खेलमा वाट्फोर्डलाई',\n",
       " 'एजेन्सी ? चैत १ । स्पेनिस ला लिगा रियल मड्रिडले लास पालमसविरुद्ध कठिन जित हात पारेको छ । आइतबार राति भएको खेलमा रियल मड्रिडले लास पालमसलाई २१ ले हरायो । रियलको लागि सर्जियो रामोस र कासेमिरोले गोल गरे ',\n",
       " 'काठमाडौं ? फागुन ३० । नेपाली राष्ट्रिय फुटबल टिमका मुख्य प्रशिक्षक ग्योटुकी कोजीले मलेसियासँगको खेलका लागि ४४ खेलाडीलाई प्रशिक्षणमा बोलाएका छन् । कोजीले आगामी चैत्र १४ मा मलेसियाको यु २२ टोलीसँग हुने मै',\n",
       " 'एजेन्सी ? फागुन ३० । चेल्सीलाई हराउँदै एभर्टन एफ । ए कप फुटबल प्रतियोगिताको सेमिफाइनलमा पुगेको छ । घरेलु मैदान गोडिन पार्कमा शनिबार राति भएको क्वाटरफाइनलमा चेल्सीलाई २० ले हराएर एभर्टनले सेमिफाइलनमा स्था',\n",
       " 'एजेन्सी ? फागुन ३० । स्पेनिस ला लिगाको शीर्ष स्थानमा रहेको बार्सिलोनाले विजयी यात्रा कायम राखेको छ । ला लिगा अन्तर्गत शनिबार राति भएको खेलमा बार्सिलोनाले गेटाफेमाथि ६० को जित निकालेको हो । बार्सिलोनाल',\n",
       " 'एजेन्सी ? फागुन ३० । जिम्बाबेलाई हराउँदै अफगानिस्तान ट्वेन्टीट्वेन्टी विश्वकप क्रिकेटको दोस्रो चरण अर्थात सुपर टेनमा पुगेको छ । शनिबार भएको खेलमा जिम्बाबेलाई ५९ रनले हराउँदै अफगानिस्तानले सुपर टेनमा पु',\n",
       " '२९ फागुन ? काठमाडौं । टोलीका युवा स्टारद्धय फरवार्ड अन्जन बिष्ट र रक्षक अनन्त तामाङ ट्रायलका लागि आज नेदरल्याण्डस् जाने भएका छन् । नागरिक दैनिकमा समाचार छ । स्पेनको मावैला युनाइटेड एफसीमा ट्रायलको अ',\n",
       " '२९ फागुन ? एजेन्सी । जारी आइसिसी ट्वेन्टी ट्वेन्टी विश्वकप क्रिकेट भन्दा बढी चर्चामा रहेको पाकीस्तानी टोलीको भारत यात्रा तय भएको छ । पाकिस्तान सरकारले आफ्नो क्रिकेट टिमलाई प्रतियोगितामा सहभागी हुन भा',\n",
       " 'काठमाडौं ? फागुन २८ । नेपालले मलेसियासँग दुई मैत्रिपूर्ण फुटबलको खेल मिति परिर्वतन भएको छ । यसअघि चैत १६ मा हुने भनिएको मलेसियाको राष्ट्रिय टोलीसँगको मैत्रिपूर्ण खेलको मिति परिवर्तन भएको हो । मिति परिवर',\n",
       " 'एजेन्सी ? फागुन २८ । युरोपा लिग फुटबल प्रतियोगिता लिभरपुलले म्यानचेस्टर युनाईटेडलाई २० ले हराएको छ । गएराति भएको खेलमा युरोपियन क्लब फुटबलको दोस्रो ठूलो प्रतियोगिता युरोपा लिग अन्तर्गत प्रि क्वाटरफाइन',\n",
       " 'एजेन्सी ? फागुन २७ । पेसिर सेन्ट जर्मेन पिएसजी च्याम्पियन्स लिग फुटबलको क्वाटर फाइनलमा प्रवेश गरेको छ । बुधबार राति आफ्नै मैदानमा भएको दोस्रो लेगको खेल २१ ले हारेको चेल्सी समग्रमा ४२ ले पराजित हुँदै प',\n",
       " 'सिमरा ? फागुन २६ । त्रिभुवन आर्मी क्लबलाई सडन डेथमा स्तब्ध पार्दै विजय युथ क्लब हेटौडा सिमरा गोल्डकपको फाइनलमा प्रवेश गरेको छ । सिमरा रंगशालामा बुधबार भएको सेमिफाइनल खेलमा आर्मीविरुद्ध सडन डेथमा नतिजा',\n",
       " 'काठमाडौं ? फागुन २६ । ट्वेन्टी ट्वेन्टी आइसीसी वर्ल्डकप प्रतियोगिताको भारत र पाकिस्तानबीचको खेल स्थान परिवर्तन गरिएको छ । खेल हुने स्थलमा सुरक्षाको कारण देखाउँदै यसअघि धर्मशालामा हुने भनिएको खेल अब कोलकत',\n",
       " 'काठमाडौं ? फागुन २६ । आर्सनल एफए कप फुटबल प्रतियोगिताको क्वाटरफाइनलमा प्रवेश गरेको छ । मंगलबार राति हल सिटीको मैदानमा हल सिटीलाई ४ गोल हान्दै आर्सनल क्वाटरफाइनलमा प्रवेश गरेको हो । पाँचौ चरणको रि प्ले',\n",
       " 'काठमाडौं ? फागुन २६ । स्पेनिस क्लव रियल मड्रिड च्याम्पियन्स लिग फुटबलको क्वार्टरफाइनलमा प्रवेश गरेको छ । मंगलबार राति भएको च्याम्पियन्स लिगको अन्तिम १६ को दोस्रो लेगमा रियल मड्रिडले रोमालाई २० ले हरा',\n",
       " 'काठमाडौं ? फागुन २५ । भरतमा आजवाट सुरु भएको आइसिसी ट्वेन्टीट्वेन्टी विश्वकप क्रिकेटमा जिम्बावेले बिजयी सुरुवात गरेको छ । ओपनिङ खेलमा आज जिम्बावेले हङकङलाई १४ रनले हराएको छ । जिम्बावेले दिएको १५९ रनको ल',\n",
       " 'एजेन्सी ? फागुन २५ । आइसिसी ट्वेन्टीट्वेन्टी विश्वकप क्रिकेट आजदेखि भारतमा सुरु हुँदैछ । प्रतियोगिता अन्तर्गत मंगलबारदेखि पहिलो चरणको खेल हुदैछ । अन्तर्राष्ट्रिय क्रिकेट परिषद आइसिसीको आयोजनामा हुन लाग',\n",
       " 'एजेन्सी ? फागुन २४ । स्पेनिस ला लिगाको शीर्ष स्थानमा रहेको बार्सिलोनाले ८ अंकको अग्रता कायमै राखेको छ । आइतबार राति भएको खेलमा एइबरलाई ४० ले हराउँदै बार्सिलोनाले एथ्लेटिको मड्रिडसँगको अग्रता कायमै राखे',\n",
       " 'एजेन्सी ? फागुन २४ । इंग्लिस प्रिमियर लिग फुटबल अन्तर्गत आइतबार राति भएको खेलमा म्यानचेस्टर युनाइटेड पराजित हुँदा १० खेलाडीमा सिमित लिभरपु भने विजयी भएको छ । म्यानचेस्टर युनाइटेड वेष्ट ब्रोमसँग १० ले',\n",
       " '२४ फागुन ? एजेन्सी । एशिया कपको उपाधि भारतले जितेको छ । बंगलादेशलाई उसकै घरेलु मैदानमा ८ विकेटले हराउँदै गएराती भएको खेलमा भारतले एशिया कप क्रिकेटको उपाधि जितेको हो । फाइनलमा भारतले बंगलादेशलाई ८ व',\n",
       " '२४ फागुन ? चितवन । जारी पद्मोदय अन्तर्राष्ट्रिय पुरुष भलिबल प्रतियोगिताको दोस्रो दिन आज ४ खेल हुँदैछन् । चितवनको पटिहानीमा जारी खेलमा दुई समूहमा विभाजित ६ वटा टिमले दिनभर समूह चरणमा ४ खेल खेल्दैछन् । ',\n",
       " 'एजेन्सी ? फागुन २३ । एशिया कप क्रिकेटको उपाधि भिडन्त आज हुदैछ । फाइनलमा आयोजक बंगलादेशले भारतको सामना गर्दैछ । यी दुई देश बीच एशियाकपको फाइनलमा पहिलो पटक भिड्न लागेका हुन् । खेल मीरपुरमा नेपाली समयअनु',\n",
       " 'एजेन्सी ? फागुन २३ । इंग्लिस प्रिमियर लिग फुटबलमा शिर्ष स्थानमा रहेको लेष्टर सिटीले अग्रतालाई फराकिलो पारेको छ । शनिबार राती भएको खेलमा वाट्फोर्डलाई १० ले हराँउदै लेष्टरले अग्रता फराकिलो पारेको हो । त',\n",
       " 'अमेरिकाले उत्तर कोरियाली नेता किम जोङ उनको हत्या योजना बनाएको खुलासा भइरहँदा उत्तर कोरियाले भने अमेरिकामाथि अहिलेसम्मकै खतरनाक आक्रमणको योजना बनाएको खुलासा भएको छ । अमेरिकी गुप्तचर अधिकारीहरुले उत्तर कोरियाले अन्तरिक्षबाटै अमेरिका ? जापान सहित का आफु बिरुद्ध लाग्ने देशहरुमाथि आणविक आक्रमण गरेको दाबी गरेका छन् । कोरियाले इलेक्ट्रोनिक पल्स हतियारलाई अमेरिकाविरुद्ध प्रयोग गर्ने सम्भावना रहेको भन्दै अमेरिकी गुप्तचर अधिकारीहरुले उत्तरले यसको तयारी गरिरहेको दाबी समेत गरेका छन् । उत्तर कोरियाले अन्तरिक्षमा दुई उपग्रह पठाएको र अन्तरिक्षबाटै अमेरिका र जापानमाथि निगरानी गरिरहेको पाइएको उल्लेख गरेका छन् । स्मरणीय छ उत्तर कोरियाकाले सन् २०१२ र २०१६ मा अन्तरिक्षमा दुई उपग्रहको सफल प्रक्षेपण गरेको थियो । जुन अत्याधुनिक अन्तरिक्ष उपग्रहले केवल ९४ मिनेटमा पृथ्वीलाई एक फन्को लगाउन सक्ने क्षमता राख्दछ । अमेरिकी गुप्तचर अधिकारीहरुले यहि उच्च क्षमताको उपग्रहकै मद्दतबाट उत्तर कोरियाले अन्तरिक्षमै हाइ अल्टिच्यूड परमाणु हतियार विस्फोट गराएर अमेरिकामाथि साइबर हमला गर्न सक्ने देखिएको बताएको छ । उत्तर कोरियाले सबै संचार उपकरणलाई ध्वस्त बनाएर हावी हुने प्रयास स्वरुप अन्तरिक्षमा परमाणुयुक्त यान विकास गरेको अमेरिकाको होमल्याण्ड सेक्युरिटी र नेशनल इलेक्ट्रोनिक्स पल्स टास्क फोर्सका कार्यकारी निर्देशक डा पिटर विनसेन्ट प्राइलाई उदृद्ध गर्दै विभिन्न अन्र्तराष्टिय समाचार संस्थाहरुले उल्लेख गरेका छन् । पाल्पाका महिलाहरु हस्तकलाबाट आत्म निर्भर बन्दै गएका छन् । हात ? सिप र कलाको प्रयोग गरी बच्चाका लागि खेलौना ? गुडिया ? घर ? कार्यालय सजावटका लागि कुसन ? चुरा ? ऐना ? फूल लगायका सामग्रीहरु व्यवसायिक रुपमा उत्पादन गर्न थालेपछि जिल्लाका महिलाहरु आत्मनिर्भर बन्दै गएका हुन् । हातले बनाएका सामाग्री पाल्पा घुम्न आउने पर्ययटकको रोजाई बन्दै गएको छ । पाल्पामा मात्रै दुई दर्जन बढी महिलाहरुले हस्तकलाको पेसालाई व्यवसायिक रुपमा संचालन गरेका छन् । दोभान२ पाल्पाकी हरिकला थापा मगरले हस्तकलाबाटै अहिले मासिक ४० देखि ५० हजारसम्म बचत हुने गरेको बताउनुभयो । झण्डै ४ सय बढीलाई तालिम दिनुभएका हरिकलाले आफ्ना उत्पादित सामग्रीहरु जिल्लामैै बढी खपत भएको बताउनु भयो । सुरुमा समय र लगानीको जोखिम रहेको बताउने हरिकला भन्नुहुन्छ ? अहिले ग्राहकको समस्या छैन ? ढुक्कले लगानी गरेका छौ । घरेलु सामानको उत्पादन र बिक्रीबाट राम्रो कमाई गर्नुभएकी तानसेन वडिज्ञान टोलकी देवी सारुले दुई चार पैसाका लागि परिवारसग हात थाप्नु नपर्ने बताउनुहुन्छ । हस्तकलाको तालिम पछि व्यवसायिक रुपमा सिपको प्रयोग हुन थालेपछि मासिक २०२५ हजार आम्दानी गर्न सफल सारुले पहिले गफ गरेर बिताउने समय अहिले नया नया सिप र प्रबिधिको बिकासमा समय जाने बताउनुभयो । कला र सिपबाट कम खर्चमै आकर्षक सजावटका सामाग्रीबाट नगद आर्जन हुने भएकाले पनि हस्तकलाका सामग्री उत्पादन र बिक्रीमा महिलाको आकर्षण बढ्दो छ । महिलालाई व्यवसायिक र अत्मनिर्भर बनाउन हस्तकला ? सिलाइकटाइ ? व्युटिपार्लर ? ढाका ? मोवाइल मर्मत लगायतका तालिमहरु संञ्चालन गर्दै आएको बताउदै घरेलु तथा साना उद्योग कार्यालय पाल्पाका प्रमुख बालकृष्ण गैरेले हस्तकलाको उत्पादन र प्रयोगमा पनि बृद्धि आएको बताउनुभयो । आर्थिक अवस्था कम्जोर भएका महिलाहरुका लागि मेशिन लगायतका आवश्यक औजार समेत वितरण गरेको जानकारी दिदै कार्यालय प्रमुख गैरेले हस्तकलाको व्यवसायिक उत्पादन र बिक्रीमा ध्यान दिनुपर्नेमा जोड दिनुभयो । नेपालमा सन् २००१ देखि विद्युतिय कार्ड भूक्तानी संचालन गर्दै आएको स्मार्ट च्वाइस टेक्नोलोजिस एससीटीले एससीटी मोको मोवाइल एप्लिकेशन सुरु गरेको छ । आइतबार राजधानीमा पत्रकार सम्मेलन आयोजना गरि आफ्ना ग्राहकहरुको सहजताका लागि फोकसवन पेमेन्ट सोलुसनसँगको सहकार्यमा सेवा सुरु गरेको जानकारी गराइएको हो । एसिटी कार्ड प्रयोगकर्ताहरुले आइफोन ? र एन्ड्रोइड स्मार्टफोन मार्फत विभिन्न साइट वा अफ साइट वित्तिय कारोवार गर्न सक्ने कम्पनीले जनाएको छ । एप्लिकेशनमा इन्टरनेट प्रयोग गरिने हुँदा एसएमएसको आवश्यकता नर्पनुका साथै संसारको कुनै पनि कुनामा यो सेवा प्रयोग गर्न सकिने छ । एससिटी नेटवर्कमा ७८ बैंक तथा वित्तिय संस्थान आवद्ध छन् । देशको विभिन्न जिल्लामा गरि १६ सय एटीएम र तीन हजार पिओएस सेवा उपलब्ध छन् । हङकङको एप्पल न्युजका अनुसार जोर्डनस्थित वुसुङ्ग स्ट्रीटमा शुक्रवार बिहान तीन नेपाली आपसमा भिडेका हुन् । १६ ? २१ र २२ वर्षीय नेपालीबीच छुरा हानाहान गरेको देखेपछि बटुवाले प्रहरीलाई खबर गरेका थिए । घाइतेको अस्पतालमा उपचार भइरहेको छ । समाचारका अनुसार हात ? टाउको र घाँटीमा छुराको चोट लागेको छ । फोटो  भिडियो सौजन्यः हङकङ एप्पल नेस्टमिडिया',\n",
       " 'उपभोक्तालाई ठगेको आरोपमा २० दिनअघि सिल गरिएका दरबारमार्गका विभिन्न व्यवसायिक फर्मलाई बिनाकारबाही छाडिएको भन्दै उपभोक्तावादीले विरोध जनाएका छन् । मन्त्रीको संख्या अत्यधिक भएर जनता आजित भइरहेका बेला संसद्को राज्यव्यवस्था समितिले अब केन्द्र सरकारमा बढीमा १५ वटा मात्रै मन्त्रालय राख्न सरकारलाई निर्देशन दिएको छ । समितिको निर्देशनअनुसार केन्द्र सरकारमा रहेका १६ वटा मन्त्रालय खारेजीमा पर्ने भएका छन् । हाल प्रधानमन्त्री तथा मन्त्रिपरिषद् कार्यालयसहित ३१ वटा मन्त्रालय छन् । यो खबर हामीले आजको ',\n",
       " 'स्थानीय तहमा खटिएका कर्मचारी कार्यालयमा नपुगेपछि उनीहरूको खोजी गर्न सरकारले आइतबार उच्चस्तरीय अनुगमन टोली गठन गरेको छ । प्रधानमन्त्री तथा मन्त्रिपरिषद् कार्यालयले संघीय मामिला तथा स्थानीय विकास मन्त्रालयका सहसचिवको नेतृत्वमा सामान्य प्रशासन मन्त्रालयका उपसचिव र महालेखा नियन्त्रक कार्यालयका उपसचिव सदस्य रहेको अनुगमन टोली गठन गरेको हो । काठमाडौँ ? रेखा कवि टंक सम्बाहाङ्फेको एकल कविता वाचन कार्यक्रम सम्पन्न भएको छ । राजधानीको अनामनगरस्थित मण्डला नाटक घरमा उनले पहिलो पटक एकल कविता वाचन गरेका हुन् । सन् २०२२ मा कतारमा हुने विश्वकपमा नेपाली मूलकी भाग्यमानी एक युवतीले रेफ्री बन्ने अबसर पाउने भएकी छन् । कतारमा हुने विश्वकपमा सिक्किमकी रेश्मी थापा क्षेत्रीले रेफ्रीको भूमिका निर्वाह गर्ने भएकी हुन् । उनले भारतमा राष्ट्रिय स्तरका फुटबल प्रतियोगितामा रेफ्रीको भूमिका निर्वाह गर्दै आइरहेकी छिन् । नेपालले विश्वकपमा भाग लिन पाउने दिन आउंला नआउला । तर नेपालीभाषी एक चेलीले विश्वकपमा रेफ्रीको भूमिका पाउनु निश्चय नै खुशी र गर्वको विषय हो । एचकेनेपाल डट कम संवाददाता १२ असोज । बडादसैँको आठौँ दिन आज दुर्गाभवानीको पूजा आराधना गरी महाअष्टमी पर्व मनाइँदै छ । महाअष्ठमीको उपलक्ष्यमा आज दसैँघर र कोतलगायत अधिराज्यका विभिन्न शक्तिपीठमा बलिसहित पूजाअर्चना तथा दुर्गा सप्तशती पाठ गरिन्छ । एचकेनेपाल डट कम संवाददाता ११ असोज । बडादशैँ अन्तर्गत सप्तमी तिथिमा आज नेपालभर परम्पराअनुसार घरघरमा फूलपाती भित्राइँदैछ । दुर्गा भवानीको प्रतिमा राखिएको दसैँघरमा फूलपातीको शुभसाइतको प्रतीकका रूपमा उखु ? अदुवा ? केराको बोट ? धानको बाला ? बेलपत्र ? दारिम ? जयन्ती ? अशोकको फूल तथा अन्य नयाँ पालुवासमेत विधिपूर्वक भित्राइन्छ । आफ्ना माग पूरा गर्न सरकारले पहल नगरेको आरोप लगाउँदै चिकित्सकहरूले दसैंमा समेत आन्दोलन जारी रहने बताएका छन् । मुलुकमा भाइबर आउट आइपी सेवाबाट मोबाइल र ल्यान्डलाइन सेवामा फोन गर्नु गैरकानुनी भएको सरकारले बताएको छ । कुनै पनि प्रतिस्पर्धामा प्रायः वैमनश्य हुन्छ । ईर्ष्या अनि डाह हुन्छ । तर ? नेपाल आइडलको फाइनलमा पुगेका बुद्ध लामा ? निशान भट्टराई र प्रताप दास एकअर्कामा दुःखसुख साटासाट गर्छन् । सरकारले निर्वाचन आचारसंहिता कुल्चँदै दसैंको मुखमा आफ्ना कार्यकर्तालाई पोस्न १० करोडभन्दा बढी रुपैयाँ वितरण गरेको छ । गत २ असोजको मन्त्रिपरिषद्को बैठकले प्रधानमन्त्री ? गृहमन्त्री तथा विभिन्न मन्त्री र कांग्रेस ? माओवादी केन्द्रका उच्च नेता निकटका ८ सय १९ जनालाई १० करोडभन्दा बढी रकम वितरण गरेको प्रधानमन्त्री तथा मन्त्रिपरिषद् कार्यालय उच्च स्रोतले जानकारी ',\n",
       " 'पर्सा ? फागुन २३ गते । हुँदै नभएको जग्गा बिक्री गरी ठग्ने एकै गिरोहका चारजनालाई ठगी गरेको अभियोगमा पर्सा प्रहरीले पक्राउ गरी शनिबार सार्वजनिक गरेको छ । वीरगन्ज उपमहानगरपालिका वडा नं ८ गीतामन्दिर रोडमा सात धुर जग्गा देखाई रु ८० लाखमा बिक्री गरी ठगी गरेको अभियोगमा मालपोत कार्यालयका नासु मुरारीप्रसाद तिमल्सिना ? जग्गाधनी बाराको कचोर्वा गाविस घर भइ हाल वीरगन्ज८ बस्दै आएकी इन्द्रावती देवी गुप्ता ? उनका श्रीमान् सन्दीप गुप्ता र सहयोगी राजेश गुप्तालाई पक्राउ गरिएको पर्साका प्रहरी उपरीक्षक राजुबाबु श्रेष्ठले जानकारी दिनुभयो । जिल्ला प्रहरी कार्यालय पर्साले पत्रकार सम्मेलनको आयोजना गरी वीरगन्जको गीतामन्दिर रोडमा हुँदै नभएको सात धुर जग्गा बिक्री गरी ठगी गरेको अभियोगमा मालपोत कार्यालय पर्साका नायब सुब्बासहित चारजनालाई पक्राउ गरी सार्वजनिक गरेको हो । उहाँले जग्गाको कीर्ते प्रमाण खडा गरी काठमाडौँ ? ललितपुर वडा नं २१ निवासी सचेन्द्र महर्जनसँग रु ८० लाखमा बिक्री गरी राजीनामा नै पास गरेकोे निवेदन महर्जनले प्रहरीलाई दिएको आधारमा अनुसन्धान गर्दा भूमाफियाहरुको गिरोह नै पक्राउ गर्न सफल भएको उपरीक्षक श्रेष्ठले बताउनुभयो । नक्कली कागज प्रमाणका आधारमा वीरगन्जको प्राइम बैंकमा हुँदै नभएको जग्गा धितो राखी रु ४० लाख झिकेकी इन्द्रावतीले पछि महर्जनसँग बिक्री गरेको र सो कार्यमा मालपोत कार्यालय तथा बैंकका कर्मचारीसमेत संलग्न रहेको अनुसन्धानबाट देखिएको उपरीक्षक श्रेष्ठले जानकारी दिनुभयो । उहाँले जग्गा ठगी प्रकारणमा मालपोत कार्यालयको कागजपत्र सच्याइएको र विधिवत् रुपमा राजीनामा समेत पास भएको देखिएकाले यस काममा अन्य कर्मचारीसमेत संलग्न रहेको अनुसन्धानबाट दोषी देखिएका अन्य कर्मचारीको खोजीकार्य भइरहेको बताउनुभयो । पक्राउ परेका अभियुक्तमाथि कीर्ते र ठगी दुवै मुद्दा चलाउने तयारी भएको प्रहरीले जनाएको छ । रासस',\n",
       " 'दक्षिण एसियाको रुपान्तरणका लागि निराशालाई आशामा र नकारात्मकताबाट सकारात्मकतातर्फ लागौँ प्रधानमन्त्री मोदी',\n",
       " 'दिपायलसिलगढी नगरपालिकाका नगरप्रमुख मञ्जु मलासी सुदुरपश्चिम प्रदेशमा एक जना मात्र महिला नगरप्रमुख हुनुहुन्छ । तत्कालीन नेकपा एमालेको तर्फबाट निर्वाचित मेयर',\n",
       " 'बैतडी ? असार १६ गते । बैतडीको मेलौली नगरपालिका१ को वडाध्यक्षमा नेपाली काँग्रेसका सुरतसिंह नेगी ६३० मतका साथ विजयी हुनुभएको छ । उहाँका निकटतम प्रतिद्वन्दी नेकपा एमालेका शरदसिंह नेगीले २७१ मत प्राप्त गर्नुभयो । मेलौली१ का सदस्यमा पनि नेकाकै उम्मेदवार विजयी भएका छन् । द्रोपती चन्द ४१८ ? द्रोपती दयाल ४४१ ? चिन्तासिंह बिष्ट ४४६ र शङ्करसिंह नायकले ४१८ मत ल्याएर विजयी हुनुभएको छ । यसैगरी ? बैतडीको पाटन नगरपालिका१० को अध्यक्षमा नेकपा एमालेका देवेन्द्र कुँवरको प्यानल नै विजयी भएको छ । निकटतम प्रतिद्वन्द्वी नेकाका हरिदत्त भट्टलाई १३३ मतान्तरले पछि पार्दै कुँवर विजयी भएका हुन् । विजयी कुँवरले ३८१ मत प्राप्त गरेका छन् । त्यस्तै ? महिला सदस्यमा जयन्ती पन्त ? दलित महिला सदस्यमा बिमला सार्की र सदस्यहरुमा नरबहादुर महरा तथा रमेशप्रसाद पन्त विजयी भएका छन् । रासस',\n",
       " 'बेलुकी करिब ८ बजे तिर संखुवाखोलामा आएको बाढिले ताम्कु ७ र बाला ६ जोड्ने पुल ? ताम्कु ८ र बाला ८ जोड्ने ? ताम्कु ९ र बाला ९ जोड्ने पुल बगाएको हो । त्यसै गरि पावाखोला ४ ? ५ ? ६ र ७ ? ८ ? ९ जोड्ने चासुवाटारको एक सय मिटर लामो झोलुङ्गे पुल बाढिले बगाएको छ । पावाखोला गाविस कै ईरखुवा खोलामा बन्दै गरेको पुल समेत बगाएको गाविस सचिब महेन्द्र पराजुलीले जानकारी दिनुभयो । बाढिले चैनपुर नगरपालिका र जलजला गाविस हुदै बग्ने हेवाखोलाको पुल समेत बगाएको छ । हेवाखोलाको मुहान क्षेत्रको बाँदरमुखे भीरमा ठुलो पहिरो गएपछि जलजला १ र चैनपुर १ जोड्ने पुल ? जलजला १ कै टाँडेबाट चैनपुर १ तर्ने पुल र आसपासको ४ वटा पुलहरु बगाएको छ । संखुवासभा ? जेठ २९ गते । बुधबार राती परेको बर्षा पछि खोलामा आएको बाढिले जिल्लाका करिब दर्जन भन्दा बढि झोलुङ्गे पुल बगाएको छ । बाढि पहिरोले भौतिक संरचनामा समेत क्षति पुगेको छ । संखुवाखोलामा आएको बाढिले ताम्कु७ मा रहेको खोङ्लुवा खोला लघु जलविद्युत आयोजनाको पावर हाउस समेत बगाएको छ । खाँदवारी १३ मा रहेको क्रसर मेसिनमा पनि सभाखोलामा आएको बाढीले क्षति पुराएको छ । बाढी कै कारण चैनपुर र वाना बिचमा रहेको अरुण हाईड्रोपावरमा पनि क्षति पुराएको छ । नुम १ मा रहेको ईन्दुवा खोलाको छेउछाउमा पहिरो गएर क्षति पुराएको स्थानियले बताएका छन् । पावाखोलामा रहेको कारमाराङ खोलाले धार परिवर्तन गरेर सिरुटार बस्तिमा पसेको स्थानिय बसन्त कोरङ्गी गुरुङले जानकारी गराउनुभयो । गाउँ नै बाढि पसेपछि अहिले सिरुटार बासीले गाउँ छोडेर अन्यत्र नै बस्दै आएका छन् । जिल्लाको प्राय सवै क्षेत्रमा बाढि पहिरो आए पनि मानविय क्षति भने नभएको जिल्ला प्रहरी कार्यालयले जनाएको छ । चीनका राष्ट्राध्यक्ष सी चिन फिङ र श्रीमती फङ ली युआनले ४ तारिख बेलुकी शाङ हाइमा स्वागत रात्री भोजको आयोजना गरी पहिलो चीन अन्तर्राष्ट्रिय आयात एक्स्पोमा उपस्थित हुनुभएका विभिन्न मुलुकका अतिथिहरुलाई स्वागत गर्नुभएको छ । सो अवसरमा सी चिन फिङले सम्बोधन गर्नुहुँदै चीन अन्तर्राष्ट्रिय आयात एक्स्पो चीनको हो र साथै विश्वको पनि हो भन्दै यो सामान्य प्रदर्शनी नभएको र चीनले अर्को चरणको उच्च स्तरीय सुधार तथा खुलापन बढाउनका लागि गरेको महत्वपूर्ण निर्णय हुनुका साथै चीनले विश्वलाई सक्रिय रुपमा बजारलाई खुला गर्ने ठूलो कदम भएको दोहोर्याउनुभएको छ । चीन अन्तर्राष्ट्रिय आयात एक्स्पोले शाङ हाइको शानलाई अझै चम्काउनसक्ने विश्वास रहेको पनि उहाँले उल्लेख गर्नुभयो । सो रात्री भोजमा सी चिन फिङले विभिन्न मुलुकका नेतासँग हार्दिक आदानप्रदान गर्नुभएको छ । अतिथिहरुले पहिलो चीन अन्तर्राष्ट्रिय आयात एक्स्पो सफलताका साथ सम्पन्न हुने र विभिन्न मुलुक सँगसँगै मिलेर विकास गरी विश्वलाई शान्तिमय र समृद्ध पार्ने इच्छा व्यक्त गरेका छन् । धर्म संस्कृति मान्ने महिलाहरूले लगाउने मेहन्दी र चुरा पछिल्लो समय प्राय सबै जसो महिला र युवतीको लगाउने गरेका छन् । अहिले त मेहन्दी र चुरा फेसनकै रूपमा स्थापित भएको पाइन्छ । रङ्गीचङ्गी पहिरनसँगै बजारमा चुरा ? मेहेन्दी र कपडाहरूको व्यापार बढेको छ । साउनमा विशेष रूपमा मनाउने सोमबारको व्रत बस्ने हरियो ? पंहेलो रङ्गको चुराका साथै मेहन्दी र हरियो रातो कपडा र श्रृङ्गारमा सजिने गर्छन् । साउन महिना आएलगत्तै हरियो चुरा र महेन्दीलगायत अन्य श्रृङ्गारिक सामानको बिक्री राम्रो भइरहेको छ । अन्य समयमाभन्दा साउनमा यस्ता सामग्रीको कारोबारबाट मनग्य आम्दानी हुँदै आएको व्यापारीको भनाई छ । प्रकृति मात्र नभई साउनसँग हरियालीको विशेष सम्बन्ध र सामिप्यता रहन्छ । यही रङ्ग भित्र उपस्थिति जनाउने एउटा महत्त्वपूर्ण पक्ष मेहन्दी पनि हो । विशेष हिन्दु नारीहरूले सुन्दरता र सौभाग्यका लागि साउन महिनाभर प्रयोग गर्ने मेहन्दीको गुनिलो पक्ष बढी भए पनि बजारमा जथाभाबी पाइने मेहन्दीले स्वास्थ्यमा पनि उत्तिकै हानी पुयाउने गरेको छ । कतिपय ठाउँमा मेहन्दीलाई डार्क बनाउनका लागि विभिन्न केमिकल ? पेट्रोल ? एसीड लगायतका वस्तु राखिन्छ जुन मानिसको स्वास्थ्यको लागि घातक हो । यस्ता मेहेन्दीको प्रयोगले छाला पोल्ने छालाको एलर्जी देखि क्यान्सरसम्मको रोग लाग्ने खतरा बढी हुन्छ । सस्तो मूल्यमा पाइयो भन्दैमा जस्तोसुकै मेहन्दी लगाउँदा मानिसको स्वास्थ्यमा गम्भीर असर परिररहेको छ । त्यसैले मेहन्दी लगाउँदा प्राकृतिक मेहेन्दी प्रयोग गर्न चिकित्सकहरूको सुझाव छ । नेपालमा मेहन्दी व्यापारका लागि न कुनै मापदण्ड छ ? न कुनै अनुगमन नै । जसले ? जहाँबाट जस्तोसुकै मेहन्दी ल्याएर बेचे पनि कसैले केही गर्दैन । नेपालमा कस्मेटिक्सको मापदण्ड र नियगमन गर्ने कुनै निकाय छैन । यस्तो विधिहिनताको पीडामा नेपाली नारीहरू हरेक साउनसँगै शरीरमा घातक रोग सङ्क्रमणको पीडित भइरहेका छन् । छालामा रोग प्रतिरोधी क्षमता बढाउन पनि उत्तिकै मद्दत गर्छ । त्यस्तै मेहन्दीको सेवन उच्च रक्तचापका बिरामीहरूको लागि वरदान मानिन्छ भने कपालको रक्षाका लागि मेहन्दीलाई रक्षाकवच नै ठानिन्छ । तर अब स्वास्थ्य प्रयोगकर्तामा नै निर्भर छ । मेहन्दीका प्रयोगकर्ताहरूले सतर्कता अपनाए मेहन्दी स्वास्थ्यका लागि हो भने जथाभाबीको प्रयोग गर्नेको लागि खतराको खानी पनि हो ।  तरकारीवाला बालकको फोटो बन्यो भाइरल ? यी बालकको खोजीमा लागेको छ पुरै सरकारपूरा पढ्नु होस ।  अरुणखोलामा सवारी दुर्घटना ? २४ यात्रु घाइते',\n",
       " 'पारिवारिक चलचित्र अनुग्रहले कार्तिक ३०गते बाट देशभरका हलहरुमा धमाका पिट्दैहेर्नुहोस् भिडियो सहित',\n",
       " 'पवन पौडेलसानफ्रान्सिस्को  आइरिस् आयरल्यान्डका बासिन्दा र स्कटिस् स्कटल्यान्डका बासिन्दाले यो चाड उत्तर अमेरिका ल्याएका हुन् । यो चाड मनाउने देशको संख्या बढ्दै गैरहेको छ र नेपाली समाजले पनि यस चाडलाई मनाउदै आइरहेको छ । नेपालीहरुको चाड दशै ? तिहार ? गाइजात्रा र गठे मंगल सँग यो फेस्टिवलको गतिबिधीहरु मिल्न जुल्न आउछन । बालबालिकाहरुले काला ? सेता बस्त्र ? भूत पर्यटक ? होलिका ? भोजमा लगाईने बिशेष पोशाकमा घर दैलोमा आई मिठाई मागी मनोरंजन गर्नु यो फेस्टिभलको गतिबिधी अन्तर्गत पर्दछ । गर्मीलाई बिदाई गर्न यो चाड मनाइन्छ । यो चाड पश्चिमा देशहरू संयुक्त राज्य अमेरिका ? क्यानडा ? स्कटल्यान्ड ? न्युजिल्यान्ड आयरल्यान्ड ? संयुक्त अधिराज्य ? जर्मनी जस्ता देशमा बढी मात्रामा मनाइन्छ । हलोविन वा ह्यालविन    को छोटकरी रुप हो । यो आत्मा दिवस को पूर्व सन्ध्या हो भने सन्यासी दिवसको नामले पनि जानिन्छ  विश्वका धेरै देशहरूमा प्रत्येक वर्ष अक्टोबर ३१ मा मनाइने पर्व हो । यो चाड इसाई धर्ममा ज्यादा प्रचलित छ । विभिन्न भूतप्रेतको पोसाकमा आएका बालबालिकालाई घर धनीले झोला लिएर आएकालाई विभिन्न प्रकारका मिठाई अजुली भरि दिने गर्दछन  हेलोविन खेल्न आउनेलाई केवल मिठाई मात्र दिनु पर्छ  यसरी बालबालिका खेल्न जाँदा उनीहरुको सुरक्षा तथा मार्ग दर्शन गर्न उनीहरुको साथमा अभिभावक पनि आउने गर्दछन ',\n",
       " 'बिगत २ वर्षदेखि हलोविन नाइटको आयोजना गर्दै आएको एस बाबु प्रोडक्सनले यसपाली पनि हलोविन नाइट ३ कार्यक्रमलाइ निरन्तरता दिएको छ । बिशेष गरि अमेरिकामा जन्मेका दोस्रा पुस्ता र बिभिन्न उदेश्य लिएर अमेरिका आएका फरक फरक उमेर समुहले यस पर्बलाई पछील्लो केही समय यता मनोरन्जनका साथ मनाउन थालेका छन् । अक्टोबर २७ तारिक शनिबारका दिन    ?   ?   ?      मा हुँदै गरेको कार्यक्रममा बे एरियाका नेपाली डी जेहरु ? डी जे एस बाबु ? डी जे काले का साथ् साथै अमेरिकन डी जे रोजको पनि जोडदार प्रस्तुति रहने जानकारी एस बाबु प्रोडक्सनका प्रमुख सन्दिप बाबुले दिनु भएको छ  फोटो बूथ र खाना पनि कार्यक्रमको आकर्शणको रुपमा रहनेछन ',\n",
       " 'टोकियो ? जापान ? २३ असार राससएएफपी  अमेरिकी विदेशमन्त्री माइक पोम्पेओले उत्तर कोरियाको दुई दिने भ्रमणका क्रममा उत्तर कोरियाली समकक्षीसँग भएको वार्ता प्योङयाङलाई आणविक रूपमा निशस्त्र बनाउन निकै लाभकारी भएको शनिबार बताउनुभएको छ । उत्तर कोरियाको भ्रमणपछि टोकियो प्रस्थान गर्नुपूर्व सञ्चारकर्मीहरूसँगको कुराकानीमा उहाँले कसरी उत्तर कोरियाले आणविक निशस्त्रीकरणबारे आफ्नो प्रतिबद्धतालाई सम्मान गर्न सक्छ भन्ने बिषयमा आफूहरूले केही नयाँ विवरणहरू आदानप्रदान गरेको बताउनुभयो । उहाँले भन्नुभयो ? यी पक्कै पनि जटिल बिषयहरू हुन् ? तर हामीले सवैखाले प्रमुख मुद्दामा प्रगति गरिसकेका छौँ । केही कुरामा निकै नै प्रगति भएको छ भने अन्यमा पनि सफलताका लागि थप कामहरू भइरहेका छन् । ',\n",
       " 'यसैबीच ? दक्षिण कोरियाली समाचार संस्था योन्हापले बताए अनुसार वासिङ्टनका सर्वग्राही माग र अति नै अफसोसजनक धारणाको आलोचना गरेको छ । पोम्पेओसँगको वार्ता सकिएको केही घण्टामै आएको हो । उत्तर कोरियाली विदेश मन्त्रालयद्वारा जारी विज्ञप्तिलाई उद्धृत गर्दै योन्हापले जनाएको छ ? उच्च स्तरीय वार्तामा अमेरिकी धारणा र अडान निकै अफसोसपूर्ण पाइयो । न्यूज अभियान डट कमको एन्ड्रोइड एपका लागि यहाँ क्लिक गर्नुहोस् । हरेक खबर विशेष खबरका लागि अफिसियल फेसबुक र ट्वीटर मार्फत जोडिनुहोस् । काठमाडौं  सरकारले सहरी गरिब ? सुकुम्बासी तथा न्यून आय भएका परिवारका लागि बनाइएको अपार्टमेन्टमा मुख्यमन्त्री र प्रदेश प्रमुखलाई राख्ने तयारी गरेको छ । नागार्जुन नगरपालिकाको इचंगुनारायणमा २०७० मा निर्मित अपार्टमेन्टमा मुख्यमन्त्री र प्रदेश प्रमुखको अस्थायी आवास बनाउन लागिएको हो । आजको कान्तिपुरको पृष्ठ ३ मा यो समाचार छापिएको छ । केही दिनअघि गृहसचिव प्रेमकुमार राई ? सहरी विकास सचिव दीपेन्द्रनाथ शर्मा ? प्रधानमन्त्रीका प्रमुख सल्लाहकार विष्णु रिमाल र उपत्यका विकास प्राधिकरणका प्रम',\n",
       " 'हे परमप्रभु ? हामी साँचो मनले तपाईंलाई पूज्दछौं यसर्थ हामीलाई तपाईंको महान् प्रेम दर्शाउनुहोस् । दाऊदको एउटा भजन । म परमेश्वरको संगत पाउनलाई उहाँकहाँ गएँ । अनि उहाँले मलाई उत्तर दिनुभयो । मेरा सब डरहरूबाट उहाँले मलाई बचाउनु भयो । यस दुःखी मानिसहले उहाँको पाउनलाई परमप्रभुलाई बोलायो । अनि परमप्रभुले मेरो कुरा सुन्नुभयो । उहाँले मलाई सबै संकटहरूबाट बचाउनुभयो । परमप्रभुको र्स्वगदूतले जुन मानिसहरूले परमप्रभुको अनुशरण गरे ? उनीहरूको चारैतिर छाउनी बनाउँछ अनि तिनीहरूलाई बचाउँछन् । स्वर्गदूतहरूले तिनीहरूको रक्षा गरे । परमप्रभुलाई जाँच गरेर हेर ? उहाँ कति असल हुनुहुन्छ । जुन मानिसले उहाँमाथि आश्रय राख्दछ ऊ अवश्य खुशी हुनेछ । परमप्रभुका पवित्र भक्तजनहरू उहाँको उपासना गर । उहाँ बाहेक परमप्रभुका भक्तजनहरूका लागि अरू कुनै सुरक्षित ठाउँ छैन । बलवान मानिसहरू कमजोर र भोका हुनेछन् । तर जुन मानिसहरू सहयोग माग्न परमेश्वर कहाँ जान्छन् तिनीहरू मध्ये प्रत्येकले असल थोकहरू पाउनेछन् । नराम्रा कामहरू गर्नदेखि आफूलाई रोक राम्रो ? असल कुराहरू मात्र गर । शान्तिको पछाडी दगुर अनि त्यसलाई समात । तर परमप्रभु ती मानिसहरूका विरूद्ध हुनुहुन्छ जसले नरम्रा कामहरू गर्छन् । उहाँले तिनीहरूलाई पूर्ण प्रकारले ध्वंश पारिदिनु हुन्छ',\n",
       " 'जब कष्टले सताँउछ केही मानिसहरू नम्र हुन्छन् । परमप्रभु तिनीहरूको नजिक हुनुहुन्छ । अनि उहाँले तिनीहरूलाई बचाउनु हुन्छ । धार्मिक मानिसहरूका धेरै समस्याहरू हुन सक्छन् ? तर परमप्रभुले तिनीहरूका प्रत्येक समस्याबाट बचाँउनु हुनेछ । यस रिचमण्ड फेलोशिप नेपाल डिएडिक्सन सेन्टर हालको लागि चोभारमा नै रहेको रिचमण्ड फेलोशिप नेपाल मेल सेन्टरमा मोर्डन ईन्डियन स्कूल सँगै सारेको व्यहोरा जानकारी गराउँदछौँ ।  कोसेली पुयाउन गएका आफ्नै साथीको हत्या गर्ने एक नेपाली युवालाई साउदीमा मृत्यु दण्डको सजाय',\n",
       " 'पीडित परिवारलाई ब्लडमनि लिएर माफी दिन स्थानीय अधिकारीहरुले समेत गरेको प्रस्ताव अस्वीकार गरेपछि साउदीको अदालतले एक नेपालीलाई मृत्युदण्डको सजाय सुनाएको छ । यो फैसलालाई साउदी प्रशासनले जुन सुकै बेला कार्यान्वयन गर्न सक्नेछ । एउटै कम्पनीमा कार्यरत भएको बेला कामीको कोसेली लिएर कुश्मेली पर्वत पुगेका थिए । बिदाबाट फर्के लगत्तै आबेगमा आएका कामीले कुश्मेलीको हत्या गर्न पुगेका थिए । उसैको कोसेली पुर्याउन घर गइदिनुछ । फेरि उसैबाट मारिएको छ ?  पीडित परिवारको भनाइ उद्धत गर्दै दूतावासका एक अधिकारीले भने ? हामी कसरी माफी दिन सक्छौं ? ']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter_first[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ffe64cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 35000 5000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter_first),len(train_iter_second),len(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ee116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer(None)\n",
    "\n",
    "# vocab = build_vocab_from_iterator(\n",
    "#     map(tokenizer, train_iter_first), specials=['<unk>'])\n",
    "# vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "\n",
    "# Save for first time\n",
    "# with open('transformer_vocab.pickle','wb') as f:\n",
    "#     pickle.dump(vocab,f)\n",
    "\n",
    "with open('transformer_vocab.pickle','rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32332624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long)\n",
    "            for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dc61642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "14e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test Split\n",
    "\n",
    "train_data = data_process(train_iter_second)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d0ca4903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19020,   1025,    511,    761,  24762,    827, 103693,      1,   3726,\n",
       "            32,  11495,      3,    792,      1,   1374,     32,   2702,    324,\n",
       "             2,    107,     14,    330,  24336,     77,   5527,   8387,  28118,\n",
       "        123255,  11037,     33,   6619,    480,     14,  34020,   3207,     13,\n",
       "        176772,  44337,    640,   2987, 314958,   4492,   1852, 316978,    436,\n",
       "         20999,    130,      3, 321145,      9, 151169,      4,      1,    950,\n",
       "           305,  67904,  10488,    693,      1,     60,      2,    203,  27756,\n",
       "             1,    387,   3505,   1589,  15993,   9954,    230,  11080,   1190,\n",
       "          1417,     98,   6243,   3793,    783,      7,      1,    387,   3505,\n",
       "          1589,  14473,  16876,   7713,  33868,  40783,  32454,   4344,  47676,\n",
       "          7188,   9612,    230,  11080,   1190,    161,      2,    593,    302,\n",
       "             1])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf00df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342570\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b8f1855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342570\n",
      "35000 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(len(train_iter_second),len(test_iter))\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "torch.cuda.memory_allocated() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e24971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b349f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 35\n",
    "# eval_batch_size = 10\n",
    "\n",
    "# train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "# test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d40be4",
   "metadata": {},
   "source": [
    "# Working with a dummy Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fda3aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Data\n",
    "\n",
    "\n",
    "text = ['आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य']\n",
    "#text = ['जनसंख्या']\n",
    "sample_data = data_process(\n",
    "    text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16c88184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2086,  5694,   568,     0,   897, 28361,     0,   357,   465,   410,\n",
       "         6548,   293,     0,   357,   465])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9de8a21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word: आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2086,   357],\n",
       "        [ 5694,   465],\n",
       "        [  568,   410],\n",
       "        [    0,  6548],\n",
       "        [  897,   293],\n",
       "        [28361,     0],\n",
       "        [    0,   357]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = batchify(sample_data, 2)\n",
    "print(\"Given word:\", text[0])\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "185318b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7540440])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee046362",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 16\n",
    "train_data = batchify(train_data, bptt)  # shape [seq_len, batch_size]\n",
    "test_data = batchify(test_data, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb776cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "def get_batch(source: Tensor, i: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    #target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd7744",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6290c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4619fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "037dbee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "60e6bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf7ec3",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727ab3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7cfdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 300  # embedding dimension\n",
    "d_hid = 800  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.05  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize,nhead, d_hid,\n",
    "                         nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52bed039",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99227194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bcf16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        #print(type(output))\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e875bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "#softmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61169dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_softmax = softmax(output)\n",
    "            output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "            indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "            target_indices = targets.t()\n",
    "            # print(output)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "\n",
    "    return total_loss / (len(eval_data) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "405c2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#            print('data')\n",
    "#             print(list([vocab.lookup_tokens(list(index))\n",
    "#                         for index in data.t()]))\n",
    "#             print(indices)\n",
    "#             print(list([vocab.lookup_tokens(list(index))\n",
    "#                         for index in indices]))\n",
    "#             print(len(targets))\n",
    "#             print(list([vocab.lookup_tokens(list(index))\n",
    "#                         for index in target_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9a140c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # target_indices = targets.t()\n",
    "        # print(indices.shape)\n",
    "        # print(indices)\n",
    "        # temp_gen_data = [vocab.lookup_tokens(\n",
    "        #    list(index)) for index in indices][0][i]\n",
    "        # gen_data = [vocab.lookup_tokens(\n",
    "        #    list(index)) for index in indices][0]\n",
    "        # print(temp_text)\n",
    "        # print([[vocab.lookup_tokens(list(index))\n",
    "        #        for index in indices][0][i]])\n",
    "#         temp_text = [[vocab.lookup_tokens(list(index))]\n",
    "#                       for index in indices[0][i]]\n",
    "        # print(temp_text)\n",
    "#         temp_text = [' '.join(temp_text)]\n",
    "        # print(temp_text)\n",
    "        # gen_data = vocab.lookup_tokens((list(gen_data))) + list(temp_gen_data)\n",
    "        # gen_data = torch.tensor(\n",
    "        #    (list(gen_data[0])+list(indices[0][i]))).unsqueeze(0)\n",
    "        # gen_data = torch.concat([gen_data, torch.tensor(indices[0][i])], dim=1)\n",
    "        #gen_data = ' '.join(gen_data)\n",
    "        #return temp_txt\n",
    "#         gen_data = data_process(temp_text)\n",
    "#         gen_data = batchify(gen_data, 1)\n",
    "        #return gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60119ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73f1d",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f532766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/29454 batches | lr 1.00 | ms/batch 153.47 | loss 10.72 | ppl 45202.67\n",
      "| epoch   1 |   400/29454 batches | lr 1.00 | ms/batch 150.94 | loss  9.61 | ppl 14900.30\n",
      "| epoch   1 |   600/29454 batches | lr 1.00 | ms/batch 151.98 | loss  9.26 | ppl 10511.59\n",
      "| epoch   1 |   800/29454 batches | lr 1.00 | ms/batch 152.24 | loss  9.10 | ppl  8980.99\n",
      "| epoch   1 |  1000/29454 batches | lr 1.00 | ms/batch 152.16 | loss  8.98 | ppl  7973.21\n",
      "| epoch   1 |  1200/29454 batches | lr 1.00 | ms/batch 152.26 | loss  8.79 | ppl  6593.27\n",
      "| epoch   1 |  1400/29454 batches | lr 1.00 | ms/batch 152.27 | loss  8.70 | ppl  5998.29\n",
      "| epoch   1 |  1600/29454 batches | lr 1.00 | ms/batch 153.06 | loss  8.64 | ppl  5667.70\n",
      "| epoch   1 |  1800/29454 batches | lr 1.00 | ms/batch 152.68 | loss  8.64 | ppl  5674.85\n",
      "| epoch   1 |  2000/29454 batches | lr 1.00 | ms/batch 152.50 | loss  8.53 | ppl  5072.71\n",
      "| epoch   1 |  2200/29454 batches | lr 1.00 | ms/batch 152.49 | loss  8.50 | ppl  4928.34\n",
      "| epoch   1 |  2400/29454 batches | lr 1.00 | ms/batch 152.50 | loss  8.56 | ppl  5227.42\n",
      "| epoch   1 |  2600/29454 batches | lr 1.00 | ms/batch 152.59 | loss  8.49 | ppl  4862.73\n",
      "| epoch   1 |  2800/29454 batches | lr 1.00 | ms/batch 152.70 | loss  8.38 | ppl  4363.58\n",
      "| epoch   1 |  3000/29454 batches | lr 1.00 | ms/batch 152.67 | loss  8.55 | ppl  5152.14\n",
      "| epoch   1 |  3200/29454 batches | lr 1.00 | ms/batch 152.60 | loss  8.38 | ppl  4353.19\n",
      "| epoch   1 |  3400/29454 batches | lr 1.00 | ms/batch 152.72 | loss  8.33 | ppl  4153.17\n",
      "| epoch   1 |  3600/29454 batches | lr 1.00 | ms/batch 153.90 | loss  8.29 | ppl  3987.97\n",
      "| epoch   1 |  3800/29454 batches | lr 1.00 | ms/batch 153.41 | loss  8.25 | ppl  3841.13\n",
      "| epoch   1 |  4000/29454 batches | lr 1.00 | ms/batch 152.95 | loss  8.20 | ppl  3657.96\n",
      "| epoch   1 |  4200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  8.21 | ppl  3680.63\n",
      "| epoch   1 |  4400/29454 batches | lr 1.00 | ms/batch 152.88 | loss  8.15 | ppl  3455.92\n",
      "| epoch   1 |  4600/29454 batches | lr 1.00 | ms/batch 152.86 | loss  8.08 | ppl  3226.07\n",
      "| epoch   1 |  4800/29454 batches | lr 1.00 | ms/batch 152.83 | loss  8.11 | ppl  3313.92\n",
      "| epoch   1 |  5000/29454 batches | lr 1.00 | ms/batch 152.92 | loss  8.17 | ppl  3539.02\n",
      "| epoch   1 |  5200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  8.10 | ppl  3310.75\n",
      "| epoch   1 |  5400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  8.13 | ppl  3410.61\n",
      "| epoch   1 |  5600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  7.88 | ppl  2649.06\n",
      "| epoch   1 |  5800/29454 batches | lr 1.00 | ms/batch 154.15 | loss  8.03 | ppl  3087.08\n",
      "| epoch   1 |  6000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  8.13 | ppl  3410.77\n",
      "| epoch   1 |  6200/29454 batches | lr 1.00 | ms/batch 152.84 | loss  8.02 | ppl  3051.11\n",
      "| epoch   1 |  6400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  8.07 | ppl  3201.22\n",
      "| epoch   1 |  6600/29454 batches | lr 1.00 | ms/batch 152.81 | loss  8.04 | ppl  3093.84\n",
      "| epoch   1 |  6800/29454 batches | lr 1.00 | ms/batch 153.43 | loss  7.98 | ppl  2932.95\n",
      "| epoch   1 |  7000/29454 batches | lr 1.00 | ms/batch 153.16 | loss  8.01 | ppl  3015.09\n",
      "| epoch   1 |  7200/29454 batches | lr 1.00 | ms/batch 153.32 | loss  7.87 | ppl  2626.92\n",
      "| epoch   1 |  7400/29454 batches | lr 1.00 | ms/batch 153.40 | loss  7.90 | ppl  2693.51\n",
      "| epoch   1 |  7600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  8.09 | ppl  3258.21\n",
      "| epoch   1 |  7800/29454 batches | lr 1.00 | ms/batch 153.07 | loss  7.97 | ppl  2903.33\n",
      "| epoch   1 |  8000/29454 batches | lr 1.00 | ms/batch 153.17 | loss  8.15 | ppl  3472.70\n",
      "| epoch   1 |  8200/29454 batches | lr 1.00 | ms/batch 153.19 | loss  7.88 | ppl  2646.40\n",
      "| epoch   1 |  8400/29454 batches | lr 1.00 | ms/batch 155.30 | loss  7.76 | ppl  2333.63\n",
      "| epoch   1 |  8600/29454 batches | lr 1.00 | ms/batch 153.90 | loss  7.93 | ppl  2765.92\n",
      "| epoch   1 |  8800/29454 batches | lr 1.00 | ms/batch 156.17 | loss  7.89 | ppl  2675.16\n",
      "| epoch   1 |  9000/29454 batches | lr 1.00 | ms/batch 153.24 | loss  7.69 | ppl  2190.73\n",
      "| epoch   1 |  9200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  7.91 | ppl  2715.83\n",
      "| epoch   1 |  9400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  7.74 | ppl  2290.05\n",
      "| epoch   1 |  9600/29454 batches | lr 1.00 | ms/batch 153.16 | loss  7.83 | ppl  2526.71\n",
      "| epoch   1 |  9800/29454 batches | lr 1.00 | ms/batch 152.83 | loss  7.79 | ppl  2427.95\n",
      "| epoch   1 | 10000/29454 batches | lr 1.00 | ms/batch 152.85 | loss  7.80 | ppl  2445.03\n",
      "| epoch   1 | 10200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.78 | ppl  2390.90\n",
      "| epoch   1 | 10400/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.65 | ppl  2100.34\n",
      "| epoch   1 | 10600/29454 batches | lr 1.00 | ms/batch 152.64 | loss  7.73 | ppl  2270.98\n",
      "| epoch   1 | 10800/29454 batches | lr 1.00 | ms/batch 152.66 | loss  7.55 | ppl  1893.59\n",
      "| epoch   1 | 11000/29454 batches | lr 1.00 | ms/batch 152.72 | loss  7.61 | ppl  2027.85\n",
      "| epoch   1 | 11200/29454 batches | lr 1.00 | ms/batch 152.66 | loss  7.64 | ppl  2072.88\n",
      "| epoch   1 | 11400/29454 batches | lr 1.00 | ms/batch 152.60 | loss  7.59 | ppl  1977.85\n",
      "| epoch   1 | 11600/29454 batches | lr 1.00 | ms/batch 152.66 | loss  7.61 | ppl  2019.98\n",
      "| epoch   1 | 11800/29454 batches | lr 1.00 | ms/batch 152.58 | loss  7.61 | ppl  2011.03\n",
      "| epoch   1 | 12000/29454 batches | lr 1.00 | ms/batch 152.65 | loss  7.68 | ppl  2171.99\n",
      "| epoch   1 | 12200/29454 batches | lr 1.00 | ms/batch 152.61 | loss  7.74 | ppl  2306.94\n",
      "| epoch   1 | 12400/29454 batches | lr 1.00 | ms/batch 152.68 | loss  7.72 | ppl  2258.67\n",
      "| epoch   1 | 12600/29454 batches | lr 1.00 | ms/batch 152.52 | loss  7.46 | ppl  1738.29\n",
      "| epoch   1 | 12800/29454 batches | lr 1.00 | ms/batch 152.59 | loss  7.55 | ppl  1897.60\n",
      "| epoch   1 | 13000/29454 batches | lr 1.00 | ms/batch 152.53 | loss  7.47 | ppl  1758.53\n",
      "| epoch   1 | 13200/29454 batches | lr 1.00 | ms/batch 152.74 | loss  7.51 | ppl  1820.37\n",
      "| epoch   1 | 13400/29454 batches | lr 1.00 | ms/batch 152.76 | loss  7.59 | ppl  1987.21\n",
      "| epoch   1 | 13600/29454 batches | lr 1.00 | ms/batch 153.00 | loss  7.56 | ppl  1920.08\n",
      "| epoch   1 | 13800/29454 batches | lr 1.00 | ms/batch 152.61 | loss  7.69 | ppl  2192.97\n",
      "| epoch   1 | 14000/29454 batches | lr 1.00 | ms/batch 152.56 | loss  7.62 | ppl  2035.12\n",
      "| epoch   1 | 14200/29454 batches | lr 1.00 | ms/batch 152.55 | loss  7.65 | ppl  2101.50\n",
      "| epoch   1 | 14400/29454 batches | lr 1.00 | ms/batch 152.46 | loss  7.70 | ppl  2217.44\n",
      "| epoch   1 | 14600/29454 batches | lr 1.00 | ms/batch 152.41 | loss  7.60 | ppl  1990.30\n",
      "| epoch   1 | 14800/29454 batches | lr 1.00 | ms/batch 152.36 | loss  7.47 | ppl  1760.37\n",
      "| epoch   1 | 15000/29454 batches | lr 1.00 | ms/batch 152.35 | loss  7.47 | ppl  1754.58\n",
      "| epoch   1 | 15200/29454 batches | lr 1.00 | ms/batch 152.34 | loss  7.41 | ppl  1650.84\n",
      "| epoch   1 | 15400/29454 batches | lr 1.00 | ms/batch 152.28 | loss  7.44 | ppl  1694.78\n",
      "| epoch   1 | 15600/29454 batches | lr 1.00 | ms/batch 152.24 | loss  7.43 | ppl  1684.71\n",
      "| epoch   1 | 15800/29454 batches | lr 1.00 | ms/batch 152.20 | loss  7.38 | ppl  1607.45\n",
      "| epoch   1 | 16000/29454 batches | lr 1.00 | ms/batch 152.26 | loss  7.44 | ppl  1705.42\n",
      "| epoch   1 | 16200/29454 batches | lr 1.00 | ms/batch 152.27 | loss  7.46 | ppl  1736.64\n",
      "| epoch   1 | 16400/29454 batches | lr 1.00 | ms/batch 152.37 | loss  7.55 | ppl  1895.27\n",
      "| epoch   1 | 16600/29454 batches | lr 1.00 | ms/batch 152.37 | loss  7.40 | ppl  1643.94\n",
      "| epoch   1 | 16800/29454 batches | lr 1.00 | ms/batch 152.43 | loss  7.44 | ppl  1707.92\n",
      "| epoch   1 | 17000/29454 batches | lr 1.00 | ms/batch 152.44 | loss  7.45 | ppl  1724.93\n",
      "| epoch   1 | 17200/29454 batches | lr 1.00 | ms/batch 152.46 | loss  7.49 | ppl  1794.32\n",
      "| epoch   1 | 17400/29454 batches | lr 1.00 | ms/batch 152.67 | loss  7.33 | ppl  1521.55\n",
      "| epoch   1 | 17600/29454 batches | lr 1.00 | ms/batch 152.55 | loss  7.37 | ppl  1585.62\n",
      "| epoch   1 | 17800/29454 batches | lr 1.00 | ms/batch 152.56 | loss  7.49 | ppl  1786.36\n",
      "| epoch   1 | 18000/29454 batches | lr 1.00 | ms/batch 152.55 | loss  7.38 | ppl  1600.22\n",
      "| epoch   1 | 18200/29454 batches | lr 1.00 | ms/batch 152.53 | loss  7.44 | ppl  1709.73\n",
      "| epoch   1 | 18400/29454 batches | lr 1.00 | ms/batch 152.54 | loss  7.45 | ppl  1724.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 18600/29454 batches | lr 1.00 | ms/batch 152.56 | loss  7.58 | ppl  1962.75\n",
      "| epoch   1 | 18800/29454 batches | lr 1.00 | ms/batch 152.65 | loss  7.57 | ppl  1929.49\n",
      "| epoch   1 | 19000/29454 batches | lr 1.00 | ms/batch 152.69 | loss  7.34 | ppl  1545.26\n",
      "| epoch   1 | 19200/29454 batches | lr 1.00 | ms/batch 152.71 | loss  7.27 | ppl  1434.26\n",
      "| epoch   1 | 19400/29454 batches | lr 1.00 | ms/batch 152.70 | loss  7.35 | ppl  1554.65\n",
      "| epoch   1 | 19600/29454 batches | lr 1.00 | ms/batch 152.58 | loss  7.21 | ppl  1352.80\n",
      "| epoch   1 | 19800/29454 batches | lr 1.00 | ms/batch 152.56 | loss  7.27 | ppl  1441.17\n",
      "| epoch   1 | 20000/29454 batches | lr 1.00 | ms/batch 152.54 | loss  7.34 | ppl  1536.96\n",
      "| epoch   1 | 20200/29454 batches | lr 1.00 | ms/batch 152.53 | loss  7.37 | ppl  1585.04\n",
      "| epoch   1 | 20400/29454 batches | lr 1.00 | ms/batch 152.59 | loss  7.28 | ppl  1449.97\n",
      "| epoch   1 | 20600/29454 batches | lr 1.00 | ms/batch 152.59 | loss  7.38 | ppl  1609.06\n",
      "| epoch   1 | 20800/29454 batches | lr 1.00 | ms/batch 152.57 | loss  7.30 | ppl  1483.57\n",
      "| epoch   1 | 21000/29454 batches | lr 1.00 | ms/batch 152.61 | loss  7.30 | ppl  1478.29\n",
      "| epoch   1 | 21200/29454 batches | lr 1.00 | ms/batch 152.59 | loss  7.31 | ppl  1490.45\n",
      "| epoch   1 | 21400/29454 batches | lr 1.00 | ms/batch 152.85 | loss  7.33 | ppl  1522.95\n",
      "| epoch   1 | 21600/29454 batches | lr 1.00 | ms/batch 152.51 | loss  7.29 | ppl  1465.07\n",
      "| epoch   1 | 21800/29454 batches | lr 1.00 | ms/batch 152.53 | loss  7.33 | ppl  1523.02\n",
      "| epoch   1 | 22000/29454 batches | lr 1.00 | ms/batch 152.52 | loss  7.35 | ppl  1551.04\n",
      "| epoch   1 | 22200/29454 batches | lr 1.00 | ms/batch 152.56 | loss  7.28 | ppl  1449.75\n",
      "| epoch   1 | 22400/29454 batches | lr 1.00 | ms/batch 152.55 | loss  7.41 | ppl  1658.09\n",
      "| epoch   1 | 22600/29454 batches | lr 1.00 | ms/batch 152.58 | loss  7.33 | ppl  1532.41\n",
      "| epoch   1 | 22800/29454 batches | lr 1.00 | ms/batch 152.55 | loss  7.27 | ppl  1433.54\n",
      "| epoch   1 | 23000/29454 batches | lr 1.00 | ms/batch 152.70 | loss  7.38 | ppl  1608.02\n",
      "| epoch   1 | 23200/29454 batches | lr 1.00 | ms/batch 152.54 | loss  7.28 | ppl  1443.93\n",
      "| epoch   1 | 23400/29454 batches | lr 1.00 | ms/batch 152.55 | loss  7.24 | ppl  1399.15\n",
      "| epoch   1 | 23600/29454 batches | lr 1.00 | ms/batch 152.53 | loss  7.35 | ppl  1555.53\n",
      "| epoch   1 | 23800/29454 batches | lr 1.00 | ms/batch 152.50 | loss  7.28 | ppl  1448.60\n",
      "| epoch   1 | 24000/29454 batches | lr 1.00 | ms/batch 152.53 | loss  7.32 | ppl  1509.76\n",
      "| epoch   1 | 24200/29454 batches | lr 1.00 | ms/batch 152.56 | loss  7.07 | ppl  1170.29\n",
      "| epoch   1 | 24400/29454 batches | lr 1.00 | ms/batch 152.54 | loss  7.14 | ppl  1257.43\n",
      "| epoch   1 | 24600/29454 batches | lr 1.00 | ms/batch 152.60 | loss  7.24 | ppl  1396.93\n",
      "| epoch   1 | 24800/29454 batches | lr 1.00 | ms/batch 152.56 | loss  7.26 | ppl  1417.43\n",
      "| epoch   1 | 25000/29454 batches | lr 1.00 | ms/batch 152.63 | loss  7.18 | ppl  1318.39\n",
      "| epoch   1 | 25200/29454 batches | lr 1.00 | ms/batch 152.66 | loss  7.19 | ppl  1325.96\n",
      "| epoch   1 | 25400/29454 batches | lr 1.00 | ms/batch 152.80 | loss  7.29 | ppl  1468.03\n",
      "| epoch   1 | 25600/29454 batches | lr 1.00 | ms/batch 152.63 | loss  7.29 | ppl  1472.56\n",
      "| epoch   1 | 25800/29454 batches | lr 1.00 | ms/batch 152.57 | loss  7.22 | ppl  1372.55\n",
      "| epoch   1 | 26000/29454 batches | lr 1.00 | ms/batch 152.60 | loss  7.36 | ppl  1567.33\n",
      "| epoch   1 | 26200/29454 batches | lr 1.00 | ms/batch 152.51 | loss  7.17 | ppl  1300.95\n",
      "| epoch   1 | 26400/29454 batches | lr 1.00 | ms/batch 152.61 | loss  7.30 | ppl  1485.95\n",
      "| epoch   1 | 26600/29454 batches | lr 1.00 | ms/batch 152.65 | loss  7.20 | ppl  1338.29\n",
      "| epoch   1 | 26800/29454 batches | lr 1.00 | ms/batch 152.59 | loss  7.38 | ppl  1604.31\n",
      "| epoch   1 | 27000/29454 batches | lr 1.00 | ms/batch 152.57 | loss  7.35 | ppl  1561.14\n",
      "| epoch   1 | 27200/29454 batches | lr 1.00 | ms/batch 152.54 | loss  7.27 | ppl  1433.55\n",
      "| epoch   1 | 27400/29454 batches | lr 1.00 | ms/batch 152.68 | loss  7.28 | ppl  1455.36\n",
      "| epoch   1 | 27600/29454 batches | lr 1.00 | ms/batch 152.59 | loss  7.28 | ppl  1453.54\n",
      "| epoch   1 | 27800/29454 batches | lr 1.00 | ms/batch 152.58 | loss  7.30 | ppl  1477.41\n",
      "| epoch   1 | 28000/29454 batches | lr 1.00 | ms/batch 152.70 | loss  7.32 | ppl  1507.28\n",
      "| epoch   1 | 28200/29454 batches | lr 1.00 | ms/batch 152.63 | loss  7.26 | ppl  1421.42\n",
      "| epoch   1 | 28400/29454 batches | lr 1.00 | ms/batch 152.62 | loss  7.28 | ppl  1447.77\n",
      "| epoch   1 | 28600/29454 batches | lr 1.00 | ms/batch 152.68 | loss  7.09 | ppl  1205.56\n",
      "| epoch   1 | 28800/29454 batches | lr 1.00 | ms/batch 152.69 | loss  7.14 | ppl  1267.01\n",
      "| epoch   1 | 29000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  7.20 | ppl  1340.01\n",
      "| epoch   1 | 29200/29454 batches | lr 1.00 | ms/batch 152.89 | loss  7.18 | ppl  1318.59\n",
      "| epoch   1 | 29400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  7.10 | ppl  1213.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 4668.45s | valid loss  6.96 | valid ppl  1057.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/29454 batches | lr 1.00 | ms/batch 153.75 | loss  7.34 | ppl  1541.42\n",
      "| epoch   2 |   400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  7.14 | ppl  1259.97\n",
      "| epoch   2 |   600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  7.09 | ppl  1199.70\n",
      "| epoch   2 |   800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  7.11 | ppl  1221.59\n",
      "| epoch   2 |  1000/29454 batches | lr 1.00 | ms/batch 152.80 | loss  7.22 | ppl  1364.00\n",
      "| epoch   2 |  1200/29454 batches | lr 1.00 | ms/batch 152.73 | loss  7.03 | ppl  1126.91\n",
      "| epoch   2 |  1400/29454 batches | lr 1.00 | ms/batch 152.66 | loss  7.03 | ppl  1125.58\n",
      "| epoch   2 |  1600/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.02 | ppl  1117.97\n",
      "| epoch   2 |  1800/29454 batches | lr 1.00 | ms/batch 152.84 | loss  7.11 | ppl  1224.89\n",
      "| epoch   2 |  2000/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.05 | ppl  1151.11\n",
      "| epoch   2 |  2200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  7.00 | ppl  1098.86\n",
      "| epoch   2 |  2400/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.14 | ppl  1258.14\n",
      "| epoch   2 |  2600/29454 batches | lr 1.00 | ms/batch 152.92 | loss  7.07 | ppl  1171.82\n",
      "| epoch   2 |  2800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.98 | ppl  1074.98\n",
      "| epoch   2 |  3000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  7.36 | ppl  1569.59\n",
      "| epoch   2 |  3200/29454 batches | lr 1.00 | ms/batch 152.83 | loss  7.13 | ppl  1245.75\n",
      "| epoch   2 |  3400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  7.11 | ppl  1227.22\n",
      "| epoch   2 |  3600/29454 batches | lr 1.00 | ms/batch 152.77 | loss  7.09 | ppl  1202.22\n",
      "| epoch   2 |  3800/29454 batches | lr 1.00 | ms/batch 152.87 | loss  7.03 | ppl  1126.71\n",
      "| epoch   2 |  4000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  7.01 | ppl  1106.77\n",
      "| epoch   2 |  4200/29454 batches | lr 1.00 | ms/batch 152.87 | loss  7.03 | ppl  1128.77\n",
      "| epoch   2 |  4400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  7.00 | ppl  1093.03\n",
      "| epoch   2 |  4600/29454 batches | lr 1.00 | ms/batch 152.83 | loss  7.01 | ppl  1104.94\n",
      "| epoch   2 |  4800/29454 batches | lr 1.00 | ms/batch 152.72 | loss  7.01 | ppl  1107.99\n",
      "| epoch   2 |  5000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  7.12 | ppl  1236.15\n",
      "| epoch   2 |  5200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.05 | ppl  1152.16\n",
      "| epoch   2 |  5400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  7.10 | ppl  1217.71\n",
      "| epoch   2 |  5600/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.82 | ppl   916.60\n",
      "| epoch   2 |  5800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.99 | ppl  1089.07\n",
      "| epoch   2 |  6000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  7.18 | ppl  1311.92\n",
      "| epoch   2 |  6200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  7.03 | ppl  1127.97\n",
      "| epoch   2 |  6400/29454 batches | lr 1.00 | ms/batch 152.84 | loss  7.14 | ppl  1261.22\n",
      "| epoch   2 |  6600/29454 batches | lr 1.00 | ms/batch 152.95 | loss  7.13 | ppl  1250.85\n",
      "| epoch   2 |  6800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.08 | ppl  1184.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  7000/29454 batches | lr 1.00 | ms/batch 152.86 | loss  7.10 | ppl  1207.73\n",
      "| epoch   2 |  7200/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.98 | ppl  1080.15\n",
      "| epoch   2 |  7400/29454 batches | lr 1.00 | ms/batch 152.82 | loss  7.01 | ppl  1103.26\n",
      "| epoch   2 |  7600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  7.19 | ppl  1319.76\n",
      "| epoch   2 |  7800/29454 batches | lr 1.00 | ms/batch 152.82 | loss  7.08 | ppl  1193.51\n",
      "| epoch   2 |  8000/29454 batches | lr 1.00 | ms/batch 152.81 | loss  7.35 | ppl  1554.22\n",
      "| epoch   2 |  8200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  7.01 | ppl  1108.28\n",
      "| epoch   2 |  8400/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.89 | ppl   983.90\n",
      "| epoch   2 |  8600/29454 batches | lr 1.00 | ms/batch 152.86 | loss  7.10 | ppl  1206.01\n",
      "| epoch   2 |  8800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  7.05 | ppl  1151.95\n",
      "| epoch   2 |  9000/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.83 | ppl   929.74\n",
      "| epoch   2 |  9200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  7.08 | ppl  1186.57\n",
      "| epoch   2 |  9400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.92 | ppl  1016.45\n",
      "| epoch   2 |  9600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  7.05 | ppl  1155.25\n",
      "| epoch   2 |  9800/29454 batches | lr 1.00 | ms/batch 152.72 | loss  7.00 | ppl  1093.77\n",
      "| epoch   2 | 10000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  7.03 | ppl  1130.19\n",
      "| epoch   2 | 10200/29454 batches | lr 1.00 | ms/batch 152.72 | loss  7.02 | ppl  1113.82\n",
      "| epoch   2 | 10400/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.86 | ppl   957.27\n",
      "| epoch   2 | 10600/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.99 | ppl  1082.98\n",
      "| epoch   2 | 10800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.77 | ppl   875.62\n",
      "| epoch   2 | 11000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.82 | ppl   918.98\n",
      "| epoch   2 | 11200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.89 | ppl   977.51\n",
      "| epoch   2 | 11400/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.82 | ppl   917.49\n",
      "| epoch   2 | 11600/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.84 | ppl   935.10\n",
      "| epoch   2 | 11800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.85 | ppl   947.01\n",
      "| epoch   2 | 12000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.99 | ppl  1084.91\n",
      "| epoch   2 | 12200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  7.01 | ppl  1108.04\n",
      "| epoch   2 | 12400/29454 batches | lr 1.00 | ms/batch 152.76 | loss  7.03 | ppl  1124.67\n",
      "| epoch   2 | 12600/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.75 | ppl   851.12\n",
      "| epoch   2 | 12800/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.86 | ppl   957.98\n",
      "| epoch   2 | 13000/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.76 | ppl   859.31\n",
      "| epoch   2 | 13200/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.78 | ppl   879.14\n",
      "| epoch   2 | 13400/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.88 | ppl   970.18\n",
      "| epoch   2 | 13600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.88 | ppl   976.86\n",
      "| epoch   2 | 13800/29454 batches | lr 1.00 | ms/batch 152.68 | loss  7.06 | ppl  1160.65\n",
      "| epoch   2 | 14000/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.96 | ppl  1050.38\n",
      "| epoch   2 | 14200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.96 | ppl  1053.23\n",
      "| epoch   2 | 14400/29454 batches | lr 1.00 | ms/batch 152.92 | loss  7.06 | ppl  1163.54\n",
      "| epoch   2 | 14600/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.93 | ppl  1019.71\n",
      "| epoch   2 | 14800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.82 | ppl   917.04\n",
      "| epoch   2 | 15000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.80 | ppl   900.58\n",
      "| epoch   2 | 15200/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.74 | ppl   848.47\n",
      "| epoch   2 | 15400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.78 | ppl   878.48\n",
      "| epoch   2 | 15600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.77 | ppl   872.71\n",
      "| epoch   2 | 15800/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.73 | ppl   841.10\n",
      "| epoch   2 | 16000/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.80 | ppl   901.24\n",
      "| epoch   2 | 16200/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.82 | ppl   915.05\n",
      "| epoch   2 | 16400/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.93 | ppl  1019.24\n",
      "| epoch   2 | 16600/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.77 | ppl   870.96\n",
      "| epoch   2 | 16800/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.83 | ppl   923.90\n",
      "| epoch   2 | 17000/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.84 | ppl   935.69\n",
      "| epoch   2 | 17200/29454 batches | lr 1.00 | ms/batch 152.66 | loss  6.92 | ppl  1016.63\n",
      "| epoch   2 | 17400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.73 | ppl   836.75\n",
      "| epoch   2 | 17600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.77 | ppl   871.21\n",
      "| epoch   2 | 17800/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.91 | ppl  1005.47\n",
      "| epoch   2 | 18000/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.80 | ppl   898.04\n",
      "| epoch   2 | 18200/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.88 | ppl   971.92\n",
      "| epoch   2 | 18400/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.88 | ppl   975.73\n",
      "| epoch   2 | 18600/29454 batches | lr 1.00 | ms/batch 152.66 | loss  7.03 | ppl  1131.60\n",
      "| epoch   2 | 18800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  7.01 | ppl  1109.73\n",
      "| epoch   2 | 19000/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.77 | ppl   870.79\n",
      "| epoch   2 | 19200/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.70 | ppl   810.99\n",
      "| epoch   2 | 19400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.79 | ppl   886.42\n",
      "| epoch   2 | 19600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.65 | ppl   775.36\n",
      "| epoch   2 | 19800/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.71 | ppl   821.44\n",
      "| epoch   2 | 20000/29454 batches | lr 1.00 | ms/batch 152.64 | loss  6.81 | ppl   909.77\n",
      "| epoch   2 | 20200/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.81 | ppl   907.53\n",
      "| epoch   2 | 20400/29454 batches | lr 1.00 | ms/batch 152.66 | loss  6.72 | ppl   832.42\n",
      "| epoch   2 | 20600/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.87 | ppl   960.95\n",
      "| epoch   2 | 20800/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.75 | ppl   852.55\n",
      "| epoch   2 | 21000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.74 | ppl   848.62\n",
      "| epoch   2 | 21200/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.77 | ppl   870.74\n",
      "| epoch   2 | 21400/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.79 | ppl   891.02\n",
      "| epoch   2 | 21600/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.75 | ppl   856.73\n",
      "| epoch   2 | 21800/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.81 | ppl   908.97\n",
      "| epoch   2 | 22000/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.82 | ppl   915.02\n",
      "| epoch   2 | 22200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  6.73 | ppl   838.76\n",
      "| epoch   2 | 22400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.87 | ppl   967.71\n",
      "| epoch   2 | 22600/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.83 | ppl   927.14\n",
      "| epoch   2 | 22800/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.76 | ppl   858.92\n",
      "| epoch   2 | 23000/29454 batches | lr 1.00 | ms/batch 152.62 | loss  6.89 | ppl   979.34\n",
      "| epoch   2 | 23200/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.79 | ppl   890.56\n",
      "| epoch   2 | 23400/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.72 | ppl   826.23\n",
      "| epoch   2 | 23600/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.86 | ppl   953.02\n",
      "| epoch   2 | 23800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.79 | ppl   887.93\n",
      "| epoch   2 | 24000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.83 | ppl   924.73\n",
      "| epoch   2 | 24200/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.56 | ppl   708.48\n",
      "| epoch   2 | 24400/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.64 | ppl   764.14\n",
      "| epoch   2 | 24600/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.77 | ppl   869.09\n",
      "| epoch   2 | 24800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.76 | ppl   860.56\n",
      "| epoch   2 | 25000/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.70 | ppl   812.37\n",
      "| epoch   2 | 25200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.70 | ppl   813.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 25400/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.81 | ppl   905.35\n",
      "| epoch   2 | 25600/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.81 | ppl   905.63\n",
      "| epoch   2 | 25800/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.75 | ppl   855.11\n",
      "| epoch   2 | 26000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.90 | ppl   992.94\n",
      "| epoch   2 | 26200/29454 batches | lr 1.00 | ms/batch 152.98 | loss  6.66 | ppl   784.22\n",
      "| epoch   2 | 26400/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.82 | ppl   920.46\n",
      "| epoch   2 | 26600/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.75 | ppl   852.40\n",
      "| epoch   2 | 26800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.94 | ppl  1028.38\n",
      "| epoch   2 | 27000/29454 batches | lr 1.00 | ms/batch 152.63 | loss  6.90 | ppl   995.50\n",
      "| epoch   2 | 27200/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.80 | ppl   894.98\n",
      "| epoch   2 | 27400/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.80 | ppl   901.30\n",
      "| epoch   2 | 27600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.82 | ppl   918.65\n",
      "| epoch   2 | 27800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.85 | ppl   945.64\n",
      "| epoch   2 | 28000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.87 | ppl   963.61\n",
      "| epoch   2 | 28200/29454 batches | lr 1.00 | ms/batch 152.90 | loss  6.81 | ppl   906.45\n",
      "| epoch   2 | 28400/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.83 | ppl   922.61\n",
      "| epoch   2 | 28600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.63 | ppl   757.63\n",
      "| epoch   2 | 28800/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.69 | ppl   805.01\n",
      "| epoch   2 | 29000/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.76 | ppl   865.93\n",
      "| epoch   2 | 29200/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.72 | ppl   830.51\n",
      "| epoch   2 | 29400/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.65 | ppl   771.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 4670.36s | valid loss  6.64 | valid ppl   766.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/29454 batches | lr 1.00 | ms/batch 154.21 | loss  6.91 | ppl  1000.63\n",
      "| epoch   3 |   400/29454 batches | lr 1.00 | ms/batch 152.97 | loss  6.71 | ppl   822.34\n",
      "| epoch   3 |   600/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.66 | ppl   781.29\n",
      "| epoch   3 |   800/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.64 | ppl   767.06\n",
      "| epoch   3 |  1000/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.80 | ppl   898.33\n",
      "| epoch   3 |  1200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  6.59 | ppl   726.51\n",
      "| epoch   3 |  1400/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.61 | ppl   739.56\n",
      "| epoch   3 |  1600/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.59 | ppl   728.48\n",
      "| epoch   3 |  1800/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.69 | ppl   807.61\n",
      "| epoch   3 |  2000/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.64 | ppl   767.09\n",
      "| epoch   3 |  2200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.57 | ppl   711.52\n",
      "| epoch   3 |  2400/29454 batches | lr 1.00 | ms/batch 152.65 | loss  6.72 | ppl   832.83\n",
      "| epoch   3 |  2600/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.65 | ppl   771.05\n",
      "| epoch   3 |  2800/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.56 | ppl   704.12\n",
      "| epoch   3 |  3000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.97 | ppl  1061.08\n",
      "| epoch   3 |  3200/29454 batches | lr 1.00 | ms/batch 153.89 | loss  6.72 | ppl   830.62\n",
      "| epoch   3 |  3400/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.71 | ppl   823.22\n",
      "| epoch   3 |  3600/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.70 | ppl   811.53\n",
      "| epoch   3 |  3800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.62 | ppl   749.55\n",
      "| epoch   3 |  4000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.62 | ppl   747.53\n",
      "| epoch   3 |  4200/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.62 | ppl   752.66\n",
      "| epoch   3 |  4400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.60 | ppl   732.39\n",
      "| epoch   3 |  4600/29454 batches | lr 1.00 | ms/batch 152.58 | loss  6.64 | ppl   765.89\n",
      "| epoch   3 |  4800/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.62 | ppl   749.19\n",
      "| epoch   3 |  5000/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.75 | ppl   851.88\n",
      "| epoch   3 |  5200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.67 | ppl   785.72\n",
      "| epoch   3 |  5400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.72 | ppl   831.31\n",
      "| epoch   3 |  5600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.43 | ppl   621.69\n",
      "| epoch   3 |  5800/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.60 | ppl   732.81\n",
      "| epoch   3 |  6000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.82 | ppl   914.76\n",
      "| epoch   3 |  6200/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.65 | ppl   774.39\n",
      "| epoch   3 |  6400/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.77 | ppl   875.20\n",
      "| epoch   3 |  6600/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.76 | ppl   865.49\n",
      "| epoch   3 |  6800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.72 | ppl   831.87\n",
      "| epoch   3 |  7000/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.73 | ppl   839.22\n",
      "| epoch   3 |  7200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.64 | ppl   765.07\n",
      "| epoch   3 |  7400/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.66 | ppl   776.70\n",
      "| epoch   3 |  7600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.81 | ppl   909.48\n",
      "| epoch   3 |  7800/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.71 | ppl   823.65\n",
      "| epoch   3 |  8000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  7.00 | ppl  1100.14\n",
      "| epoch   3 |  8200/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.64 | ppl   765.29\n",
      "| epoch   3 |  8400/29454 batches | lr 1.00 | ms/batch 152.64 | loss  6.51 | ppl   673.29\n",
      "| epoch   3 |  8600/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.74 | ppl   843.28\n",
      "| epoch   3 |  8800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.69 | ppl   806.26\n",
      "| epoch   3 |  9000/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.48 | ppl   651.50\n",
      "| epoch   3 |  9200/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.71 | ppl   818.43\n",
      "| epoch   3 |  9400/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.57 | ppl   711.58\n",
      "| epoch   3 |  9600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.70 | ppl   811.93\n",
      "| epoch   3 |  9800/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.65 | ppl   774.32\n",
      "| epoch   3 | 10000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.68 | ppl   796.93\n",
      "| epoch   3 | 10200/29454 batches | lr 1.00 | ms/batch 152.64 | loss  6.67 | ppl   788.42\n",
      "| epoch   3 | 10400/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.52 | ppl   676.37\n",
      "| epoch   3 | 10600/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.66 | ppl   778.77\n",
      "| epoch   3 | 10800/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.43 | ppl   621.70\n",
      "| epoch   3 | 11000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.47 | ppl   644.99\n",
      "| epoch   3 | 11200/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.53 | ppl   688.58\n",
      "| epoch   3 | 11400/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.47 | ppl   645.27\n",
      "| epoch   3 | 11600/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.48 | ppl   649.04\n",
      "| epoch   3 | 11800/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.50 | ppl   666.84\n",
      "| epoch   3 | 12000/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.65 | ppl   776.65\n",
      "| epoch   3 | 12200/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.68 | ppl   793.03\n",
      "| epoch   3 | 12400/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.69 | ppl   802.32\n",
      "| epoch   3 | 12600/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.41 | ppl   608.87\n",
      "| epoch   3 | 12800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.53 | ppl   686.65\n",
      "| epoch   3 | 13000/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.42 | ppl   615.07\n",
      "| epoch   3 | 13200/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.43 | ppl   619.80\n",
      "| epoch   3 | 13400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.54 | ppl   692.51\n",
      "| epoch   3 | 13600/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.56 | ppl   706.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 13800/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.74 | ppl   843.76\n",
      "| epoch   3 | 14000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.63 | ppl   755.16\n",
      "| epoch   3 | 14200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.62 | ppl   752.67\n",
      "| epoch   3 | 14400/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.74 | ppl   845.77\n",
      "| epoch   3 | 14600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.60 | ppl   733.52\n",
      "| epoch   3 | 14800/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.49 | ppl   661.01\n",
      "| epoch   3 | 15000/29454 batches | lr 1.00 | ms/batch 152.65 | loss  6.48 | ppl   652.73\n",
      "| epoch   3 | 15200/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.42 | ppl   610.94\n",
      "| epoch   3 | 15400/29454 batches | lr 1.00 | ms/batch 152.64 | loss  6.45 | ppl   633.30\n",
      "| epoch   3 | 15600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.45 | ppl   634.47\n",
      "| epoch   3 | 15800/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.42 | ppl   611.99\n",
      "| epoch   3 | 16000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.48 | ppl   652.81\n",
      "| epoch   3 | 16200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.50 | ppl   663.79\n",
      "| epoch   3 | 16400/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.60 | ppl   735.73\n",
      "| epoch   3 | 16600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.45 | ppl   630.84\n",
      "| epoch   3 | 16800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.51 | ppl   670.36\n",
      "| epoch   3 | 17000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.52 | ppl   676.30\n",
      "| epoch   3 | 17200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.62 | ppl   753.56\n",
      "| epoch   3 | 17400/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.42 | ppl   611.29\n",
      "| epoch   3 | 17600/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.45 | ppl   635.85\n",
      "| epoch   3 | 17800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.62 | ppl   746.69\n",
      "| epoch   3 | 18000/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.49 | ppl   658.55\n",
      "| epoch   3 | 18200/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.57 | ppl   713.56\n",
      "| epoch   3 | 18400/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.58 | ppl   717.85\n",
      "| epoch   3 | 18600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.72 | ppl   830.26\n",
      "| epoch   3 | 18800/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.70 | ppl   813.81\n",
      "| epoch   3 | 19000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.45 | ppl   633.57\n",
      "| epoch   3 | 19200/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.40 | ppl   601.37\n",
      "| epoch   3 | 19400/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.48 | ppl   654.60\n",
      "| epoch   3 | 19600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.35 | ppl   574.36\n",
      "| epoch   3 | 19800/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.41 | ppl   606.81\n",
      "| epoch   3 | 20000/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.54 | ppl   689.53\n",
      "| epoch   3 | 20200/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.50 | ppl   664.32\n",
      "| epoch   3 | 20400/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.43 | ppl   619.53\n",
      "| epoch   3 | 20600/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.58 | ppl   718.23\n",
      "| epoch   3 | 20800/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.44 | ppl   629.40\n",
      "| epoch   3 | 21000/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.43 | ppl   622.75\n",
      "| epoch   3 | 21200/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.48 | ppl   649.38\n",
      "| epoch   3 | 21400/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.49 | ppl   655.38\n",
      "| epoch   3 | 21600/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.46 | ppl   637.41\n",
      "| epoch   3 | 21800/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.53 | ppl   682.15\n",
      "| epoch   3 | 22000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.52 | ppl   678.36\n",
      "| epoch   3 | 22200/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.43 | ppl   619.98\n",
      "| epoch   3 | 22400/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.56 | ppl   706.88\n",
      "| epoch   3 | 22600/29454 batches | lr 1.00 | ms/batch 152.64 | loss  6.55 | ppl   700.16\n",
      "| epoch   3 | 22800/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.46 | ppl   641.67\n",
      "| epoch   3 | 23000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.60 | ppl   733.77\n",
      "| epoch   3 | 23200/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.51 | ppl   674.01\n",
      "| epoch   3 | 23400/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.43 | ppl   617.22\n",
      "| epoch   3 | 23600/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.58 | ppl   722.89\n",
      "| epoch   3 | 23800/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.50 | ppl   666.21\n",
      "| epoch   3 | 24000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.53 | ppl   688.33\n",
      "| epoch   3 | 24200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.27 | ppl   530.47\n",
      "| epoch   3 | 24400/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.35 | ppl   572.56\n",
      "| epoch   3 | 24600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.49 | ppl   658.19\n",
      "| epoch   3 | 24800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.47 | ppl   643.53\n",
      "| epoch   3 | 25000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.41 | ppl   609.63\n",
      "| epoch   3 | 25200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.41 | ppl   609.02\n",
      "| epoch   3 | 25400/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.52 | ppl   680.57\n",
      "| epoch   3 | 25600/29454 batches | lr 1.00 | ms/batch 152.65 | loss  6.51 | ppl   671.75\n",
      "| epoch   3 | 25800/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.47 | ppl   647.51\n",
      "| epoch   3 | 26000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.63 | ppl   754.48\n",
      "| epoch   3 | 26200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.36 | ppl   580.45\n",
      "| epoch   3 | 26400/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.53 | ppl   682.82\n",
      "| epoch   3 | 26600/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.47 | ppl   646.96\n",
      "| epoch   3 | 26800/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.67 | ppl   786.84\n",
      "| epoch   3 | 27000/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.62 | ppl   750.22\n",
      "| epoch   3 | 27200/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.51 | ppl   673.52\n",
      "| epoch   3 | 27400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.52 | ppl   675.31\n",
      "| epoch   3 | 27600/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.55 | ppl   696.21\n",
      "| epoch   3 | 27800/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.57 | ppl   716.31\n",
      "| epoch   3 | 28000/29454 batches | lr 1.00 | ms/batch 152.62 | loss  6.60 | ppl   731.56\n",
      "| epoch   3 | 28200/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.53 | ppl   686.99\n",
      "| epoch   3 | 28400/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.55 | ppl   701.38\n",
      "| epoch   3 | 28600/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.36 | ppl   577.86\n",
      "| epoch   3 | 28800/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.41 | ppl   609.45\n",
      "| epoch   3 | 29000/29454 batches | lr 1.00 | ms/batch 152.64 | loss  6.49 | ppl   659.74\n",
      "| epoch   3 | 29200/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.42 | ppl   616.61\n",
      "| epoch   3 | 29400/29454 batches | lr 1.00 | ms/batch 152.53 | loss  6.37 | ppl   585.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 4669.64s | valid loss  6.49 | valid ppl   658.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/29454 batches | lr 1.00 | ms/batch 153.75 | loss  6.63 | ppl   757.68\n",
      "| epoch   4 |   400/29454 batches | lr 1.00 | ms/batch 153.00 | loss  6.45 | ppl   631.14\n",
      "| epoch   4 |   600/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.39 | ppl   596.70\n",
      "| epoch   4 |   800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.36 | ppl   579.25\n",
      "| epoch   4 |  1000/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.53 | ppl   685.79\n",
      "| epoch   4 |  1200/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.32 | ppl   554.43\n",
      "| epoch   4 |  1400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.34 | ppl   568.59\n",
      "| epoch   4 |  1600/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.32 | ppl   554.85\n",
      "| epoch   4 |  1800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.43 | ppl   619.59\n",
      "| epoch   4 |  2000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.39 | ppl   593.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  2200/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.29 | ppl   540.75\n",
      "| epoch   4 |  2400/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.46 | ppl   639.96\n",
      "| epoch   4 |  2600/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.38 | ppl   591.34\n",
      "| epoch   4 |  2800/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.30 | ppl   543.65\n",
      "| epoch   4 |  3000/29454 batches | lr 1.00 | ms/batch 152.61 | loss  6.71 | ppl   817.46\n",
      "| epoch   4 |  3200/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.46 | ppl   636.53\n",
      "| epoch   4 |  3400/29454 batches | lr 1.00 | ms/batch 152.61 | loss  6.46 | ppl   637.17\n",
      "| epoch   4 |  3600/29454 batches | lr 1.00 | ms/batch 152.66 | loss  6.44 | ppl   625.21\n",
      "| epoch   4 |  3800/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.36 | ppl   578.93\n",
      "| epoch   4 |  4000/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.36 | ppl   580.44\n",
      "| epoch   4 |  4200/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.36 | ppl   579.66\n",
      "| epoch   4 |  4400/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.33 | ppl   562.02\n",
      "| epoch   4 |  4600/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.40 | ppl   599.42\n",
      "| epoch   4 |  4800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.36 | ppl   576.48\n",
      "| epoch   4 |  5000/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.49 | ppl   659.35\n",
      "| epoch   4 |  5200/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.41 | ppl   610.68\n",
      "| epoch   4 |  5400/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.47 | ppl   645.46\n",
      "| epoch   4 |  5600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.18 | ppl   483.91\n",
      "| epoch   4 |  5800/29454 batches | lr 1.00 | ms/batch 152.65 | loss  6.34 | ppl   568.15\n",
      "| epoch   4 |  6000/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.58 | ppl   719.53\n",
      "| epoch   4 |  6200/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.40 | ppl   602.81\n",
      "| epoch   4 |  6400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.53 | ppl   685.74\n",
      "| epoch   4 |  6600/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.52 | ppl   677.32\n",
      "| epoch   4 |  6800/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.48 | ppl   651.62\n",
      "| epoch   4 |  7000/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.48 | ppl   653.44\n",
      "| epoch   4 |  7200/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.40 | ppl   603.81\n",
      "| epoch   4 |  7400/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.42 | ppl   611.90\n",
      "| epoch   4 |  7600/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.56 | ppl   704.73\n",
      "| epoch   4 |  7800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.47 | ppl   643.19\n",
      "| epoch   4 |  8000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.76 | ppl   865.62\n",
      "| epoch   4 |  8200/29454 batches | lr 1.00 | ms/batch 152.90 | loss  6.39 | ppl   597.40\n",
      "| epoch   4 |  8400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.25 | ppl   517.51\n",
      "| epoch   4 |  8600/29454 batches | lr 1.00 | ms/batch 152.66 | loss  6.49 | ppl   658.83\n",
      "| epoch   4 |  8800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.45 | ppl   632.33\n",
      "| epoch   4 |  9000/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.24 | ppl   511.11\n",
      "| epoch   4 |  9200/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.45 | ppl   632.69\n",
      "| epoch   4 |  9400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.33 | ppl   559.14\n",
      "| epoch   4 |  9600/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.47 | ppl   642.90\n",
      "| epoch   4 |  9800/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.42 | ppl   611.04\n",
      "| epoch   4 | 10000/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.44 | ppl   626.04\n",
      "| epoch   4 | 10200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.44 | ppl   624.48\n",
      "| epoch   4 | 10400/29454 batches | lr 1.00 | ms/batch 152.72 | loss  6.28 | ppl   534.16\n",
      "| epoch   4 | 10600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.42 | ppl   615.87\n",
      "| epoch   4 | 10800/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.19 | ppl   489.46\n",
      "| epoch   4 | 11000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.23 | ppl   505.24\n",
      "| epoch   4 | 11200/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.29 | ppl   538.35\n",
      "| epoch   4 | 11400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.23 | ppl   508.18\n",
      "| epoch   4 | 11600/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.23 | ppl   507.55\n",
      "| epoch   4 | 11800/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.26 | ppl   522.26\n",
      "| epoch   4 | 12000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.43 | ppl   617.91\n",
      "| epoch   4 | 12200/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.44 | ppl   626.79\n",
      "| epoch   4 | 12400/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.45 | ppl   634.07\n",
      "| epoch   4 | 12600/29454 batches | lr 1.00 | ms/batch 152.63 | loss  6.18 | ppl   484.38\n",
      "| epoch   4 | 12800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.30 | ppl   545.92\n",
      "| epoch   4 | 13000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.19 | ppl   488.42\n",
      "| epoch   4 | 13200/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.19 | ppl   485.72\n",
      "| epoch   4 | 13400/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.31 | ppl   549.41\n",
      "| epoch   4 | 13600/29454 batches | lr 1.00 | ms/batch 152.66 | loss  6.33 | ppl   563.96\n",
      "| epoch   4 | 13800/29454 batches | lr 1.00 | ms/batch 152.64 | loss  6.51 | ppl   672.02\n",
      "| epoch   4 | 14000/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.39 | ppl   593.87\n",
      "| epoch   4 | 14200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.40 | ppl   598.98\n",
      "| epoch   4 | 14400/29454 batches | lr 1.00 | ms/batch 153.12 | loss  6.51 | ppl   669.23\n",
      "| epoch   4 | 14600/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.36 | ppl   580.18\n",
      "| epoch   4 | 14800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.26 | ppl   525.52\n",
      "| epoch   4 | 15000/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.26 | ppl   520.64\n",
      "| epoch   4 | 15200/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.19 | ppl   486.62\n",
      "| epoch   4 | 15400/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.22 | ppl   502.01\n",
      "| epoch   4 | 15600/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.23 | ppl   506.58\n",
      "| epoch   4 | 15800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.19 | ppl   487.49\n",
      "| epoch   4 | 16000/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.25 | ppl   519.14\n",
      "| epoch   4 | 16200/29454 batches | lr 1.00 | ms/batch 152.90 | loss  6.27 | ppl   530.08\n",
      "| epoch   4 | 16400/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.37 | ppl   584.57\n",
      "| epoch   4 | 16600/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.22 | ppl   502.61\n",
      "| epoch   4 | 16800/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.28 | ppl   531.18\n",
      "| epoch   4 | 17000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.29 | ppl   538.85\n",
      "| epoch   4 | 17200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.41 | ppl   609.16\n",
      "| epoch   4 | 17400/29454 batches | lr 1.00 | ms/batch 80208.45 | loss  6.18 | ppl   485.31\n",
      "| epoch   4 | 17600/29454 batches | lr 1.00 | ms/batch 148.59 | loss  6.23 | ppl   506.49\n",
      "| epoch   4 | 17800/29454 batches | lr 1.00 | ms/batch 149.17 | loss  6.39 | ppl   598.54\n",
      "| epoch   4 | 18000/29454 batches | lr 1.00 | ms/batch 149.85 | loss  6.26 | ppl   524.65\n",
      "| epoch   4 | 18200/29454 batches | lr 1.00 | ms/batch 150.30 | loss  6.35 | ppl   569.66\n",
      "| epoch   4 | 18400/29454 batches | lr 1.00 | ms/batch 150.46 | loss  6.36 | ppl   577.86\n",
      "| epoch   4 | 18600/29454 batches | lr 1.00 | ms/batch 150.70 | loss  6.50 | ppl   663.97\n",
      "| epoch   4 | 18800/29454 batches | lr 1.00 | ms/batch 150.73 | loss  6.48 | ppl   648.95\n",
      "| epoch   4 | 19000/29454 batches | lr 1.00 | ms/batch 150.97 | loss  6.22 | ppl   505.18\n",
      "| epoch   4 | 19200/29454 batches | lr 1.00 | ms/batch 151.21 | loss  6.18 | ppl   482.11\n",
      "| epoch   4 | 19400/29454 batches | lr 1.00 | ms/batch 151.29 | loss  6.27 | ppl   527.46\n",
      "| epoch   4 | 19600/29454 batches | lr 1.00 | ms/batch 151.38 | loss  6.14 | ppl   463.12\n",
      "| epoch   4 | 19800/29454 batches | lr 1.00 | ms/batch 151.57 | loss  6.18 | ppl   485.09\n",
      "| epoch   4 | 20000/29454 batches | lr 1.00 | ms/batch 151.66 | loss  6.33 | ppl   563.58\n",
      "| epoch   4 | 20200/29454 batches | lr 1.00 | ms/batch 151.78 | loss  6.28 | ppl   534.32\n",
      "| epoch   4 | 20400/29454 batches | lr 1.00 | ms/batch 151.89 | loss  6.21 | ppl   498.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 20600/29454 batches | lr 1.00 | ms/batch 152.02 | loss  6.37 | ppl   581.83\n",
      "| epoch   4 | 20800/29454 batches | lr 1.00 | ms/batch 152.00 | loss  6.22 | ppl   501.98\n",
      "| epoch   4 | 21000/29454 batches | lr 1.00 | ms/batch 152.05 | loss  6.21 | ppl   499.00\n",
      "| epoch   4 | 21200/29454 batches | lr 1.00 | ms/batch 152.14 | loss  6.26 | ppl   521.30\n",
      "| epoch   4 | 21400/29454 batches | lr 1.00 | ms/batch 152.18 | loss  6.27 | ppl   526.04\n",
      "| epoch   4 | 21600/29454 batches | lr 1.00 | ms/batch 152.27 | loss  6.24 | ppl   512.16\n",
      "| epoch   4 | 21800/29454 batches | lr 1.00 | ms/batch 152.14 | loss  6.32 | ppl   552.87\n",
      "| epoch   4 | 22000/29454 batches | lr 1.00 | ms/batch 152.05 | loss  6.30 | ppl   542.56\n",
      "| epoch   4 | 22200/29454 batches | lr 1.00 | ms/batch 152.09 | loss  6.21 | ppl   496.34\n",
      "| epoch   4 | 22400/29454 batches | lr 1.00 | ms/batch 152.04 | loss  6.33 | ppl   562.03\n",
      "| epoch   4 | 22600/29454 batches | lr 1.00 | ms/batch 152.13 | loss  6.34 | ppl   564.18\n",
      "| epoch   4 | 22800/29454 batches | lr 1.00 | ms/batch 151.93 | loss  6.25 | ppl   519.90\n",
      "| epoch   4 | 23000/29454 batches | lr 1.00 | ms/batch 152.12 | loss  6.38 | ppl   590.41\n",
      "| epoch   4 | 23200/29454 batches | lr 1.00 | ms/batch 152.19 | loss  6.30 | ppl   547.21\n",
      "| epoch   4 | 23400/29454 batches | lr 1.00 | ms/batch 152.25 | loss  6.21 | ppl   498.71\n",
      "| epoch   4 | 23600/29454 batches | lr 1.00 | ms/batch 152.17 | loss  6.37 | ppl   584.66\n",
      "| epoch   4 | 23800/29454 batches | lr 1.00 | ms/batch 152.20 | loss  6.29 | ppl   538.67\n",
      "| epoch   4 | 24000/29454 batches | lr 1.00 | ms/batch 152.23 | loss  6.32 | ppl   553.56\n",
      "| epoch   4 | 24200/29454 batches | lr 1.00 | ms/batch 152.32 | loss  6.06 | ppl   427.93\n",
      "| epoch   4 | 24400/29454 batches | lr 1.00 | ms/batch 152.33 | loss  6.14 | ppl   462.12\n",
      "| epoch   4 | 24600/29454 batches | lr 1.00 | ms/batch 152.27 | loss  6.28 | ppl   532.95\n",
      "| epoch   4 | 24800/29454 batches | lr 1.00 | ms/batch 152.26 | loss  6.25 | ppl   516.90\n",
      "| epoch   4 | 25000/29454 batches | lr 1.00 | ms/batch 152.37 | loss  6.20 | ppl   493.66\n",
      "| epoch   4 | 25200/29454 batches | lr 1.00 | ms/batch 152.36 | loss  6.20 | ppl   492.02\n",
      "| epoch   4 | 25400/29454 batches | lr 1.00 | ms/batch 152.43 | loss  6.31 | ppl   551.01\n",
      "| epoch   4 | 25600/29454 batches | lr 1.00 | ms/batch 152.43 | loss  6.29 | ppl   538.24\n",
      "| epoch   4 | 25800/29454 batches | lr 1.00 | ms/batch 152.45 | loss  6.26 | ppl   524.81\n",
      "| epoch   4 | 26000/29454 batches | lr 1.00 | ms/batch 152.40 | loss  6.40 | ppl   602.84\n",
      "| epoch   4 | 26200/29454 batches | lr 1.00 | ms/batch 152.38 | loss  6.14 | ppl   463.02\n",
      "| epoch   4 | 26400/29454 batches | lr 1.00 | ms/batch 152.42 | loss  6.30 | ppl   546.27\n",
      "| epoch   4 | 26600/29454 batches | lr 1.00 | ms/batch 152.45 | loss  6.26 | ppl   523.08\n",
      "| epoch   4 | 26800/29454 batches | lr 1.00 | ms/batch 152.42 | loss  6.46 | ppl   641.61\n",
      "| epoch   4 | 27000/29454 batches | lr 1.00 | ms/batch 152.40 | loss  6.40 | ppl   604.77\n",
      "| epoch   4 | 27200/29454 batches | lr 1.00 | ms/batch 152.43 | loss  6.30 | ppl   544.98\n",
      "| epoch   4 | 27400/29454 batches | lr 1.00 | ms/batch 152.43 | loss  6.30 | ppl   542.36\n",
      "| epoch   4 | 27600/29454 batches | lr 1.00 | ms/batch 152.46 | loss  6.34 | ppl   564.05\n",
      "| epoch   4 | 27800/29454 batches | lr 1.00 | ms/batch 152.54 | loss  6.37 | ppl   581.19\n",
      "| epoch   4 | 28000/29454 batches | lr 1.00 | ms/batch 152.50 | loss  6.39 | ppl   594.74\n",
      "| epoch   4 | 28200/29454 batches | lr 1.00 | ms/batch 152.48 | loss  6.33 | ppl   560.08\n",
      "| epoch   4 | 28400/29454 batches | lr 1.00 | ms/batch 152.48 | loss  6.34 | ppl   566.14\n",
      "| epoch   4 | 28600/29454 batches | lr 1.00 | ms/batch 152.55 | loss  6.15 | ppl   466.83\n",
      "| epoch   4 | 28800/29454 batches | lr 1.00 | ms/batch 152.60 | loss  6.20 | ppl   493.14\n",
      "| epoch   4 | 29000/29454 batches | lr 1.00 | ms/batch 152.53 | loss  6.29 | ppl   537.61\n",
      "| epoch   4 | 29200/29454 batches | lr 1.00 | ms/batch 152.53 | loss  6.20 | ppl   493.69\n",
      "| epoch   4 | 29400/29454 batches | lr 1.00 | ms/batch 152.48 | loss  6.17 | ppl   476.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 20670.84s | valid loss  6.41 | valid ppl   608.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/29454 batches | lr 1.00 | ms/batch 154.97 | loss  6.42 | ppl   611.81\n",
      "| epoch   5 |   400/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.25 | ppl   517.77\n",
      "| epoch   5 |   600/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.18 | ppl   481.76\n",
      "| epoch   5 |   800/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.14 | ppl   465.96\n",
      "| epoch   5 |  1000/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.33 | ppl   558.55\n",
      "| epoch   5 |  1200/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.11 | ppl   450.39\n",
      "| epoch   5 |  1400/29454 batches | lr 1.00 | ms/batch 152.65 | loss  6.14 | ppl   463.76\n",
      "| epoch   5 |  1600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.11 | ppl   448.12\n",
      "| epoch   5 |  1800/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.22 | ppl   503.90\n",
      "| epoch   5 |  2000/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.18 | ppl   485.08\n",
      "| epoch   5 |  2200/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.08 | ppl   438.84\n",
      "| epoch   5 |  2400/29454 batches | lr 1.00 | ms/batch 152.67 | loss  6.26 | ppl   524.08\n",
      "| epoch   5 |  2600/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.18 | ppl   483.48\n",
      "| epoch   5 |  2800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.09 | ppl   443.57\n",
      "| epoch   5 |  3000/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.49 | ppl   660.78\n",
      "| epoch   5 |  3200/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.25 | ppl   518.61\n",
      "| epoch   5 |  3400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.25 | ppl   517.02\n",
      "| epoch   5 |  3600/29454 batches | lr 1.00 | ms/batch 152.66 | loss  6.23 | ppl   510.21\n",
      "| epoch   5 |  3800/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.16 | ppl   474.52\n",
      "| epoch   5 |  4000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.16 | ppl   471.92\n",
      "| epoch   5 |  4200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.16 | ppl   471.72\n",
      "| epoch   5 |  4400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.13 | ppl   457.96\n",
      "| epoch   5 |  4600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.21 | ppl   495.59\n",
      "| epoch   5 |  4800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.15 | ppl   470.37\n",
      "| epoch   5 |  5000/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.30 | ppl   543.16\n",
      "| epoch   5 |  5200/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.22 | ppl   501.09\n",
      "| epoch   5 |  5400/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.27 | ppl   526.92\n",
      "| epoch   5 |  5600/29454 batches | lr 1.00 | ms/batch 152.76 | loss  5.98 | ppl   394.85\n",
      "| epoch   5 |  5800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.15 | ppl   466.60\n",
      "| epoch   5 |  6000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.40 | ppl   599.04\n",
      "| epoch   5 |  6200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.20 | ppl   493.02\n",
      "| epoch   5 |  6400/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.34 | ppl   564.55\n",
      "| epoch   5 |  6600/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.33 | ppl   558.42\n",
      "| epoch   5 |  6800/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.28 | ppl   536.04\n",
      "| epoch   5 |  7000/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.29 | ppl   540.30\n",
      "| epoch   5 |  7200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.22 | ppl   501.51\n",
      "| epoch   5 |  7400/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.23 | ppl   507.85\n",
      "| epoch   5 |  7600/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.36 | ppl   576.92\n",
      "| epoch   5 |  7800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.27 | ppl   526.62\n",
      "| epoch   5 |  8000/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.56 | ppl   709.43\n",
      "| epoch   5 |  8200/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.19 | ppl   489.54\n",
      "| epoch   5 |  8400/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.04 | ppl   419.60\n",
      "| epoch   5 |  8600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.30 | ppl   544.23\n",
      "| epoch   5 |  8800/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.25 | ppl   518.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  9000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.05 | ppl   422.98\n",
      "| epoch   5 |  9200/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.25 | ppl   516.99\n",
      "| epoch   5 |  9400/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.14 | ppl   463.03\n",
      "| epoch   5 |  9600/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.27 | ppl   529.79\n",
      "| epoch   5 |  9800/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.23 | ppl   509.60\n",
      "| epoch   5 | 10000/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.25 | ppl   517.82\n",
      "| epoch   5 | 10200/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.25 | ppl   516.58\n",
      "| epoch   5 | 10400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  6.09 | ppl   442.98\n",
      "| epoch   5 | 10600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.23 | ppl   509.48\n",
      "| epoch   5 | 10800/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.01 | ppl   408.10\n",
      "| epoch   5 | 11000/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.04 | ppl   418.27\n",
      "| epoch   5 | 11200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.10 | ppl   444.66\n",
      "| epoch   5 | 11400/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.04 | ppl   418.74\n",
      "| epoch   5 | 11600/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.03 | ppl   414.73\n",
      "| epoch   5 | 11800/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.06 | ppl   428.62\n",
      "| epoch   5 | 12000/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.24 | ppl   511.59\n",
      "| epoch   5 | 12200/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.26 | ppl   520.63\n",
      "| epoch   5 | 12400/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.26 | ppl   523.30\n",
      "| epoch   5 | 12600/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.00 | ppl   402.26\n",
      "| epoch   5 | 12800/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.12 | ppl   454.46\n",
      "| epoch   5 | 13000/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.01 | ppl   405.63\n",
      "| epoch   5 | 13200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.99 | ppl   400.24\n",
      "| epoch   5 | 13400/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.12 | ppl   453.36\n",
      "| epoch   5 | 13600/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.15 | ppl   468.73\n",
      "| epoch   5 | 13800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.33 | ppl   559.84\n",
      "| epoch   5 | 14000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  6.20 | ppl   492.49\n",
      "| epoch   5 | 14200/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.20 | ppl   494.78\n",
      "| epoch   5 | 14400/29454 batches | lr 1.00 | ms/batch 153.01 | loss  6.32 | ppl   554.63\n",
      "| epoch   5 | 14600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.18 | ppl   482.39\n",
      "| epoch   5 | 14800/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.08 | ppl   437.18\n",
      "| epoch   5 | 15000/29454 batches | lr 1.00 | ms/batch 152.92 | loss  6.08 | ppl   435.87\n",
      "| epoch   5 | 15200/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.01 | ppl   405.48\n",
      "| epoch   5 | 15400/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.03 | ppl   415.48\n",
      "| epoch   5 | 15600/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.04 | ppl   421.46\n",
      "| epoch   5 | 15800/29454 batches | lr 1.00 | ms/batch 152.90 | loss  6.01 | ppl   406.23\n",
      "| epoch   5 | 16000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.07 | ppl   430.96\n",
      "| epoch   5 | 16200/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.10 | ppl   444.04\n",
      "| epoch   5 | 16400/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.18 | ppl   483.65\n",
      "| epoch   5 | 16600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.03 | ppl   415.26\n",
      "| epoch   5 | 16800/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.09 | ppl   441.35\n",
      "| epoch   5 | 17000/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.09 | ppl   442.10\n",
      "| epoch   5 | 17200/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.24 | ppl   510.90\n",
      "| epoch   5 | 17400/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.00 | ppl   404.63\n",
      "| epoch   5 | 17600/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.05 | ppl   423.49\n",
      "| epoch   5 | 17800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.21 | ppl   498.83\n",
      "| epoch   5 | 18000/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.08 | ppl   439.02\n",
      "| epoch   5 | 18200/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.16 | ppl   475.68\n",
      "| epoch   5 | 18400/29454 batches | lr 1.00 | ms/batch 152.70 | loss  6.18 | ppl   481.11\n",
      "| epoch   5 | 18600/29454 batches | lr 1.00 | ms/batch 152.68 | loss  6.31 | ppl   550.86\n",
      "| epoch   5 | 18800/29454 batches | lr 1.00 | ms/batch 152.69 | loss  6.28 | ppl   535.26\n",
      "| epoch   5 | 19000/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.04 | ppl   418.25\n",
      "| epoch   5 | 19200/29454 batches | lr 1.00 | ms/batch 152.73 | loss  6.00 | ppl   404.29\n",
      "| epoch   5 | 19400/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.09 | ppl   439.37\n",
      "| epoch   5 | 19600/29454 batches | lr 1.00 | ms/batch 152.76 | loss  5.97 | ppl   389.67\n",
      "| epoch   5 | 19800/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.00 | ppl   403.08\n",
      "| epoch   5 | 20000/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.18 | ppl   480.82\n",
      "| epoch   5 | 20200/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.10 | ppl   444.79\n",
      "| epoch   5 | 20400/29454 batches | lr 1.00 | ms/batch 152.75 | loss  6.04 | ppl   420.91\n",
      "| epoch   5 | 20600/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.19 | ppl   487.18\n",
      "| epoch   5 | 20800/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.04 | ppl   419.90\n",
      "| epoch   5 | 21000/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.02 | ppl   413.29\n",
      "| epoch   5 | 21200/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.07 | ppl   433.44\n",
      "| epoch   5 | 21400/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.08 | ppl   437.50\n",
      "| epoch   5 | 21600/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.06 | ppl   428.03\n",
      "| epoch   5 | 21800/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.14 | ppl   462.89\n",
      "| epoch   5 | 22000/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.11 | ppl   452.10\n",
      "| epoch   5 | 22200/29454 batches | lr 1.00 | ms/batch 153.00 | loss  6.02 | ppl   412.78\n",
      "| epoch   5 | 22400/29454 batches | lr 1.00 | ms/batch 152.95 | loss  6.14 | ppl   465.17\n",
      "| epoch   5 | 22600/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.16 | ppl   474.27\n",
      "| epoch   5 | 22800/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.08 | ppl   436.72\n",
      "| epoch   5 | 23000/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.21 | ppl   495.57\n",
      "| epoch   5 | 23200/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.13 | ppl   459.81\n",
      "| epoch   5 | 23400/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.04 | ppl   419.42\n",
      "| epoch   5 | 23600/29454 batches | lr 1.00 | ms/batch 152.77 | loss  6.20 | ppl   491.65\n",
      "| epoch   5 | 23800/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.11 | ppl   452.14\n",
      "| epoch   5 | 24000/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.13 | ppl   461.52\n",
      "| epoch   5 | 24200/29454 batches | lr 1.00 | ms/batch 152.83 | loss  5.89 | ppl   360.19\n",
      "| epoch   5 | 24400/29454 batches | lr 1.00 | ms/batch 152.77 | loss  5.96 | ppl   388.73\n",
      "| epoch   5 | 24600/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.10 | ppl   446.38\n",
      "| epoch   5 | 24800/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.07 | ppl   431.48\n",
      "| epoch   5 | 25000/29454 batches | lr 1.00 | ms/batch 152.78 | loss  6.03 | ppl   415.56\n",
      "| epoch   5 | 25200/29454 batches | lr 1.00 | ms/batch 152.76 | loss  6.02 | ppl   412.54\n",
      "| epoch   5 | 25400/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.13 | ppl   460.68\n",
      "| epoch   5 | 25600/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.10 | ppl   447.76\n",
      "| epoch   5 | 25800/29454 batches | lr 1.00 | ms/batch 152.81 | loss  6.09 | ppl   442.62\n",
      "| epoch   5 | 26000/29454 batches | lr 1.00 | ms/batch 152.74 | loss  6.22 | ppl   502.08\n",
      "| epoch   5 | 26200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.96 | ppl   385.77\n",
      "| epoch   5 | 26400/29454 batches | lr 1.00 | ms/batch 152.71 | loss  6.11 | ppl   451.26\n",
      "| epoch   5 | 26600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.09 | ppl   441.66\n",
      "| epoch   5 | 26800/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.30 | ppl   541.88\n",
      "| epoch   5 | 27000/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.22 | ppl   503.74\n",
      "| epoch   5 | 27200/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.12 | ppl   456.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 27400/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.12 | ppl   454.76\n",
      "| epoch   5 | 27600/29454 batches | lr 1.00 | ms/batch 152.79 | loss  6.15 | ppl   470.80\n",
      "| epoch   5 | 27800/29454 batches | lr 1.00 | ms/batch 152.86 | loss  6.19 | ppl   487.49\n",
      "| epoch   5 | 28000/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.21 | ppl   499.33\n",
      "| epoch   5 | 28200/29454 batches | lr 1.00 | ms/batch 152.92 | loss  6.15 | ppl   470.69\n",
      "| epoch   5 | 28400/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.16 | ppl   475.12\n",
      "| epoch   5 | 28600/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.98 | ppl   395.54\n",
      "| epoch   5 | 28800/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.02 | ppl   410.08\n",
      "| epoch   5 | 29000/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.11 | ppl   452.53\n",
      "| epoch   5 | 29200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  6.02 | ppl   410.76\n",
      "| epoch   5 | 29400/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.00 | ppl   401.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 4671.57s | valid loss  6.37 | valid ppl   581.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/29454 batches | lr 1.00 | ms/batch 153.90 | loss  6.24 | ppl   515.29\n",
      "| epoch   6 |   400/29454 batches | lr 1.00 | ms/batch 153.07 | loss  6.08 | ppl   435.35\n",
      "| epoch   6 |   600/29454 batches | lr 1.00 | ms/batch 153.06 | loss  6.01 | ppl   405.79\n",
      "| epoch   6 |   800/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.96 | ppl   386.61\n",
      "| epoch   6 |  1000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  6.15 | ppl   469.50\n",
      "| epoch   6 |  1200/29454 batches | lr 1.00 | ms/batch 153.06 | loss  5.94 | ppl   378.38\n",
      "| epoch   6 |  1400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.96 | ppl   388.94\n",
      "| epoch   6 |  1600/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.94 | ppl   379.21\n",
      "| epoch   6 |  1800/29454 batches | lr 1.00 | ms/batch 153.05 | loss  6.05 | ppl   425.39\n",
      "| epoch   6 |  2000/29454 batches | lr 1.00 | ms/batch 153.05 | loss  6.02 | ppl   410.57\n",
      "| epoch   6 |  2200/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.90 | ppl   365.07\n",
      "| epoch   6 |  2400/29454 batches | lr 1.00 | ms/batch 153.00 | loss  6.09 | ppl   442.60\n",
      "| epoch   6 |  2600/29454 batches | lr 1.00 | ms/batch 152.96 | loss  6.01 | ppl   407.62\n",
      "| epoch   6 |  2800/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.92 | ppl   372.28\n",
      "| epoch   6 |  3000/29454 batches | lr 1.00 | ms/batch 153.01 | loss  6.32 | ppl   554.98\n",
      "| epoch   6 |  3200/29454 batches | lr 1.00 | ms/batch 153.08 | loss  6.07 | ppl   433.97\n",
      "| epoch   6 |  3400/29454 batches | lr 1.00 | ms/batch 153.24 | loss  6.08 | ppl   436.00\n",
      "| epoch   6 |  3600/29454 batches | lr 1.00 | ms/batch 153.15 | loss  6.06 | ppl   429.19\n",
      "| epoch   6 |  3800/29454 batches | lr 1.00 | ms/batch 153.08 | loss  6.00 | ppl   401.43\n",
      "| epoch   6 |  4000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.99 | ppl   397.64\n",
      "| epoch   6 |  4200/29454 batches | lr 1.00 | ms/batch 153.08 | loss  5.98 | ppl   395.00\n",
      "| epoch   6 |  4400/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.95 | ppl   383.84\n",
      "| epoch   6 |  4600/29454 batches | lr 1.00 | ms/batch 153.05 | loss  6.04 | ppl   421.35\n",
      "| epoch   6 |  4800/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.98 | ppl   396.69\n",
      "| epoch   6 |  5000/29454 batches | lr 1.00 | ms/batch 153.10 | loss  6.12 | ppl   455.80\n",
      "| epoch   6 |  5200/29454 batches | lr 1.00 | ms/batch 153.02 | loss  6.05 | ppl   423.79\n",
      "| epoch   6 |  5400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  6.10 | ppl   444.91\n",
      "| epoch   6 |  5600/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.81 | ppl   333.10\n",
      "| epoch   6 |  5800/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.98 | ppl   394.31\n",
      "| epoch   6 |  6000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  6.23 | ppl   507.24\n",
      "| epoch   6 |  6200/29454 batches | lr 1.00 | ms/batch 153.01 | loss  6.04 | ppl   418.80\n",
      "| epoch   6 |  6400/29454 batches | lr 1.00 | ms/batch 152.95 | loss  6.16 | ppl   472.63\n",
      "| epoch   6 |  6600/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.16 | ppl   475.03\n",
      "| epoch   6 |  6800/29454 batches | lr 1.00 | ms/batch 152.97 | loss  6.12 | ppl   455.89\n",
      "| epoch   6 |  7000/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.12 | ppl   454.35\n",
      "| epoch   6 |  7200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  6.06 | ppl   427.59\n",
      "| epoch   6 |  7400/29454 batches | lr 1.00 | ms/batch 153.04 | loss  6.07 | ppl   431.67\n",
      "| epoch   6 |  7600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.18 | ppl   482.62\n",
      "| epoch   6 |  7800/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.10 | ppl   443.66\n",
      "| epoch   6 |  8000/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.39 | ppl   598.25\n",
      "| epoch   6 |  8200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  6.02 | ppl   412.67\n",
      "| epoch   6 |  8400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.86 | ppl   350.18\n",
      "| epoch   6 |  8600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.13 | ppl   458.93\n",
      "| epoch   6 |  8800/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.08 | ppl   436.41\n",
      "| epoch   6 |  9000/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.88 | ppl   358.90\n",
      "| epoch   6 |  9200/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.07 | ppl   433.99\n",
      "| epoch   6 |  9400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.97 | ppl   392.23\n",
      "| epoch   6 |  9600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.11 | ppl   450.08\n",
      "| epoch   6 |  9800/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.07 | ppl   432.25\n",
      "| epoch   6 | 10000/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.08 | ppl   438.64\n",
      "| epoch   6 | 10200/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.08 | ppl   437.42\n",
      "| epoch   6 | 10400/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.93 | ppl   377.73\n",
      "| epoch   6 | 10600/29454 batches | lr 1.00 | ms/batch 152.94 | loss  6.08 | ppl   435.92\n",
      "| epoch   6 | 10800/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.84 | ppl   345.23\n",
      "| epoch   6 | 11000/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.87 | ppl   355.53\n",
      "| epoch   6 | 11200/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.92 | ppl   372.60\n",
      "| epoch   6 | 11400/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.87 | ppl   354.90\n",
      "| epoch   6 | 11600/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.85 | ppl   347.78\n",
      "| epoch   6 | 11800/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.89 | ppl   360.45\n",
      "| epoch   6 | 12000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  6.08 | ppl   434.86\n",
      "| epoch   6 | 12200/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.09 | ppl   442.86\n",
      "| epoch   6 | 12400/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.09 | ppl   441.72\n",
      "| epoch   6 | 12600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.84 | ppl   343.42\n",
      "| epoch   6 | 12800/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.96 | ppl   386.94\n",
      "| epoch   6 | 13000/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.85 | ppl   346.17\n",
      "| epoch   6 | 13200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.82 | ppl   338.52\n",
      "| epoch   6 | 13400/29454 batches | lr 1.00 | ms/batch 152.84 | loss  5.96 | ppl   385.93\n",
      "| epoch   6 | 13600/29454 batches | lr 1.00 | ms/batch 152.84 | loss  5.99 | ppl   399.06\n",
      "| epoch   6 | 13800/29454 batches | lr 1.00 | ms/batch 152.82 | loss  6.16 | ppl   475.27\n",
      "| epoch   6 | 14000/29454 batches | lr 1.00 | ms/batch 152.92 | loss  6.03 | ppl   414.42\n",
      "| epoch   6 | 14200/29454 batches | lr 1.00 | ms/batch 152.80 | loss  6.04 | ppl   419.53\n",
      "| epoch   6 | 14400/29454 batches | lr 1.00 | ms/batch 152.95 | loss  6.15 | ppl   470.40\n",
      "| epoch   6 | 14600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.02 | ppl   409.76\n",
      "| epoch   6 | 14800/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.92 | ppl   373.95\n",
      "| epoch   6 | 15000/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.92 | ppl   372.09\n",
      "| epoch   6 | 15200/29454 batches | lr 1.00 | ms/batch 153.11 | loss  5.85 | ppl   346.68\n",
      "| epoch   6 | 15400/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.86 | ppl   352.37\n",
      "| epoch   6 | 15600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.89 | ppl   361.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 | 15800/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.85 | ppl   348.48\n",
      "| epoch   6 | 16000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.90 | ppl   366.75\n",
      "| epoch   6 | 16200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.93 | ppl   377.28\n",
      "| epoch   6 | 16400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  6.01 | ppl   407.54\n",
      "| epoch   6 | 16600/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.87 | ppl   353.56\n",
      "| epoch   6 | 16800/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.92 | ppl   373.95\n",
      "| epoch   6 | 17000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.93 | ppl   375.36\n",
      "| epoch   6 | 17200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  6.07 | ppl   434.14\n",
      "| epoch   6 | 17400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.84 | ppl   344.59\n",
      "| epoch   6 | 17600/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.89 | ppl   360.13\n",
      "| epoch   6 | 17800/29454 batches | lr 1.00 | ms/batch 152.95 | loss  6.06 | ppl   428.42\n",
      "| epoch   6 | 18000/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.93 | ppl   374.65\n",
      "| epoch   6 | 18200/29454 batches | lr 1.00 | ms/batch 152.89 | loss  6.00 | ppl   404.47\n",
      "| epoch   6 | 18400/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.02 | ppl   410.16\n",
      "| epoch   6 | 18600/29454 batches | lr 1.00 | ms/batch 152.83 | loss  6.14 | ppl   463.31\n",
      "| epoch   6 | 18800/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.11 | ppl   451.41\n",
      "| epoch   6 | 19000/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.88 | ppl   356.18\n",
      "| epoch   6 | 19200/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.84 | ppl   345.19\n",
      "| epoch   6 | 19400/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.92 | ppl   373.76\n",
      "| epoch   6 | 19600/29454 batches | lr 1.00 | ms/batch 152.83 | loss  5.82 | ppl   336.08\n",
      "| epoch   6 | 19800/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.85 | ppl   346.12\n",
      "| epoch   6 | 20000/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.03 | ppl   415.02\n",
      "| epoch   6 | 20200/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.94 | ppl   378.23\n",
      "| epoch   6 | 20400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.89 | ppl   361.13\n",
      "| epoch   6 | 20600/29454 batches | lr 1.00 | ms/batch 161.67 | loss  6.04 | ppl   418.48\n",
      "| epoch   6 | 20800/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.89 | ppl   359.90\n",
      "| epoch   6 | 21000/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.87 | ppl   354.22\n",
      "| epoch   6 | 21200/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.92 | ppl   371.58\n",
      "| epoch   6 | 21400/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.93 | ppl   375.79\n",
      "| epoch   6 | 21600/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.90 | ppl   363.38\n",
      "| epoch   6 | 21800/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.99 | ppl   399.96\n",
      "| epoch   6 | 22000/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.95 | ppl   382.72\n",
      "| epoch   6 | 22200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.86 | ppl   351.59\n",
      "| epoch   6 | 22400/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.97 | ppl   391.40\n",
      "| epoch   6 | 22600/29454 batches | lr 1.00 | ms/batch 152.92 | loss  6.01 | ppl   408.00\n",
      "| epoch   6 | 22800/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.93 | ppl   376.96\n",
      "| epoch   6 | 23000/29454 batches | lr 1.00 | ms/batch 153.02 | loss  6.05 | ppl   422.76\n",
      "| epoch   6 | 23200/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.98 | ppl   394.83\n",
      "| epoch   6 | 23400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.89 | ppl   360.23\n",
      "| epoch   6 | 23600/29454 batches | lr 1.00 | ms/batch 152.87 | loss  6.04 | ppl   419.96\n",
      "| epoch   6 | 23800/29454 batches | lr 1.00 | ms/batch 152.86 | loss  5.96 | ppl   387.75\n",
      "| epoch   6 | 24000/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.97 | ppl   392.39\n",
      "| epoch   6 | 24200/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.73 | ppl   308.94\n",
      "| epoch   6 | 24400/29454 batches | lr 1.00 | ms/batch 152.82 | loss  5.81 | ppl   334.46\n",
      "| epoch   6 | 24600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.95 | ppl   385.23\n",
      "| epoch   6 | 24800/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.91 | ppl   369.41\n",
      "| epoch   6 | 25000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.87 | ppl   355.67\n",
      "| epoch   6 | 25200/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.86 | ppl   351.64\n",
      "| epoch   6 | 25400/29454 batches | lr 1.00 | ms/batch 152.86 | loss  5.98 | ppl   395.11\n",
      "| epoch   6 | 25600/29454 batches | lr 1.00 | ms/batch 152.83 | loss  5.94 | ppl   381.52\n",
      "| epoch   6 | 25800/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.94 | ppl   378.37\n",
      "| epoch   6 | 26000/29454 batches | lr 1.00 | ms/batch 152.84 | loss  6.05 | ppl   423.33\n",
      "| epoch   6 | 26200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.80 | ppl   329.30\n",
      "| epoch   6 | 26400/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.95 | ppl   382.94\n",
      "| epoch   6 | 26600/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.94 | ppl   378.77\n",
      "| epoch   6 | 26800/29454 batches | lr 1.00 | ms/batch 153.02 | loss  6.14 | ppl   463.42\n",
      "| epoch   6 | 27000/29454 batches | lr 1.00 | ms/batch 153.11 | loss  6.06 | ppl   426.37\n",
      "| epoch   6 | 27200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.97 | ppl   390.15\n",
      "| epoch   6 | 27400/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.96 | ppl   387.56\n",
      "| epoch   6 | 27600/29454 batches | lr 1.00 | ms/batch 152.92 | loss  6.00 | ppl   403.45\n",
      "| epoch   6 | 27800/29454 batches | lr 1.00 | ms/batch 152.90 | loss  6.03 | ppl   413.75\n",
      "| epoch   6 | 28000/29454 batches | lr 1.00 | ms/batch 152.91 | loss  6.05 | ppl   425.16\n",
      "| epoch   6 | 28200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  6.00 | ppl   402.88\n",
      "| epoch   6 | 28400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  6.01 | ppl   405.72\n",
      "| epoch   6 | 28600/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.82 | ppl   338.57\n",
      "| epoch   6 | 28800/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.86 | ppl   351.94\n",
      "| epoch   6 | 29000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.96 | ppl   386.91\n",
      "| epoch   6 | 29200/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.85 | ppl   347.13\n",
      "| epoch   6 | 29400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.85 | ppl   346.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 4677.01s | valid loss  6.35 | valid ppl   570.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/29454 batches | lr 1.00 | ms/batch 155.29 | loss  6.08 | ppl   436.03\n",
      "| epoch   7 |   400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.93 | ppl   376.82\n",
      "| epoch   7 |   600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.85 | ppl   346.90\n",
      "| epoch   7 |   800/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.80 | ppl   329.90\n",
      "| epoch   7 |  1000/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.99 | ppl   401.41\n",
      "| epoch   7 |  1200/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.78 | ppl   323.02\n",
      "| epoch   7 |  1400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.82 | ppl   335.61\n",
      "| epoch   7 |  1600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.78 | ppl   324.80\n",
      "| epoch   7 |  1800/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.89 | ppl   362.38\n",
      "| epoch   7 |  2000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.87 | ppl   353.79\n",
      "| epoch   7 |  2200/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.74 | ppl   311.08\n",
      "| epoch   7 |  2400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.94 | ppl   378.71\n",
      "| epoch   7 |  2600/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.85 | ppl   348.92\n",
      "| epoch   7 |  2800/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.77 | ppl   321.61\n",
      "| epoch   7 |  3000/29454 batches | lr 1.00 | ms/batch 152.99 | loss  6.15 | ppl   469.37\n",
      "| epoch   7 |  3200/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.91 | ppl   370.36\n",
      "| epoch   7 |  3400/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.92 | ppl   371.21\n",
      "| epoch   7 |  3600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.90 | ppl   366.74\n",
      "| epoch   7 |  3800/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.84 | ppl   342.82\n",
      "| epoch   7 |  4000/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.83 | ppl   339.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |  4200/29454 batches | lr 1.00 | ms/batch 153.74 | loss  5.82 | ppl   337.34\n",
      "| epoch   7 |  4400/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.79 | ppl   326.00\n",
      "| epoch   7 |  4600/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.90 | ppl   364.31\n",
      "| epoch   7 |  4800/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.82 | ppl   338.56\n",
      "| epoch   7 |  5000/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.97 | ppl   391.07\n",
      "| epoch   7 |  5200/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.89 | ppl   363.17\n",
      "| epoch   7 |  5400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.94 | ppl   381.70\n",
      "| epoch   7 |  5600/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.66 | ppl   287.87\n",
      "| epoch   7 |  5800/29454 batches | lr 1.00 | ms/batch 153.07 | loss  5.82 | ppl   338.03\n",
      "| epoch   7 |  6000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  6.07 | ppl   434.81\n",
      "| epoch   7 |  6200/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.89 | ppl   360.33\n",
      "| epoch   7 |  6400/29454 batches | lr 1.00 | ms/batch 153.00 | loss  6.01 | ppl   406.47\n",
      "| epoch   7 |  6600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  6.01 | ppl   406.66\n",
      "| epoch   7 |  6800/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.97 | ppl   391.06\n",
      "| epoch   7 |  7000/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.98 | ppl   393.88\n",
      "| epoch   7 |  7200/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.91 | ppl   367.44\n",
      "| epoch   7 |  7400/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.92 | ppl   372.52\n",
      "| epoch   7 |  7600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  6.02 | ppl   411.89\n",
      "| epoch   7 |  7800/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.94 | ppl   379.80\n",
      "| epoch   7 |  8000/29454 batches | lr 1.00 | ms/batch 153.09 | loss  6.23 | ppl   507.65\n",
      "| epoch   7 |  8200/29454 batches | lr 1.00 | ms/batch 153.10 | loss  5.87 | ppl   354.71\n",
      "| epoch   7 |  8400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.69 | ppl   296.77\n",
      "| epoch   7 |  8600/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.98 | ppl   393.65\n",
      "| epoch   7 |  8800/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.92 | ppl   373.89\n",
      "| epoch   7 |  9000/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.73 | ppl   308.40\n",
      "| epoch   7 |  9200/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.91 | ppl   369.60\n",
      "| epoch   7 |  9400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.83 | ppl   338.91\n",
      "| epoch   7 |  9600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.96 | ppl   389.48\n",
      "| epoch   7 |  9800/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.93 | ppl   374.55\n",
      "| epoch   7 | 10000/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.94 | ppl   378.59\n",
      "| epoch   7 | 10200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.93 | ppl   377.50\n",
      "| epoch   7 | 10400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.79 | ppl   325.78\n",
      "| epoch   7 | 10600/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.93 | ppl   375.15\n",
      "| epoch   7 | 10800/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.69 | ppl   296.97\n",
      "| epoch   7 | 11000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.73 | ppl   307.42\n",
      "| epoch   7 | 11200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.77 | ppl   321.16\n",
      "| epoch   7 | 11400/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.72 | ppl   304.32\n",
      "| epoch   7 | 11600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.69 | ppl   296.44\n",
      "| epoch   7 | 11800/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.75 | ppl   312.74\n",
      "| epoch   7 | 12000/29454 batches | lr 1.00 | ms/batch 153.10 | loss  5.93 | ppl   376.09\n",
      "| epoch   7 | 12200/29454 batches | lr 1.00 | ms/batch 152.86 | loss  5.94 | ppl   380.08\n",
      "| epoch   7 | 12400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.94 | ppl   379.82\n",
      "| epoch   7 | 12600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.69 | ppl   296.42\n",
      "| epoch   7 | 12800/29454 batches | lr 1.00 | ms/batch 152.83 | loss  5.81 | ppl   332.25\n",
      "| epoch   7 | 13000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.71 | ppl   300.97\n",
      "| epoch   7 | 13200/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.68 | ppl   292.64\n",
      "| epoch   7 | 13400/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.81 | ppl   334.80\n",
      "| epoch   7 | 13600/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.86 | ppl   349.68\n",
      "| epoch   7 | 13800/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.01 | ppl   405.86\n",
      "| epoch   7 | 14000/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.87 | ppl   354.24\n",
      "| epoch   7 | 14200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.90 | ppl   363.22\n",
      "| epoch   7 | 14400/29454 batches | lr 1.00 | ms/batch 152.88 | loss  6.00 | ppl   404.41\n",
      "| epoch   7 | 14600/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.87 | ppl   352.76\n",
      "| epoch   7 | 14800/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.78 | ppl   324.21\n",
      "| epoch   7 | 15000/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.77 | ppl   321.71\n",
      "| epoch   7 | 15200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.71 | ppl   301.68\n",
      "| epoch   7 | 15400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.71 | ppl   302.93\n",
      "| epoch   7 | 15600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.74 | ppl   311.65\n",
      "| epoch   7 | 15800/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.71 | ppl   301.39\n",
      "| epoch   7 | 16000/29454 batches | lr 1.00 | ms/batch 153.10 | loss  5.75 | ppl   315.75\n",
      "| epoch   7 | 16200/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.79 | ppl   327.53\n",
      "| epoch   7 | 16400/29454 batches | lr 1.00 | ms/batch 152.86 | loss  5.85 | ppl   347.40\n",
      "| epoch   7 | 16600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.72 | ppl   303.94\n",
      "| epoch   7 | 16800/29454 batches | lr 1.00 | ms/batch 152.82 | loss  5.78 | ppl   324.50\n",
      "| epoch   7 | 17000/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.78 | ppl   323.96\n",
      "| epoch   7 | 17200/29454 batches | lr 1.00 | ms/batch 152.86 | loss  5.93 | ppl   374.65\n",
      "| epoch   7 | 17400/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.70 | ppl   297.64\n",
      "| epoch   7 | 17600/29454 batches | lr 1.00 | ms/batch 152.84 | loss  5.75 | ppl   313.69\n",
      "| epoch   7 | 17800/29454 batches | lr 1.00 | ms/batch 152.79 | loss  5.92 | ppl   372.71\n",
      "| epoch   7 | 18000/29454 batches | lr 1.00 | ms/batch 152.83 | loss  5.79 | ppl   326.06\n",
      "| epoch   7 | 18200/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.86 | ppl   349.43\n",
      "| epoch   7 | 18400/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.87 | ppl   355.82\n",
      "| epoch   7 | 18600/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.99 | ppl   398.83\n",
      "| epoch   7 | 18800/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.96 | ppl   386.81\n",
      "| epoch   7 | 19000/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.73 | ppl   308.92\n",
      "| epoch   7 | 19200/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.70 | ppl   299.34\n",
      "| epoch   7 | 19400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.78 | ppl   323.14\n",
      "| epoch   7 | 19600/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.68 | ppl   292.44\n",
      "| epoch   7 | 19800/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.70 | ppl   300.07\n",
      "| epoch   7 | 20000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.90 | ppl   364.36\n",
      "| epoch   7 | 20200/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.79 | ppl   325.96\n",
      "| epoch   7 | 20400/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.75 | ppl   315.00\n",
      "| epoch   7 | 20600/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.90 | ppl   364.05\n",
      "| epoch   7 | 20800/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.74 | ppl   312.29\n",
      "| epoch   7 | 21000/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.73 | ppl   306.51\n",
      "| epoch   7 | 21200/29454 batches | lr 1.00 | ms/batch 152.82 | loss  5.77 | ppl   321.61\n",
      "| epoch   7 | 21400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.79 | ppl   327.34\n",
      "| epoch   7 | 21600/29454 batches | lr 1.00 | ms/batch 152.84 | loss  5.75 | ppl   315.50\n",
      "| epoch   7 | 21800/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.85 | ppl   347.00\n",
      "| epoch   7 | 22000/29454 batches | lr 1.00 | ms/batch 152.84 | loss  5.80 | ppl   331.13\n",
      "| epoch   7 | 22200/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.72 | ppl   303.44\n",
      "| epoch   7 | 22400/29454 batches | lr 1.00 | ms/batch 152.79 | loss  5.82 | ppl   336.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 | 22600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.87 | ppl   354.24\n",
      "| epoch   7 | 22800/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.79 | ppl   328.57\n",
      "| epoch   7 | 23000/29454 batches | lr 1.00 | ms/batch 152.84 | loss  5.90 | ppl   364.60\n",
      "| epoch   7 | 23200/29454 batches | lr 1.00 | ms/batch 152.87 | loss  5.84 | ppl   343.07\n",
      "| epoch   7 | 23400/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.75 | ppl   314.05\n",
      "| epoch   7 | 23600/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.90 | ppl   365.63\n",
      "| epoch   7 | 23800/29454 batches | lr 1.00 | ms/batch 153.13 | loss  5.81 | ppl   334.38\n",
      "| epoch   7 | 24000/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.83 | ppl   339.18\n",
      "| epoch   7 | 24200/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.60 | ppl   270.06\n",
      "| epoch   7 | 24400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.67 | ppl   290.14\n",
      "| epoch   7 | 24600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.81 | ppl   335.21\n",
      "| epoch   7 | 24800/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.77 | ppl   319.66\n",
      "| epoch   7 | 25000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.74 | ppl   311.07\n",
      "| epoch   7 | 25200/29454 batches | lr 1.00 | ms/batch 153.05 | loss  5.72 | ppl   304.96\n",
      "| epoch   7 | 25400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.84 | ppl   342.86\n",
      "| epoch   7 | 25600/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.80 | ppl   331.48\n",
      "| epoch   7 | 25800/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.80 | ppl   329.25\n",
      "| epoch   7 | 26000/29454 batches | lr 1.00 | ms/batch 153.07 | loss  5.89 | ppl   361.79\n",
      "| epoch   7 | 26200/29454 batches | lr 1.00 | ms/batch 153.08 | loss  5.65 | ppl   282.96\n",
      "| epoch   7 | 26400/29454 batches | lr 1.00 | ms/batch 153.06 | loss  5.79 | ppl   328.16\n",
      "| epoch   7 | 26600/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.79 | ppl   327.38\n",
      "| epoch   7 | 26800/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.99 | ppl   399.04\n",
      "| epoch   7 | 27000/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.90 | ppl   366.03\n",
      "| epoch   7 | 27200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.82 | ppl   336.32\n",
      "| epoch   7 | 27400/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.82 | ppl   336.52\n",
      "| epoch   7 | 27600/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.85 | ppl   348.53\n",
      "| epoch   7 | 27800/29454 batches | lr 1.00 | ms/batch 153.22 | loss  5.88 | ppl   359.28\n",
      "| epoch   7 | 28000/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.91 | ppl   366.96\n",
      "| epoch   7 | 28200/29454 batches | lr 1.00 | ms/batch 153.05 | loss  5.85 | ppl   347.61\n",
      "| epoch   7 | 28400/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.86 | ppl   351.50\n",
      "| epoch   7 | 28600/29454 batches | lr 1.00 | ms/batch 153.06 | loss  5.68 | ppl   294.20\n",
      "| epoch   7 | 28800/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.72 | ppl   304.46\n",
      "| epoch   7 | 29000/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.82 | ppl   336.54\n",
      "| epoch   7 | 29200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.70 | ppl   299.97\n",
      "| epoch   7 | 29400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.71 | ppl   300.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 4676.12s | valid loss  6.34 | valid ppl   568.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/29454 batches | lr 1.00 | ms/batch 153.94 | loss  5.94 | ppl   378.81\n",
      "| epoch   8 |   400/29454 batches | lr 1.00 | ms/batch 153.11 | loss  5.79 | ppl   327.77\n",
      "| epoch   8 |   600/29454 batches | lr 1.00 | ms/batch 153.05 | loss  5.70 | ppl   299.29\n",
      "| epoch   8 |   800/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.65 | ppl   283.05\n",
      "| epoch   8 |  1000/29454 batches | lr 1.00 | ms/batch 153.17 | loss  5.86 | ppl   349.75\n",
      "| epoch   8 |  1200/29454 batches | lr 1.00 | ms/batch 153.12 | loss  5.64 | ppl   280.22\n",
      "| epoch   8 |  1400/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.67 | ppl   290.78\n",
      "| epoch   8 |  1600/29454 batches | lr 1.00 | ms/batch 153.05 | loss  5.64 | ppl   282.13\n",
      "| epoch   8 |  1800/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.74 | ppl   311.04\n",
      "| epoch   8 |  2000/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.73 | ppl   308.01\n",
      "| epoch   8 |  2200/29454 batches | lr 1.00 | ms/batch 153.17 | loss  5.59 | ppl   267.01\n",
      "| epoch   8 |  2400/29454 batches | lr 1.00 | ms/batch 153.05 | loss  5.79 | ppl   326.76\n",
      "| epoch   8 |  2600/29454 batches | lr 1.00 | ms/batch 153.08 | loss  5.72 | ppl   304.41\n",
      "| epoch   8 |  2800/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.63 | ppl   277.62\n",
      "| epoch   8 |  3000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.99 | ppl   399.98\n",
      "| epoch   8 |  3200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.77 | ppl   319.41\n",
      "| epoch   8 |  3400/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.77 | ppl   319.55\n",
      "| epoch   8 |  3600/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.76 | ppl   315.86\n",
      "| epoch   8 |  3800/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.70 | ppl   297.74\n",
      "| epoch   8 |  4000/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.68 | ppl   292.62\n",
      "| epoch   8 |  4200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.67 | ppl   290.57\n",
      "| epoch   8 |  4400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.64 | ppl   281.10\n",
      "| epoch   8 |  4600/29454 batches | lr 1.00 | ms/batch 153.06 | loss  5.77 | ppl   319.60\n",
      "| epoch   8 |  4800/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.69 | ppl   295.00\n",
      "| epoch   8 |  5000/29454 batches | lr 1.00 | ms/batch 153.10 | loss  5.82 | ppl   338.43\n",
      "| epoch   8 |  5200/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.75 | ppl   314.78\n",
      "| epoch   8 |  5400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.80 | ppl   328.95\n",
      "| epoch   8 |  5600/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.53 | ppl   250.97\n",
      "| epoch   8 |  5800/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.68 | ppl   292.25\n",
      "| epoch   8 |  6000/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.94 | ppl   378.28\n",
      "| epoch   8 |  6200/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.75 | ppl   314.64\n",
      "| epoch   8 |  6400/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.86 | ppl   350.23\n",
      "| epoch   8 |  6600/29454 batches | lr 1.00 | ms/batch 152.88 | loss  5.86 | ppl   352.41\n",
      "| epoch   8 |  6800/29454 batches | lr 1.00 | ms/batch 152.85 | loss  5.84 | ppl   342.65\n",
      "| epoch   8 |  7000/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.83 | ppl   341.36\n",
      "| epoch   8 |  7200/29454 batches | lr 1.00 | ms/batch 152.86 | loss  5.77 | ppl   318.97\n",
      "| epoch   8 |  7400/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.78 | ppl   325.27\n",
      "| epoch   8 |  7600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.87 | ppl   354.40\n",
      "| epoch   8 |  7800/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.80 | ppl   329.27\n",
      "| epoch   8 |  8000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  6.08 | ppl   435.17\n",
      "| epoch   8 |  8200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.73 | ppl   307.31\n",
      "| epoch   8 |  8400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.54 | ppl   255.10\n",
      "| epoch   8 |  8600/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.83 | ppl   341.01\n",
      "| epoch   8 |  8800/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.79 | ppl   325.59\n",
      "| epoch   8 |  9000/29454 batches | lr 1.00 | ms/batch 153.11 | loss  5.60 | ppl   269.12\n",
      "| epoch   8 |  9200/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.77 | ppl   320.81\n",
      "| epoch   8 |  9400/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.69 | ppl   294.75\n",
      "| epoch   8 |  9600/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.83 | ppl   341.45\n",
      "| epoch   8 |  9800/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.79 | ppl   326.52\n",
      "| epoch   8 | 10000/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.80 | ppl   330.45\n",
      "| epoch   8 | 10200/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.79 | ppl   325.54\n",
      "| epoch   8 | 10400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.65 | ppl   285.36\n",
      "| epoch   8 | 10600/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.80 | ppl   328.91\n",
      "| epoch   8 | 10800/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.56 | ppl   260.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 | 11000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.59 | ppl   266.67\n",
      "| epoch   8 | 11200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.63 | ppl   278.16\n",
      "| epoch   8 | 11400/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.58 | ppl   263.84\n",
      "| epoch   8 | 11600/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.55 | ppl   257.05\n",
      "| epoch   8 | 11800/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.60 | ppl   270.91\n",
      "| epoch   8 | 12000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.79 | ppl   325.69\n",
      "| epoch   8 | 12200/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.80 | ppl   331.34\n",
      "| epoch   8 | 12400/29454 batches | lr 1.00 | ms/batch 153.06 | loss  5.79 | ppl   327.78\n",
      "| epoch   8 | 12600/29454 batches | lr 1.00 | ms/batch 153.14 | loss  5.55 | ppl   258.40\n",
      "| epoch   8 | 12800/29454 batches | lr 1.00 | ms/batch 153.23 | loss  5.67 | ppl   290.62\n",
      "| epoch   8 | 13000/29454 batches | lr 1.00 | ms/batch 153.09 | loss  5.57 | ppl   262.38\n",
      "| epoch   8 | 13200/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.53 | ppl   253.07\n",
      "| epoch   8 | 13400/29454 batches | lr 1.00 | ms/batch 153.11 | loss  5.67 | ppl   290.28\n",
      "| epoch   8 | 13600/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.72 | ppl   304.88\n",
      "| epoch   8 | 13800/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.87 | ppl   353.87\n",
      "| epoch   8 | 14000/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.73 | ppl   308.12\n",
      "| epoch   8 | 14200/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.75 | ppl   315.17\n",
      "| epoch   8 | 14400/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.86 | ppl   350.31\n",
      "| epoch   8 | 14600/29454 batches | lr 1.00 | ms/batch 153.11 | loss  5.73 | ppl   306.87\n",
      "| epoch   8 | 14800/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.65 | ppl   283.61\n",
      "| epoch   8 | 15000/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.64 | ppl   280.07\n",
      "| epoch   8 | 15200/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.58 | ppl   263.99\n",
      "| epoch   8 | 15400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.58 | ppl   264.96\n",
      "| epoch   8 | 15600/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.60 | ppl   271.24\n",
      "| epoch   8 | 15800/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.58 | ppl   264.78\n",
      "| epoch   8 | 16000/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.62 | ppl   275.32\n",
      "| epoch   8 | 16200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.66 | ppl   285.85\n",
      "| epoch   8 | 16400/29454 batches | lr 1.00 | ms/batch 153.08 | loss  5.70 | ppl   299.75\n",
      "| epoch   8 | 16600/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.58 | ppl   265.53\n",
      "| epoch   8 | 16800/29454 batches | lr 1.00 | ms/batch 153.15 | loss  5.64 | ppl   280.07\n",
      "| epoch   8 | 17000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.64 | ppl   280.25\n",
      "| epoch   8 | 17200/29454 batches | lr 1.00 | ms/batch 153.02 | loss  5.79 | ppl   327.89\n",
      "| epoch   8 | 17400/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.55 | ppl   258.33\n",
      "| epoch   8 | 17600/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.61 | ppl   274.37\n",
      "| epoch   8 | 17800/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.79 | ppl   325.85\n",
      "| epoch   8 | 18000/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.64 | ppl   282.54\n",
      "| epoch   8 | 18200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.72 | ppl   306.00\n",
      "| epoch   8 | 18400/29454 batches | lr 1.00 | ms/batch 153.10 | loss  5.74 | ppl   310.62\n",
      "| epoch   8 | 18600/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.84 | ppl   344.53\n",
      "| epoch   8 | 18800/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.81 | ppl   334.11\n",
      "| epoch   8 | 19000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.59 | ppl   268.49\n",
      "| epoch   8 | 19200/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.57 | ppl   262.48\n",
      "| epoch   8 | 19400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.64 | ppl   282.22\n",
      "| epoch   8 | 19600/29454 batches | lr 1.00 | ms/batch 152.92 | loss  5.55 | ppl   257.05\n",
      "| epoch   8 | 19800/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.57 | ppl   262.48\n",
      "| epoch   8 | 20000/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.78 | ppl   322.83\n",
      "| epoch   8 | 20200/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.65 | ppl   283.29\n",
      "| epoch   8 | 20400/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.62 | ppl   275.18\n",
      "| epoch   8 | 20600/29454 batches | lr 1.00 | ms/batch 153.05 | loss  5.76 | ppl   317.69\n",
      "| epoch   8 | 20800/29454 batches | lr 1.00 | ms/batch 153.09 | loss  5.61 | ppl   272.25\n",
      "| epoch   8 | 21000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.58 | ppl   266.27\n",
      "| epoch   8 | 21200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.65 | ppl   283.23\n",
      "| epoch   8 | 21400/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.66 | ppl   286.52\n",
      "| epoch   8 | 21600/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.62 | ppl   275.42\n",
      "| epoch   8 | 21800/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.72 | ppl   304.22\n",
      "| epoch   8 | 22000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.66 | ppl   288.23\n",
      "| epoch   8 | 22200/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.58 | ppl   264.77\n",
      "| epoch   8 | 22400/29454 batches | lr 1.00 | ms/batch 153.06 | loss  5.67 | ppl   290.44\n",
      "| epoch   8 | 22600/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.73 | ppl   308.91\n",
      "| epoch   8 | 22800/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.66 | ppl   286.95\n",
      "| epoch   8 | 23000/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.76 | ppl   317.88\n",
      "| epoch   8 | 23200/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.71 | ppl   302.76\n",
      "| epoch   8 | 23400/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.61 | ppl   274.29\n",
      "| epoch   8 | 23600/29454 batches | lr 1.00 | ms/batch 152.97 | loss  5.76 | ppl   316.63\n",
      "| epoch   8 | 23800/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.68 | ppl   292.72\n",
      "| epoch   8 | 24000/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.68 | ppl   294.04\n",
      "| epoch   8 | 24200/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.47 | ppl   237.17\n",
      "| epoch   8 | 24400/29454 batches | lr 1.00 | ms/batch 152.99 | loss  5.54 | ppl   255.18\n",
      "| epoch   8 | 24600/29454 batches | lr 1.00 | ms/batch 153.07 | loss  5.68 | ppl   294.15\n",
      "| epoch   8 | 24800/29454 batches | lr 1.00 | ms/batch 153.05 | loss  5.63 | ppl   277.59\n",
      "| epoch   8 | 25000/29454 batches | lr 1.00 | ms/batch 153.00 | loss  5.61 | ppl   273.57\n",
      "| epoch   8 | 25200/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.58 | ppl   264.88\n",
      "| epoch   8 | 25400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.70 | ppl   299.06\n",
      "| epoch   8 | 25600/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.66 | ppl   287.02\n",
      "| epoch   8 | 25800/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.67 | ppl   288.82\n",
      "| epoch   8 | 26000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.74 | ppl   312.23\n",
      "| epoch   8 | 26200/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.51 | ppl   248.06\n",
      "| epoch   8 | 26400/29454 batches | lr 1.00 | ms/batch 152.93 | loss  5.65 | ppl   283.64\n",
      "| epoch   8 | 26600/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.66 | ppl   286.10\n",
      "| epoch   8 | 26800/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.86 | ppl   349.38\n",
      "| epoch   8 | 27000/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.76 | ppl   317.42\n",
      "| epoch   8 | 27200/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.68 | ppl   293.90\n",
      "| epoch   8 | 27400/29454 batches | lr 1.00 | ms/batch 152.98 | loss  5.69 | ppl   294.95\n",
      "| epoch   8 | 27600/29454 batches | lr 1.00 | ms/batch 152.95 | loss  5.71 | ppl   302.72\n",
      "| epoch   8 | 27800/29454 batches | lr 1.00 | ms/batch 152.90 | loss  5.74 | ppl   311.93\n",
      "| epoch   8 | 28000/29454 batches | lr 1.00 | ms/batch 152.96 | loss  5.77 | ppl   319.67\n",
      "| epoch   8 | 28200/29454 batches | lr 1.00 | ms/batch 153.03 | loss  5.72 | ppl   304.25\n",
      "| epoch   8 | 28400/29454 batches | lr 1.00 | ms/batch 153.01 | loss  5.73 | ppl   308.83\n",
      "| epoch   8 | 28600/29454 batches | lr 1.00 | ms/batch 153.04 | loss  5.56 | ppl   258.73\n",
      "| epoch   8 | 28800/29454 batches | lr 1.00 | ms/batch 152.91 | loss  5.58 | ppl   266.06\n",
      "| epoch   8 | 29000/29454 batches | lr 1.00 | ms/batch 152.89 | loss  5.68 | ppl   291.90\n",
      "| epoch   8 | 29200/29454 batches | lr 1.00 | ms/batch 152.84 | loss  5.56 | ppl   259.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 | 29400/29454 batches | lr 1.00 | ms/batch 152.94 | loss  5.57 | ppl   263.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 4676.89s | valid loss  6.35 | valid ppl   571.52\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs. Save the model if the validation loss is the best\n",
    "# we've seen so far. Adjust the learning rate after each epoch.\n",
    "import time\n",
    "import copy\n",
    "best_val_loss = float('inf')\n",
    "epochs = 8\n",
    "best_model = None\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    eval_loss = evaluate(model, test_data)\n",
    "    eval_ppl = math.exp(eval_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "         f'valid loss {eval_loss:5.2f} | valid ppl {eval_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "    #best_model = copy.deepcopy(model)\n",
    "    if eval_loss < best_val_loss:\n",
    "       best_val_loss = eval_loss\n",
    "       best_model = copy.deepcopy(model)\n",
    "\n",
    "#     scheduler.step()\n",
    "#save model\n",
    "#torch.save(best_model.state_dict(),'best_model_3bigx3.pt')\n",
    "torch.save(best_model.state_dict(),'best_model_3bigx3_corrected.pt')\n",
    "torch.save(model.state_dict(),'training_model_3bigx3_corrected.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1fd27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(),'best_model_3bigx3_corrected.pt')\n",
    "torch.save(model.state_dict(),'training_model_3bigx3_corrected.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c57d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsoftmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b4922d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_3bigx3_corrected.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa4c44",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c2453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "    model.eval()\n",
    "    temp_text = text\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        print('i:', i)\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        else:\n",
    "            src_mask_ = src_mask[:,:]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "        #print(indices[0],indices[1])\n",
    "        #for j in range(batch_size):\n",
    "        print('next word: ', [vocab.lookup_tokens(list(index))\n",
    "                                  for index in indices][0][-1])\n",
    "        print(i,\"Gen_data: \",gen_data,\"Pred_data: \",indices)\n",
    "        pred_text.append([vocab.lookup_tokens(list(index))\n",
    "                                  for index in indices][0][-1])\n",
    "        if(batch_size < 16):\n",
    "            gen_data = torch.cat((gen_data[:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],indices.t()[-1:][:]),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c93c46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ['म भारत भ्रमण गर्न']\n",
    "st_i = data_process(st)\n",
    "\n",
    "st_i = st_i.unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ba7301a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70f3c92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 83],\n",
       "        [301],\n",
       "        [700],\n",
       "        [ 10]], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3e1b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsoftmax = nn.LogSoftmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80565776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "bptt = 35\n",
    "def probability(model: nn.Module, sent: Tensor):\n",
    "    model.eval()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    prob = 0\n",
    "    for i in range(sent.shape[0]-1):\n",
    "        print('i:', i)\n",
    "        batch_size = i+1\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        else:\n",
    "            src_mask_ = src_mask[:,:]\n",
    "        output_softmax = model(sent[:i+1,:], src_mask_)\n",
    "        output_softmax_permuted = lnsoftmax(output_softmax.permute(1, 0, 2))\n",
    "        \n",
    "        print(output_softmax_permuted,output_softmax_permuted[0,i,sent[i+1,0]],output_softmax_permuted.max())\n",
    "        #Index for maximum probability word\n",
    "        indices = torch.argmax(output_softmax_permuted, dim=2)\n",
    "        \n",
    "        #Max probability word\n",
    "        print('next word: ', [vocab.lookup_tokens(list(index))\n",
    "                                  for index in indices][0][-1])\n",
    "        \n",
    "        prob+= output_softmax_permuted[0,i,sent[i+1,0]]\n",
    "    return prob\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7201269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "tensor([[[ -5.0913,  -6.2857,  -6.0924,  ..., -18.0961, -16.7388, -16.7699]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>) tensor(-8.7135, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-3.5160, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "next word:  नै\n",
      "i: 1\n",
      "tensor([[[ -5.0913,  -6.2857,  -6.0924,  ..., -18.0961, -16.7388, -16.7699],\n",
      "         [ -6.0678,  -5.9991,  -4.4271,  ..., -20.0123, -16.5251, -18.4687]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>) tensor(-5.6694, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-2.3711, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "next word:  जान्छु\n",
      "i: 2\n",
      "tensor([[[ -5.0913,  -6.2857,  -6.0924,  ..., -18.0961, -16.7388, -16.7699],\n",
      "         [ -6.0678,  -5.9991,  -4.4271,  ..., -20.0123, -16.5251, -18.4687],\n",
      "         [ -6.4965,  -6.5571,  -4.5201,  ..., -19.2898, -16.4366, -16.8484]]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>) tensor(-1.0052, device='cuda:0', grad_fn=<SelectBackward0>) tensor(-1.0052, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "next word:  गर्न\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-15.3881, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability(best_model,st_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08777245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# def nonnaive_generator(model: nn.Module, gen_data: Tensor, no_words = 10):\n",
    "#     model.eval()\n",
    "#     temp_text = text\n",
    "#     src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "#     pred_text = []\n",
    "#     for i in range(no_words):\n",
    "#         print('i:', i)\n",
    "#         batch_size = gen_data.size(0)\n",
    "#         if batch_size != bptt:\n",
    "#             src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "#         output_softmax = model(gen_data, src_mask_)\n",
    "#         output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "#         #print(softmax(output_softmax_permuted))\n",
    "#         indices = torch.topk(output_softmax_permuted,10 ,dim=2,sorted=True).indices.squeeze(0)\n",
    "#         values = torch.topk(softmax(output_softmax_permuted),10 ,dim=2,sorted = True).values.squeeze(0)\n",
    "#         values = values/torch.sum(values,dim = 1,keepdims = True)\n",
    "#         values = torch.flip(values,dims = (1,))\n",
    "#         #print(output_softmax_permuted[indices])\n",
    "#         print(indices,values)\n",
    "#         ind_sampled = torch.distributions.Categorical(values).sample()\n",
    "# #         index = indices.squeeze(0)[ind_sampled.unsqueeze(0)]\n",
    "#         print('is',ind_sampled)\n",
    "#         next_index = indices[-1][ind_sampled[-1]]\n",
    "#         print(indices[-1][ind_sampled[-1]])\n",
    "        \n",
    "# #     return indices\n",
    "        \n",
    "#         #print(indices[0],indices[1])\n",
    "#         #for j in range(batch_size):\n",
    "        \n",
    "#         print('next word: ', [vocab.lookup_token(next_index)],'values: ',values.squeeze(0)[-1])\n",
    "                                  \n",
    "# #         print(i,\"Gen_data: \",gen_data,\"Pred_data: \",indices)\n",
    "\n",
    "\n",
    "#         pred_text.append([vocab.lookup_token((next_index))][0])\n",
    "#         if(batch_size <= 10):\n",
    "#             gen_data = torch.cat((gen_data[:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "#             batch_size= gen_data.size(0)\n",
    "#         else:\n",
    "#             gen_data = torch.cat((gen_data[1:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "#             batch_size= gen_data.size(0)\n",
    "            \n",
    "#     return pred_text\n",
    "\n",
    "\n",
    "\n",
    "def nonnaive_generator(model: nn.Module, gen_data: Tensor, no_words = 5,k=50):\n",
    "    model.eval()\n",
    "#     temp_text = text\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    pred_text = []\n",
    "    for i in range(no_words):\n",
    "        print('i:', i)\n",
    "        batch_size = gen_data.size(0)\n",
    "        if batch_size != bptt:\n",
    "            src_mask_ = src_mask[:batch_size, :batch_size]\n",
    "        output_softmax = model(gen_data, src_mask_)\n",
    "        output_softmax_permuted = output_softmax.permute(1, 0, 2)\n",
    "        indices = torch.topk(output_softmax_permuted,k ,dim=2).indices.squeeze(0)\n",
    "        \n",
    "        values = torch.topk(softmax(output_softmax_permuted),k ,dim=2).values\n",
    "        values = values/torch.sum(values,dim = 2,keepdims = True)\n",
    "#         values = softmax(values)\n",
    "        \n",
    "#         values = torch.flip(values,dims = (2,))\n",
    "\n",
    "        \n",
    "        ind_sampled = torch.distributions.Categorical(values.squeeze(0)).sample()\n",
    "        next_index = indices[-1][ind_sampled[-1]]\n",
    "        \n",
    "        print('next word: ', vocab.lookup_token(next_index))\n",
    "\n",
    "        print(i,\"Values: \",values.squeeze(0)[-1],\"Gen_data: \",gen_data,\"possible tokens: \",indices[-1],\"Pred_data: \",next_index)\n",
    "        pred_text.append([vocab.lookup_token(next_index)][0])\n",
    "        if(batch_size < 15):\n",
    "            gen_data = torch.cat((gen_data[:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "        else:\n",
    "            gen_data = torch.cat((gen_data[1:,:],next_index.unsqueeze(0).unsqueeze(0)),0)\n",
    "            batch_size= gen_data.size(0)\n",
    "            \n",
    "    return pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cebb7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved model\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_3bigx3_corrected.pt'))\n",
    "model.to(device)\n",
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bf265b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357]], device='cuda:0')\n",
      "tensor([[ 2086,   357],\n",
      "        [ 5694,   465],\n",
      "        [  568,   410],\n",
      "        [    0,  6548],\n",
      "        [  897,   293],\n",
      "        [28361,     0],\n",
      "        [    0,   357]], device='cuda:0')\n",
      "i: 0\n",
      "next word:  राज्य\n",
      "0 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465]], device='cuda:0')\n",
      "i: 1\n",
      "next word:  अमेरिका\n",
      "1 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665]],\n",
      "       device='cuda:0')\n",
      "i: 2\n",
      "next word:  ?\n",
      "2 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2]],\n",
      "       device='cuda:0')\n",
      "i: 3\n",
      "next word:  चीन\n",
      "3 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2,   666]],\n",
      "       device='cuda:0')\n",
      "i: 4\n",
      "next word:  ?\n",
      "4 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2,   666,\n",
      "             2]], device='cuda:0')\n",
      "i: 5\n",
      "next word:  जापान\n",
      "5 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666],\n",
      "        [   2]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2,   666,\n",
      "             2,  1803]], device='cuda:0')\n",
      "i: 6\n",
      "next word:  ?\n",
      "6 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666],\n",
      "        [   2],\n",
      "        [1803]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2,   666,\n",
      "             2,  1803,     2]], device='cuda:0')\n",
      "i: 7\n",
      "next word:  जापान\n",
      "7 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2,   666,\n",
      "             2,  1803,     2,  1803]], device='cuda:0')\n",
      "i: 8\n",
      "next word:  ?\n",
      "8 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2],\n",
      "        [1803]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2,   666,\n",
      "             2,  1803,     2,  1803,     2]], device='cuda:0')\n",
      "i: 9\n",
      "next word:  जापान\n",
      "9 Gen_data:  tensor([[ 357],\n",
      "        [ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2]], device='cuda:0') Pred_data:  tensor([[   53,   665,   387, 10546,     3,     0,   465,   665,     2,   666,\n",
      "             2,  1803,     2,  1803,     2,  1803]], device='cuda:0')\n",
      "i: 10\n",
      "next word:  ?\n",
      "10 Gen_data:  tensor([[ 465],\n",
      "        [ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2],\n",
      "        [1803]], device='cuda:0') Pred_data:  tensor([[    3,   465, 10546,     3,     0,   465,   665,     2,   666,     2,\n",
      "          1803,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 11\n",
      "next word:  भियतनाम\n",
      "11 Gen_data:  tensor([[ 410],\n",
      "        [6548],\n",
      "        [ 293],\n",
      "        [   0],\n",
      "        [ 357],\n",
      "        [ 465],\n",
      "        [ 665],\n",
      "        [   2],\n",
      "        [ 666],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2],\n",
      "        [1803],\n",
      "        [   2]], device='cuda:0') Pred_data:  tensor([[  387, 10546,     3,     0,   465,   665,     2,   666,     2,  1803,\n",
      "             2,  1803,     2, 14639,     2, 14639]], device='cuda:0')\n",
      "i: 12\n",
      "next word:  ?\n",
      "12 Gen_data:  tensor([[ 6548],\n",
      "        [  293],\n",
      "        [    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    2],\n",
      "        [  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[10546,     3,     0,   465,   665,     2,   666,     2,  1803,     2,\n",
      "          1803,     2, 14639,     2, 14639,     2]], device='cuda:0')\n",
      "i: 13\n",
      "next word:  जापान\n",
      "13 Gen_data:  tensor([[  293],\n",
      "        [    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    2],\n",
      "        [  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[ 2739,     1,    53,   665,     2,  2732,     2,  1803,     2,  1803,\n",
      "             2, 14639,     2, 14639,     2,  1803]], device='cuda:0')\n",
      "i: 14\n",
      "next word:  ?\n",
      "14 Gen_data:  tensor([[    0],\n",
      "        [  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    2],\n",
      "        [  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    1,    53,   665,     2,  2732,     2,  1803,     2,  1803,     2,\n",
      "          1803,     2, 14639,     2,  1803,     2]], device='cuda:0')\n",
      "i: 15\n",
      "next word:  भियतनाम\n",
      "15 Gen_data:  tensor([[  357],\n",
      "        [  465],\n",
      "        [  665],\n",
      "        [    2],\n",
      "        [  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[   53,   665,     2,  2732,     2,  1803,     2,  1803,     2,  1803,\n",
      "             2,  1803,     2,  1803,     2, 14639]], device='cuda:0')\n",
      "i: 16\n",
      "next word:  ?\n",
      "16 Gen_data:  tensor([[  465],\n",
      "        [  665],\n",
      "        [    2],\n",
      "        [  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[    3,     2,  2732,     2,  1803,     2,  1803,     2, 14639,     2,\n",
      "         14639,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 17\n",
      "next word:  जापान\n",
      "17 Gen_data:  tensor([[  665],\n",
      "        [    2],\n",
      "        [  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,  2732,     2,  1803,     2,  1803,     2,  1803,     2, 14639,\n",
      "             2,  1803,     2, 14639,     2,  1803]], device='cuda:0')\n",
      "i: 18\n",
      "next word:  ?\n",
      "18 Gen_data:  tensor([[    2],\n",
      "        [  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,  1803,     2,  1803,     2, 14639,     2, 14639,     2,\n",
      "          1803,     2, 14639,     2,  1803,     2]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 19\n",
      "next word:  भियतनाम\n",
      "19 Gen_data:  tensor([[  666],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,  1803,     2,  1803,     2,  1803,     2, 14639,     2,  1803,\n",
      "             2,  1803,     2,  1803,     2, 14639]], device='cuda:0')\n",
      "i: 20\n",
      "next word:  ?\n",
      "20 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2, 14639,     2, 14639,     2,  1803,     2,\n",
      "         14639,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 21\n",
      "next word:  जापान\n",
      "21 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2, 14639,     2, 14639,     2,  1803,     2, 14639,\n",
      "             2,  1803,     2, 14639,     2,  1803]], device='cuda:0')\n",
      "i: 22\n",
      "next word:  ?\n",
      "22 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2, 14639,     2,  1803,     2, 14639,     2,\n",
      "          1803,     2, 14639,     2,  1803,     2]], device='cuda:0')\n",
      "i: 23\n",
      "next word:  जापान\n",
      "23 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2, 14639,     2,  1803,     2, 14639,     2,  1803,\n",
      "             2,  1803,     2,  1803,     2,  1803]], device='cuda:0')\n",
      "i: 24\n",
      "next word:  ?\n",
      "24 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2,  1803,     2, 14639,     2,  1803,     2,\n",
      "          1803,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 25\n",
      "next word:  जापान\n",
      "25 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[   2,  665,    2, 1803,    2, 1803,    2, 1803,    2, 1803,    2, 1803,\n",
      "            2, 1803,    2, 1803]], device='cuda:0')\n",
      "i: 26\n",
      "next word:  ?\n",
      "26 Gen_data:  tensor([[    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,   666,     2, 14639,     2,  1803,     2, 14639,     2,\n",
      "          1803,     2, 14639,     2, 14639,     2]], device='cuda:0')\n",
      "i: 27\n",
      "next word:  भियतनाम\n",
      "27 Gen_data:  tensor([[14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   666,     2, 14639,     2,  1803,     2, 14639,     2,  1803,\n",
      "             2,  1803,     2, 14639,     2, 14639]], device='cuda:0')\n",
      "i: 28\n",
      "next word:  ?\n",
      "28 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2,  1803,     2, 14639,     2,  1803,     2,\n",
      "          1803,     2, 14639,     2, 14639,     2]], device='cuda:0')\n",
      "i: 29\n",
      "next word:  जापान\n",
      "29 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2,  1803,     2,  1803,     2,  1803,     2,  1803,\n",
      "             2,  1803,     2, 14639,     2,  1803]], device='cuda:0')\n",
      "i: 30\n",
      "next word:  ?\n",
      "30 Gen_data:  tensor([[    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,   666,     2, 14639,     2,  1803,     2, 14639,     2,\n",
      "         14639,     2, 14639,     2,  1803,     2]], device='cuda:0')\n",
      "i: 31\n",
      "next word:  भियतनाम\n",
      "31 Gen_data:  tensor([[14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   666,     2, 14639,     2,  1803,     2, 14639,     2, 14639,\n",
      "             2, 14639,     2,  1803,     2, 14639]], device='cuda:0')\n",
      "i: 32\n",
      "next word:  ?\n",
      "32 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2,  1803,     2, 14639,     2, 14639,     2,\n",
      "         14639,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 33\n",
      "next word:  जापान\n",
      "33 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2,  1803,     2,  1803,     2, 14639,     2, 14639,\n",
      "             2,  1803,     2, 14639,     2,  1803]], device='cuda:0')\n",
      "i: 34\n",
      "next word:  ?\n",
      "34 Gen_data:  tensor([[    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,   666,     2, 14639,     2, 14639,     2, 14639,     2,\n",
      "          1803,     2, 14639,     2,  1803,     2]], device='cuda:0')\n",
      "i: 35\n",
      "next word:  भियतनाम\n",
      "35 Gen_data:  tensor([[14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   666,     2, 14639,     2, 14639,     2, 14639,     2,  1803,\n",
      "             2, 14639,     2,  1803,     2, 14639]], device='cuda:0')\n",
      "i: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  ?\n",
      "36 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2, 14639,     2, 14639,     2,  1803,     2,\n",
      "         14639,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 37\n",
      "next word:  जापान\n",
      "37 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2, 14639,     2, 14639,     2,  1803,     2, 14639,\n",
      "             2,  1803,     2, 14639,     2,  1803]], device='cuda:0')\n",
      "i: 38\n",
      "next word:  ?\n",
      "38 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2, 14639,     2,  1803,     2, 14639,     2,\n",
      "          1803,     2, 14639,     2,  1803,     2]], device='cuda:0')\n",
      "i: 39\n",
      "next word:  जापान\n",
      "39 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2, 14639,     2,  1803,     2, 14639,     2,  1803,\n",
      "             2,  1803,     2,  1803,     2,  1803]], device='cuda:0')\n",
      "i: 40\n",
      "next word:  ?\n",
      "40 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2,  1803,     2, 14639,     2,  1803,     2,\n",
      "          1803,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 41\n",
      "next word:  जापान\n",
      "41 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[   2,  665,    2, 1803,    2, 1803,    2, 1803,    2, 1803,    2, 1803,\n",
      "            2, 1803,    2, 1803]], device='cuda:0')\n",
      "i: 42\n",
      "next word:  ?\n",
      "42 Gen_data:  tensor([[    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,   666,     2, 14639,     2,  1803,     2, 14639,     2,\n",
      "          1803,     2, 14639,     2, 14639,     2]], device='cuda:0')\n",
      "i: 43\n",
      "next word:  भियतनाम\n",
      "43 Gen_data:  tensor([[14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   666,     2, 14639,     2,  1803,     2, 14639,     2,  1803,\n",
      "             2,  1803,     2, 14639,     2, 14639]], device='cuda:0')\n",
      "i: 44\n",
      "next word:  ?\n",
      "44 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2,  1803,     2, 14639,     2,  1803,     2,\n",
      "          1803,     2, 14639,     2, 14639,     2]], device='cuda:0')\n",
      "i: 45\n",
      "next word:  जापान\n",
      "45 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2,  1803,     2,  1803,     2,  1803,     2,  1803,\n",
      "             2,  1803,     2, 14639,     2,  1803]], device='cuda:0')\n",
      "i: 46\n",
      "next word:  ?\n",
      "46 Gen_data:  tensor([[    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803]], device='cuda:0') Pred_data:  tensor([[    0,     2,   666,     2, 14639,     2,  1803,     2, 14639,     2,\n",
      "         14639,     2, 14639,     2,  1803,     2]], device='cuda:0')\n",
      "i: 47\n",
      "next word:  भियतनाम\n",
      "47 Gen_data:  tensor([[14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   666,     2, 14639,     2,  1803,     2, 14639,     2, 14639,\n",
      "             2, 14639,     2,  1803,     2, 14639]], device='cuda:0')\n",
      "i: 48\n",
      "next word:  ?\n",
      "48 Gen_data:  tensor([[    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639]], device='cuda:0') Pred_data:  tensor([[    0,     2,  2198,     2,  1803,     2, 14639,     2, 14639,     2,\n",
      "         14639,     2,  1803,     2, 14639,     2]], device='cuda:0')\n",
      "i: 49\n",
      "next word:  जापान\n",
      "49 Gen_data:  tensor([[ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2],\n",
      "        [ 1803],\n",
      "        [    2],\n",
      "        [14639],\n",
      "        [    2]], device='cuda:0') Pred_data:  tensor([[    2,   665,     2,  1803,     2,  1803,     2, 14639,     2, 14639,\n",
      "             2,  1803,     2, 14639,     2,  1803]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sample_data[:,-1].unsqueeze(1))\n",
    "print(sample_data)\n",
    "z = generator(best_model, sample_data[:,-1].unsqueeze(1),no_words = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ddefdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त राज्य अमेरिका ? चीन ? जापान ? जापान ? जापान ? भियतनाम ? जापान ? भियतनाम ? जापान ? भियतनाम ? जापान ? जापान ? जापान ? भियतनाम ? जापान ? भियतनाम ? जापान ? भियतनाम ? जापान ? जापान ? जापान ? भियतनाम ? जापान ? भियतनाम ? जापान'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त ' +' '.join(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d23f62b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.372248649597168"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(best_model, sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "343b249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nआधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , \\nसंयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त\\n\\nविकास गर्न प्रोत्साहित गरी सहकारी क्षेत्रले आर्थिक दृष्टिले सक्रिय\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "आधिकारिक निर्णयको कारणले , वाणिज्य बिभागले , \n",
    "संयुक्त राज्य अमेरिकी समुद्री पानी निर्माताद्वारा संयुक्त\n",
    "\n",
    "विकास गर्न प्रोत्साहित गरी सहकारी क्षेत्रले आर्थिक दृष्टिले सक्रिय\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef802714",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ['गर्ने']\n",
    "st_i = data_process(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28fe199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_i = st_i.unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0be960e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m z_ \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m(best_model, st_i,no_words \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "z_ = generator(best_model, st_i,no_words =50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772a4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ['म भारत भ्रमण गर्न']\n",
    "st_i = data_process(st)\n",
    "st_i = st_i.unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52e25828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 83],\n",
       "        [301],\n",
       "        [700],\n",
       "        [ 10]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "st_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9219caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "next word:  चाहन्छु\n",
      "0 Gen_data:  tensor([[ 83],\n",
      "        [301],\n",
      "        [700],\n",
      "        [ 10]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067]], device='cuda:0')\n",
      "i: 1\n",
      "next word:  ।\n",
      "1 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1]], device='cuda:0')\n",
      "i: 2\n",
      "next word:  तर\n",
      "2 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19]], device='cuda:0')\n",
      "i: 3\n",
      "next word:  ?\n",
      "3 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2]], device='cuda:0')\n",
      "i: 4\n",
      "next word:  नेपालको\n",
      "4 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110]], device='cuda:0')\n",
      "i: 5\n",
      "next word:  पहिलो\n",
      "5 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87]],\n",
      "       device='cuda:0')\n",
      "i: 6\n",
      "next word:  ठूलो\n",
      "6 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87,  125]],\n",
      "       device='cuda:0')\n",
      "i: 7\n",
      "next word:  चुनौती\n",
      "7 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87,  125, 1069]],\n",
      "       device='cuda:0')\n",
      "i: 8\n",
      "next word:  भनेको\n",
      "8 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87,  125, 1069,  246]],\n",
      "       device='cuda:0')\n",
      "i: 9\n",
      "next word:  नेपालको\n",
      "9 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87,  125, 1069,  246,\n",
      "          110]], device='cuda:0')\n",
      "i: 10\n",
      "next word:  आर्थिक\n",
      "10 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87,  125, 1069,  246,\n",
      "          110,   81]], device='cuda:0')\n",
      "i: 11\n",
      "next word:  विकासमा\n",
      "11 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87,  125, 1069,  246,\n",
      "          110,   81, 1226]], device='cuda:0')\n",
      "i: 12\n",
      "next word:  ठूलो\n",
      "12 Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226]], device='cuda:0') Pred_data:  tensor([[  17, 7752,   10, 2067,    1,   19,    2,  110,   87,  125, 1069,  246,\n",
      "          110,   81, 1226,  125]], device='cuda:0')\n",
      "i: 13\n",
      "next word:  टेवा\n",
      "13 Gen_data:  tensor([[ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125]], device='cuda:0') Pred_data:  tensor([[   3,   23,   22,    1,   19,    2,   15,   87,  840, 1069,  246,  110,\n",
      "           87, 1226,  125, 3799]], device='cuda:0')\n",
      "i: 14\n",
      "next word:  पुगेको\n",
      "14 Gen_data:  tensor([[ 700],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799]], device='cuda:0') Pred_data:  tensor([[  13,   22,    1,   19,    2,   15,   81,  840, 1069,  246,  110,   81,\n",
      "         1226,  125, 3799,  269]], device='cuda:0')\n",
      "i: 15\n",
      "next word:  छ\n",
      "15 Gen_data:  tensor([[  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269]], device='cuda:0') Pred_data:  tensor([[ 180,    1,   19,    2,   15,   81,  125, 1069,  246,  110,   81, 1226,\n",
      "          125,  901,  269,    4]], device='cuda:0')\n",
      "i: 16\n",
      "next word:  ।\n",
      "16 Gen_data:  tensor([[2067],\n",
      "        [   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4]], device='cuda:0') Pred_data:  tensor([[   1,   19,    2,   15,   81,  234, 1069,  246,  110,   81, 1226,  125,\n",
      "          901,  269,    4,    1]], device='cuda:0')\n",
      "i: 17\n",
      "next word:  नेपाल\n",
      "17 Gen_data:  tensor([[   1],\n",
      "        [  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1]], device='cuda:0') Pred_data:  tensor([[  19,    2,   15,   81,  125, 1069,  246,  110,   81, 1226,  125, 4746,\n",
      "          269,    4,    1,   22]], device='cuda:0')\n",
      "i: 18\n",
      "next word:  राष्ट्र\n",
      "18 Gen_data:  tensor([[  19],\n",
      "        [   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22]], device='cuda:0') Pred_data:  tensor([[   2,   15,   81,  125, 1069,  246,  110,   81, 1226,  125, 4746,  269,\n",
      "            4,    1,   22,  337]], device='cuda:0')\n",
      "i: 19\n",
      "next word:  बैंकले\n",
      "19 Gen_data:  tensor([[   2],\n",
      "        [ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337]], device='cuda:0') Pred_data:  tensor([[   0,   81,  938, 1069,  246,  110,   81, 1226,  125, 4746,  269,    4,\n",
      "            1,   22,  295,  513]], device='cuda:0')\n",
      "i: 20\n",
      "next word:  मौद्रिक\n",
      "20 Gen_data:  tensor([[ 110],\n",
      "        [  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513]], device='cuda:0') Pred_data:  tensor([[  81,  938, 2803, 1090,  110,   81, 1226,  125, 4746,  269,    4,    1,\n",
      "           22,  295,  513, 2635]], device='cuda:0')\n",
      "i: 21\n",
      "next word:  नीतिमा\n",
      "21 Gen_data:  tensor([[  87],\n",
      "        [ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635]], device='cuda:0') Pred_data:  tensor([[ 209,  682,  267,  110,   81, 1226,  125, 4746,  269,    4,    1,   22,\n",
      "          337,  513, 2635, 3851]], device='cuda:0')\n",
      "i: 22\n",
      "next word:  लघुवित्त\n",
      "22 Gen_data:  tensor([[ 125],\n",
      "        [1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851]], device='cuda:0') Pred_data:  tensor([[1069,   21,  123,   81, 1226,  125, 4746,  269,    4,    1,   22,  295,\n",
      "          513, 2635, 3851, 2816]], device='cuda:0')\n",
      "i: 23\n",
      "next word:  वित्तीय\n",
      "23 Gen_data:  tensor([[1069],\n",
      "        [ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816]], device='cuda:0') Pred_data:  tensor([[  21,  123,   81, 1226,  125, 4746,   17,    4,    1,   22,  295,  513,\n",
      "         2635, 3851, 2816,  589]], device='cuda:0')\n",
      "i: 24\n",
      "next word:  संस्था\n",
      "24 Gen_data:  tensor([[ 246],\n",
      "        [ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589]], device='cuda:0') Pred_data:  tensor([[   4,   81, 1226,  125, 4746,   17,    4,    1,   22,  295,  513,  334,\n",
      "         3851, 2816,  589,  478]], device='cuda:0')\n",
      "i: 25\n",
      "next word:  लिमिटेडको\n",
      "25 Gen_data:  tensor([[ 110],\n",
      "        [  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478]], device='cuda:0') Pred_data:  tensor([[  81,    2,  901,  901,  269,    4,    1,   22,  295,  513,  334, 3851,\n",
      "         2816,  589,  478, 4752]], device='cuda:0')\n",
      "i: 26\n",
      "next word:  खुद\n",
      "26 Gen_data:  tensor([[  81],\n",
      "        [1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752]], device='cuda:0') Pred_data:  tensor([[ 312,  587,  901,  269,    4,    1,   22,  337,  513,  334, 3851, 2816,\n",
      "          589,  478, 4752, 4649]], device='cuda:0')\n",
      "i: 27\n",
      "next word:  मुनाफा\n",
      "27 Gen_data:  tensor([[1226],\n",
      "        [ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649]], device='cuda:0') Pred_data:  tensor([[ 587,  901,  269,    4,    1,   22,  337,  513, 2635, 3851, 2816,  589,\n",
      "          478, 4752, 4649, 4886]], device='cuda:0')\n",
      "i: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  रु\n",
      "28 Gen_data:  tensor([[ 125],\n",
      "        [3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886]], device='cuda:0') Pred_data:  tensor([[1069,  269,    4,    1,   22,  337,  513,  334, 3851, 2816,  589,  478,\n",
      "         4752, 4649, 4886,  271]], device='cuda:0')\n",
      "i: 29\n",
      "next word:  एक\n",
      "29 Gen_data:  tensor([[3799],\n",
      "        [ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271]], device='cuda:0') Pred_data:  tensor([[1928,    4,    1,  934,  337,  513,  334, 3851, 2816,  589,  478, 4752,\n",
      "         4649, 4886,  271,   18]], device='cuda:0')\n",
      "i: 30\n",
      "next word:  करोड\n",
      "30 Gen_data:  tensor([[ 269],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18]], device='cuda:0') Pred_data:  tensor([[   4,    1,  934,  337,  513, 2635, 3851, 2816,  589, 3327,    3, 4649,\n",
      "         4886,  271,   18,  126]], device='cuda:0')\n",
      "i: 31\n",
      "next word:  ५०\n",
      "31 Gen_data:  tensor([[   4],\n",
      "        [   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126]], device='cuda:0') Pred_data:  tensor([[   1,   22,  337,  513, 2635, 3851, 2816,  589, 3327,    3, 4649, 4886,\n",
      "          271,   18,  126,  264]], device='cuda:0')\n",
      "i: 32\n",
      "next word:  लाख\n",
      "32 Gen_data:  tensor([[   1],\n",
      "        [  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264]], device='cuda:0') Pred_data:  tensor([[  19,  337,  513, 2635, 3851, 2816,  589, 3327,    3, 4649, 4886,  271,\n",
      "           18,  126,  264,   69]], device='cuda:0')\n",
      "i: 33\n",
      "next word:  रहेको\n",
      "33 Gen_data:  tensor([[  22],\n",
      "        [ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69]], device='cuda:0') Pred_data:  tensor([[ 337,  513, 1898, 3851,  210,  589, 3327,    3, 4649, 4886,  271,   18,\n",
      "          126,  264,   69,   21]], device='cuda:0')\n",
      "i: 34\n",
      "next word:  छ\n",
      "34 Gen_data:  tensor([[ 337],\n",
      "        [ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21]], device='cuda:0') Pred_data:  tensor([[ 513, 1898, 3851,  757,  589, 1200,    3, 4649, 4886,  271,   18,  126,\n",
      "          264,   69,  264,    4]], device='cuda:0')\n",
      "i: 35\n",
      "next word:  ।\n",
      "35 Gen_data:  tensor([[ 513],\n",
      "        [2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4]], device='cuda:0') Pred_data:  tensor([[1898, 3851,  210,  589, 1200,    3, 4649, 4886,  271,   18,  126,  264,\n",
      "           69,  264,    4,    1]], device='cuda:0')\n",
      "i: 36\n",
      "next word:  बैंकले\n",
      "36 Gen_data:  tensor([[2635],\n",
      "        [3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1]], device='cuda:0') Pred_data:  tensor([[5745,  210,  589, 1200,    2, 4649, 4886,  271,   18,  126,  264,   69,\n",
      "          264,    4,    1,  513]], device='cuda:0')\n",
      "i: 37\n",
      "next word:  रु\n",
      "37 Gen_data:  tensor([[3851],\n",
      "        [2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513]], device='cuda:0') Pred_data:  tensor([[ 210,  589, 1067,    2, 4649, 4886,  271,   18,  126,  264,   69,  264,\n",
      "            4,    1,  513,  271]], device='cuda:0')\n",
      "i: 38\n",
      "next word:  एक\n",
      "38 Gen_data:  tensor([[2816],\n",
      "        [ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271]], device='cuda:0') Pred_data:  tensor([[ 589, 1067, 2386, 4649, 4886,  271,   18,  126,  264,   69,  264,    4,\n",
      "            1,  513,  271,   18]], device='cuda:0')\n",
      "i: 39\n",
      "next word:  करोड\n",
      "39 Gen_data:  tensor([[ 589],\n",
      "        [ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18]], device='cuda:0') Pred_data:  tensor([[1200,    3, 4649, 4886,  271,   18,  126,  264,   69,   21,    4,    1,\n",
      "          513,  271,   18,  126]], device='cuda:0')\n",
      "i: 40\n",
      "next word:  ५०\n",
      "40 Gen_data:  tensor([[ 478],\n",
      "        [4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126]], device='cuda:0') Pred_data:  tensor([[   2, 4649, 4886,  271,   18,  126,  264,   69,  264,    4,    1,  513,\n",
      "          271,   18,  126,  264]], device='cuda:0')\n",
      "i: 41\n",
      "next word:  लाख\n",
      "41 Gen_data:  tensor([[4752],\n",
      "        [4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264]], device='cuda:0') Pred_data:  tensor([[ 271, 4886,  271,   18,  126,  264,   69,  264,    4,    1,  513,  271,\n",
      "           18,  126,  264,   69]], device='cuda:0')\n",
      "i: 42\n",
      "next word:  ५०\n",
      "42 Gen_data:  tensor([[4649],\n",
      "        [4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69]], device='cuda:0') Pred_data:  tensor([[4886,  271,  192,  126,  264,   69,  264,    4,    1,  513,  271,   18,\n",
      "          270,  264,   69,  264]], device='cuda:0')\n",
      "i: 43\n",
      "next word:  हजार\n",
      "43 Gen_data:  tensor([[4886],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [ 264]], device='cuda:0') Pred_data:  tensor([[2379,  264,  126,  264,   69,  264,    4,    1,  513,  271,   18,  270,\n",
      "          264,   69,  264,   44]], device='cuda:0')\n",
      "i: 44\n",
      "next word:  बराबरको\n",
      "44 Gen_data:  tensor([[271],\n",
      "        [ 18],\n",
      "        [126],\n",
      "        [264],\n",
      "        [ 69],\n",
      "        [ 21],\n",
      "        [  4],\n",
      "        [  1],\n",
      "        [513],\n",
      "        [271],\n",
      "        [ 18],\n",
      "        [126],\n",
      "        [264],\n",
      "        [ 69],\n",
      "        [264],\n",
      "        [ 44]], device='cuda:0') Pred_data:  tensor([[ 264,  126,  264,   69,  264,    4,    1,  513,  271,   18,  270,  264,\n",
      "           69,  264,   44, 1227]], device='cuda:0')\n",
      "i: 45\n",
      "next word:  शेयर\n",
      "45 Gen_data:  tensor([[  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [ 264],\n",
      "        [  44],\n",
      "        [1227]], device='cuda:0') Pred_data:  tensor([[  44,  264,   69,  156,    4,    1,   89,  271,   18,  270,  264,   69,\n",
      "          264,   44, 1227,  678]], device='cuda:0')\n",
      "i: 46\n",
      "next word:  निष्काशन\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Gen_data:  tensor([[ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [ 264],\n",
      "        [  44],\n",
      "        [1227],\n",
      "        [ 678]], device='cuda:0') Pred_data:  tensor([[ 264,   69,  156,    4,    1,   89,  271,   18,  270,  264,   69,  264,\n",
      "           44, 1227,  678, 5228]], device='cuda:0')\n",
      "i: 47\n",
      "next word:  गरेको\n",
      "47 Gen_data:  tensor([[ 264],\n",
      "        [  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [ 264],\n",
      "        [  44],\n",
      "        [1227],\n",
      "        [ 678],\n",
      "        [5228]], device='cuda:0') Pred_data:  tensor([[  44,  156,    4,    1,   89,  271,   18,  270,  264,   69,  264,   44,\n",
      "         1227,  678, 5228,   11]], device='cuda:0')\n",
      "i: 48\n",
      "next word:  छ\n",
      "48 Gen_data:  tensor([[  69],\n",
      "        [  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [ 264],\n",
      "        [  44],\n",
      "        [1227],\n",
      "        [ 678],\n",
      "        [5228],\n",
      "        [  11]], device='cuda:0') Pred_data:  tensor([[ 264,    4,    1,   89,  271,   18,  270,  264,   69,  264,   44, 1227,\n",
      "          678, 5228,   11,    4]], device='cuda:0')\n",
      "i: 49\n",
      "next word:  ।\n",
      "49 Gen_data:  tensor([[  21],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [ 513],\n",
      "        [ 271],\n",
      "        [  18],\n",
      "        [ 126],\n",
      "        [ 264],\n",
      "        [  69],\n",
      "        [ 264],\n",
      "        [  44],\n",
      "        [1227],\n",
      "        [ 678],\n",
      "        [5228],\n",
      "        [  11],\n",
      "        [   4]], device='cuda:0') Pred_data:  tensor([[   4,    1,  934,  334,   18,  270,  264,   69,  264,   44, 1227,  678,\n",
      "         5228,   11,    4,    1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "z_ = generator(best_model, st_i,no_words = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "3fe998f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'म भारत भ्रमण गर्न चाहन्छु । तर ? नेपालको पहिलो ठूलो चुनौती भनेको नेपालको आर्थिक विकासमा ठूलो टेवा पुगेको छ । नेपाल राष्ट्र बैंकले मौद्रिक नीतिमा लघुवित्त वित्तीय संस्था लिमिटेडको खुद मुनाफा रु एक करोड ५० लाख रहेको छ । बैंकले रु एक करोड ५० लाख ५० हजार बराबरको शेयर निष्काशन गरेको छ ।'"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(st)+' ' +' '.join(z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93be8449",
   "metadata": {},
   "outputs": [],
   "source": [
    " x = torch.arange(1., 6.)\n",
    "j = torch.topk(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5652543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 4., 3.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce24cae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39fbeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4149da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "next word:  जान्छु\n",
      "0 Values:  tensor([0.3439, 0.0863, 0.0536, 0.0429, 0.0387, 0.0298, 0.0277, 0.0277, 0.0180,\n",
      "        0.0177, 0.0165, 0.0141, 0.0130, 0.0129, 0.0127, 0.0116, 0.0109, 0.0105,\n",
      "        0.0101, 0.0097, 0.0094, 0.0088, 0.0088, 0.0087, 0.0084, 0.0082, 0.0081,\n",
      "        0.0081, 0.0076, 0.0070, 0.0067, 0.0062, 0.0061, 0.0061, 0.0060, 0.0059,\n",
      "        0.0057, 0.0057, 0.0056, 0.0056, 0.0054, 0.0052, 0.0051, 0.0050, 0.0050,\n",
      "        0.0049, 0.0047, 0.0045, 0.0045, 0.0044], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 83],\n",
      "        [301],\n",
      "        [700],\n",
      "        [ 10]], device='cuda:0') possible tokens:  tensor([ 2067,  7752,  4235,   228,   831, 18871,   479,  3055,  2680,   215,\n",
      "         9108,  8665,  3077,  9440,     3,  7966,     5,   239,  2084,    17,\n",
      "         7677,  4713,   474, 17893,  6605, 10854,  3493, 15250,     0,   903,\n",
      "         1591, 16053, 11937, 50041,   977,  1696,   162,   205,    66,  2214,\n",
      "        18855, 12762,  4198,  2699,    22,   692,  9054,    34,   175,   777],\n",
      "       device='cuda:0') Pred_data:  tensor(7752, device='cuda:0')\n",
      "i: 1\n",
      "next word:  भनेर\n",
      "1 Values:  tensor([0.3040, 0.2965, 0.0938, 0.0867, 0.0361, 0.0322, 0.0226, 0.0169, 0.0147,\n",
      "        0.0140, 0.0095, 0.0092, 0.0071, 0.0055, 0.0049, 0.0040, 0.0035, 0.0032,\n",
      "        0.0032, 0.0030, 0.0023, 0.0023, 0.0022, 0.0019, 0.0015, 0.0015, 0.0014,\n",
      "        0.0014, 0.0012, 0.0011, 0.0011, 0.0011, 0.0009, 0.0009, 0.0008, 0.0008,\n",
      "        0.0006, 0.0006, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0004, 0.0004, 0.0004], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752]], device='cuda:0') possible tokens:  tensor([    1,    93,    25,     2,   629,   121,     3,    95,    12,   246,\n",
      "           97,  4679,   643,    42,   141,  1622,   603,    83,  1887,  2437,\n",
      "           38,   240,   287,    15,  1066,   490,   147,  4587,   828,  1754,\n",
      "         7074,    19,  2248,   160,    59,  6199,  3146,  1423,  6492,    22,\n",
      "         2185,  2794,   102,    80,  5054,  8005,    32,    60,  1017, 18530],\n",
      "       device='cuda:0') Pred_data:  tensor(93, device='cuda:0')\n",
      "i: 2\n",
      "next word:  आग्रह\n",
      "2 Values:  tensor([0.1993, 0.0571, 0.0510, 0.0481, 0.0429, 0.0308, 0.0280, 0.0280, 0.0262,\n",
      "        0.0231, 0.0207, 0.0201, 0.0186, 0.0178, 0.0174, 0.0173, 0.0161, 0.0158,\n",
      "        0.0156, 0.0125, 0.0124, 0.0124, 0.0122, 0.0120, 0.0115, 0.0114, 0.0114,\n",
      "        0.0113, 0.0110, 0.0110, 0.0108, 0.0106, 0.0106, 0.0103, 0.0102, 0.0100,\n",
      "        0.0100, 0.0093, 0.0091, 0.0089, 0.0088, 0.0084, 0.0081, 0.0079, 0.0078,\n",
      "        0.0075, 0.0075, 0.0072, 0.0071, 0.0069], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93]], device='cuda:0') possible tokens:  tensor([    1,     5,   474,   160,  6347,    83,    34,    15,  1001,    22,\n",
      "          147,    28,    82,   479,   365,  2067,    17,   215,     0, 13259,\n",
      "           18,   603,   210,  7752,   135,    39,    51,   575,  5850,  2476,\n",
      "         9108,   261,  2773,  1974,    80,  7240,  1058,   903,     2,  1333,\n",
      "         7054,    45,  1039,  1918,  9316,  1469,    46,  8098,  1091,    43],\n",
      "       device='cuda:0') Pred_data:  tensor(474, device='cuda:0')\n",
      "i: 3\n",
      "next word:  गर्न\n",
      "3 Values:  tensor([0.3486, 0.1077, 0.1055, 0.0924, 0.0446, 0.0314, 0.0297, 0.0241, 0.0174,\n",
      "        0.0163, 0.0158, 0.0149, 0.0135, 0.0134, 0.0105, 0.0103, 0.0072, 0.0067,\n",
      "        0.0067, 0.0061, 0.0054, 0.0047, 0.0044, 0.0039, 0.0035, 0.0034, 0.0033,\n",
      "        0.0030, 0.0030, 0.0030, 0.0030, 0.0029, 0.0028, 0.0026, 0.0025, 0.0024,\n",
      "        0.0022, 0.0022, 0.0021, 0.0019, 0.0018, 0.0018, 0.0017, 0.0017, 0.0016,\n",
      "        0.0013, 0.0013, 0.0013, 0.0012, 0.0012], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474]], device='cuda:0') possible tokens:  tensor([   10,  3539,  1489,  3069,    11,  4530,    23,    85,   438,   547,\n",
      "          133,    36,    13,    86,   783, 17360,  6804,   686,  7671,     5,\n",
      "        12755,  2428,  1151,    56, 13690,  1847,  5247,    55,  5937,  4816,\n",
      "           27, 11217,  3175,   359,  5056, 14827,  1927,    17,  8604, 10206,\n",
      "          430,   262, 20587,  2526,  1567, 18300,  2157, 25499,  4195,  5584],\n",
      "       device='cuda:0') Pred_data:  tensor(10, device='cuda:0')\n",
      "i: 4\n",
      "next word:  चाहन्छु\n",
      "4 Values:  tensor([0.7696, 0.0217, 0.0149, 0.0137, 0.0111, 0.0085, 0.0085, 0.0079, 0.0066,\n",
      "        0.0066, 0.0056, 0.0055, 0.0054, 0.0053, 0.0051, 0.0048, 0.0048, 0.0046,\n",
      "        0.0045, 0.0042, 0.0038, 0.0037, 0.0037, 0.0037, 0.0035, 0.0032, 0.0032,\n",
      "        0.0030, 0.0030, 0.0029, 0.0028, 0.0028, 0.0027, 0.0026, 0.0026, 0.0025,\n",
      "        0.0025, 0.0024, 0.0024, 0.0023, 0.0023, 0.0023, 0.0023, 0.0022, 0.0022,\n",
      "        0.0022, 0.0021, 0.0021, 0.0021, 0.0021], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10]], device='cuda:0') possible tokens:  tensor([ 2067, 12762,  4235,   180,  6605, 10854,     5,   474,     1,  9108,\n",
      "          862, 16053,  1378,  3077,  3055, 17893,  2046,   831,   228,  3493,\n",
      "            3,  4198,  1531, 18871,  1313,  3263,   421, 11063,  7966,  8665,\n",
      "         7343,   175,   439,  3614,   139,  3039,  6816,  2084,  8061, 18855,\n",
      "        16357, 15250,  3576,   216,   977, 50041,    17,  4366, 10853,   239],\n",
      "       device='cuda:0') Pred_data:  tensor(2067, device='cuda:0')\n",
      "i: 5\n",
      "next word:  ।\n",
      "5 Values:  tensor([9.3549e-01, 3.1546e-02, 1.0681e-02, 6.4872e-03, 5.5156e-03, 1.9713e-03,\n",
      "        7.3485e-04, 5.5768e-04, 4.9906e-04, 3.4995e-04, 3.4357e-04, 3.3831e-04,\n",
      "        2.7995e-04, 2.7575e-04, 2.6133e-04, 2.4014e-04, 2.2550e-04, 2.1650e-04,\n",
      "        1.8195e-04, 1.7916e-04, 1.7819e-04, 1.7663e-04, 1.7183e-04, 1.6654e-04,\n",
      "        1.6444e-04, 1.4935e-04, 1.4231e-04, 1.4153e-04, 1.4032e-04, 1.3712e-04,\n",
      "        1.3513e-04, 1.3116e-04, 1.3046e-04, 1.2649e-04, 1.2379e-04, 1.1149e-04,\n",
      "        1.1075e-04, 1.0682e-04, 1.0575e-04, 1.0180e-04, 9.7836e-05, 9.7764e-05,\n",
      "        9.5365e-05, 9.1340e-05, 8.8469e-05, 8.4082e-05, 8.0287e-05, 7.9676e-05,\n",
      "        7.8773e-05, 7.7891e-05], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067]], device='cuda:0') possible tokens:  tensor([    1,     2,   121,     3,    12,    19,    29,    25,   141,   318,\n",
      "           93,   245,    15,    18,   497,  1754,   287, 10019,   490,   629,\n",
      "         1856,   240,  4679,  1017,    48,   212,   680,   160,   102,  5559,\n",
      "           59,    83,  8348,    60,  2532,   148,  2800,    42,  1423,   828,\n",
      "          111,     0,  3998,   211,   810,   143,  1887,    17,   492,   161],\n",
      "       device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 6\n",
      "next word:  यदि\n",
      "6 Values:  tensor([0.0950, 0.0630, 0.0473, 0.0374, 0.0367, 0.0335, 0.0332, 0.0325, 0.0259,\n",
      "        0.0252, 0.0234, 0.0227, 0.0226, 0.0226, 0.0218, 0.0202, 0.0199, 0.0191,\n",
      "        0.0187, 0.0185, 0.0185, 0.0171, 0.0165, 0.0156, 0.0152, 0.0148, 0.0145,\n",
      "        0.0142, 0.0133, 0.0133, 0.0128, 0.0124, 0.0124, 0.0122, 0.0113, 0.0109,\n",
      "        0.0108, 0.0102, 0.0102, 0.0101, 0.0101, 0.0098, 0.0096, 0.0095, 0.0095,\n",
      "        0.0093, 0.0092, 0.0092, 0.0091, 0.0091], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([  19,   22,  102,  387,   15,  160,  171,  497,  115,   83,  161,   54,\n",
      "          60,   90,  110,   87,   29,  212,  143,  211,   89,  120, 1829,  375,\n",
      "         934, 1017,  155, 1047,  111,   39,  148,   18,  129,    0,  604,   48,\n",
      "         334,  410,  100,   34,   67,  147,  266,   59,   80,  370,  153, 1469,\n",
      "          82,  204], device='cuda:0') Pred_data:  tensor(497, device='cuda:0')\n",
      "i: 7\n",
      "next word:  तिम्रो\n",
      "7 Values:  tensor([0.1553, 0.1047, 0.0909, 0.0709, 0.0642, 0.0356, 0.0314, 0.0287, 0.0244,\n",
      "        0.0233, 0.0219, 0.0210, 0.0190, 0.0190, 0.0175, 0.0168, 0.0134, 0.0126,\n",
      "        0.0119, 0.0112, 0.0099, 0.0091, 0.0087, 0.0085, 0.0082, 0.0081, 0.0080,\n",
      "        0.0076, 0.0076, 0.0075, 0.0075, 0.0075, 0.0074, 0.0074, 0.0070, 0.0066,\n",
      "        0.0066, 0.0065, 0.0064, 0.0061, 0.0060, 0.0057, 0.0056, 0.0055, 0.0055,\n",
      "        0.0053, 0.0053, 0.0051, 0.0051, 0.0050], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497]], device='cuda:0') possible tokens:  tensor([ 318, 1773, 1887,  160, 1047, 1664, 3800, 2993,   15, 1254, 9476,  129,\n",
      "          41,  143,  666, 1257,  790,   90,  102,   22, 2042,   83,  212, 5283,\n",
      "        1016,  507,   59,  155, 2287,  147,  110,  387,   67,  115,   82, 2277,\n",
      "          48,  120,   28, 2479,   34,  470,    2,   18,  121,   54,  497, 1829,\n",
      "         111,  852], device='cuda:0') Pred_data:  tensor(2287, device='cuda:0')\n",
      "i: 8\n",
      "next word:  इच्छा\n",
      "8 Values:  tensor([0.0890, 0.0555, 0.0533, 0.0460, 0.0316, 0.0315, 0.0314, 0.0291, 0.0261,\n",
      "        0.0258, 0.0256, 0.0252, 0.0247, 0.0242, 0.0238, 0.0234, 0.0230, 0.0226,\n",
      "        0.0209, 0.0194, 0.0174, 0.0169, 0.0164, 0.0155, 0.0154, 0.0153, 0.0140,\n",
      "        0.0137, 0.0133, 0.0128, 0.0124, 0.0124, 0.0123, 0.0116, 0.0114, 0.0108,\n",
      "        0.0098, 0.0098, 0.0097, 0.0097, 0.0093, 0.0093, 0.0091, 0.0090, 0.0090,\n",
      "        0.0086, 0.0084, 0.0083, 0.0082, 0.0080], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287]], device='cuda:0') possible tokens:  tensor([ 1813,   684,   135,  1717,  4199,   233,     0,   498,  1751,  3611,\n",
      "          299,     9,   722,   441,    87,   864,    34,   969,   352,    45,\n",
      "          386,   298,   201,  7680,  1222,  2417,   222,   321,  1398,    15,\n",
      "          249,  1085,   642,  7170,   621,   770,   399,  2778,   220,    82,\n",
      "         6347,   614,  1494,   672,  1670,  2150,  1001,  1330, 11879,  4796],\n",
      "       device='cuda:0') Pred_data:  tensor(1813, device='cuda:0')\n",
      "i: 9\n",
      "next word:  हुन्छ\n",
      "9 Values:  tensor([0.3005, 0.1319, 0.1053, 0.0619, 0.0481, 0.0460, 0.0416, 0.0401, 0.0208,\n",
      "        0.0191, 0.0143, 0.0100, 0.0082, 0.0075, 0.0069, 0.0068, 0.0065, 0.0062,\n",
      "        0.0062, 0.0061, 0.0054, 0.0054, 0.0052, 0.0051, 0.0048, 0.0047, 0.0045,\n",
      "        0.0039, 0.0039, 0.0038, 0.0037, 0.0037, 0.0037, 0.0033, 0.0032, 0.0032,\n",
      "        0.0030, 0.0030, 0.0029, 0.0029, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028,\n",
      "        0.0028, 0.0027, 0.0025, 0.0024, 0.0024], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813]], device='cuda:0') possible tokens:  tensor([   3,    4,    2,    8,   63,  404,   30,  253,   16,   80,    6,   17,\n",
      "        1858,  134,  381,  141, 2822,   35,  160,   78,   68,   42,  459,    5,\n",
      "         283, 1717,    0,  400,  246,  476,  689, 1257, 4061,   12, 1559, 1773,\n",
      "          51,  726,   38,  609,  771, 3368,   34,  728,  671,  147,  300,  102,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         376,   37], device='cuda:0') Pred_data:  tensor(30, device='cuda:0')\n",
      "i: 10\n",
      "next word:  ?\n",
      "10 Values:  tensor([0.6229, 0.0909, 0.0742, 0.0357, 0.0280, 0.0214, 0.0107, 0.0086, 0.0082,\n",
      "        0.0073, 0.0070, 0.0068, 0.0059, 0.0045, 0.0044, 0.0039, 0.0035, 0.0031,\n",
      "        0.0029, 0.0027, 0.0027, 0.0027, 0.0026, 0.0026, 0.0025, 0.0024, 0.0021,\n",
      "        0.0019, 0.0018, 0.0017, 0.0017, 0.0016, 0.0015, 0.0014, 0.0014, 0.0013,\n",
      "        0.0013, 0.0013, 0.0013, 0.0012, 0.0011, 0.0011, 0.0011, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0009], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30]], device='cuda:0') possible tokens:  tensor([   12,     2,   121,     3,     1,    38,    93,  1887,    42,    59,\n",
      "         1773,    17,    25,   287,  3800,  2287,    15,    34,  1423,   643,\n",
      "         1317,   160,   629,   141,   318,   492,    68,  7074,    83,   497,\n",
      "           90,   243,     0,  1257,   309,    41,   680,   143,   726,   147,\n",
      "          390,   402,   810,    35,  1066,   153,   246, 10019,    48,   722],\n",
      "       device='cuda:0') Pred_data:  tensor(2, device='cuda:0')\n",
      "i: 11\n",
      "next word:  तपाईको\n",
      "11 Values:  tensor([0.0846, 0.0839, 0.0617, 0.0614, 0.0415, 0.0363, 0.0325, 0.0310, 0.0309,\n",
      "        0.0307, 0.0286, 0.0261, 0.0244, 0.0236, 0.0178, 0.0173, 0.0170, 0.0167,\n",
      "        0.0165, 0.0163, 0.0154, 0.0147, 0.0146, 0.0140, 0.0127, 0.0121, 0.0117,\n",
      "        0.0111, 0.0109, 0.0108, 0.0103, 0.0101, 0.0096, 0.0092, 0.0091, 0.0091,\n",
      "        0.0090, 0.0088, 0.0087, 0.0087, 0.0086, 0.0085, 0.0084, 0.0084, 0.0081,\n",
      "        0.0079, 0.0078, 0.0077, 0.0077, 0.0076], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  83],\n",
      "        [ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2]], device='cuda:0') possible tokens:  tensor([ 1773,   160,  3800,  1887,   497,    15,    83,    80,    41,    19,\n",
      "           59,    82,   143,   318,    34,   141,    90,  5283,   492,  1254,\n",
      "          147,   211,   212,   111,  1257,     0,     3,   390,    18,  1664,\n",
      "          726,  2287,   443,   672,   155,  2042,    89,   680,   375,   550,\n",
      "          184,  1881, 10019,  2993,  1111,  3635,   309,   102,   985,   702],\n",
      "       device='cuda:0') Pred_data:  tensor(2042, device='cuda:0')\n",
      "i: 12\n",
      "next word:  यो\n",
      "12 Values:  tensor([0.0845, 0.0825, 0.0506, 0.0381, 0.0380, 0.0320, 0.0294, 0.0282, 0.0271,\n",
      "        0.0250, 0.0247, 0.0233, 0.0231, 0.0225, 0.0223, 0.0220, 0.0216, 0.0216,\n",
      "        0.0201, 0.0188, 0.0180, 0.0166, 0.0164, 0.0154, 0.0154, 0.0153, 0.0150,\n",
      "        0.0149, 0.0116, 0.0115, 0.0115, 0.0114, 0.0111, 0.0110, 0.0109, 0.0108,\n",
      "        0.0108, 0.0106, 0.0104, 0.0100, 0.0093, 0.0090, 0.0089, 0.0088, 0.0086,\n",
      "        0.0085, 0.0085, 0.0085, 0.0080, 0.0079], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 301],\n",
      "        [ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042]], device='cuda:0') possible tokens:  tensor([  135,   299,  1717,   399,   233,  1085,  1813,  1751,     0,   700,\n",
      "           34,   756,   352,   386,   298, 24939,  1222,    15,   129,  1599,\n",
      "           87,  7170,  4695,  9558,  1219,   365,   746,   618,  1670,   339,\n",
      "         1001,   869,    59,  4190,   220,  2200,  1058,   200,   222,   784,\n",
      "         2504,  1264,    82,  6347,  6813,   498,   169,   969,  4904,    45],\n",
      "       device='cuda:0') Pred_data:  tensor(15, device='cuda:0')\n",
      "i: 13\n",
      "next word:  लक्ष्य\n",
      "13 Values:  tensor([0.0800, 0.0570, 0.0525, 0.0449, 0.0391, 0.0325, 0.0314, 0.0295, 0.0285,\n",
      "        0.0243, 0.0239, 0.0235, 0.0231, 0.0228, 0.0226, 0.0211, 0.0208, 0.0199,\n",
      "        0.0198, 0.0198, 0.0181, 0.0177, 0.0177, 0.0173, 0.0157, 0.0157, 0.0152,\n",
      "        0.0146, 0.0143, 0.0134, 0.0131, 0.0129, 0.0115, 0.0114, 0.0112, 0.0108,\n",
      "        0.0104, 0.0103, 0.0103, 0.0102, 0.0100, 0.0100, 0.0097, 0.0093, 0.0091,\n",
      "        0.0088, 0.0087, 0.0086, 0.0086, 0.0085], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 700],\n",
      "        [  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15]], device='cuda:0') possible tokens:  tensor([  399,  4712,   575,    45,  3397,  3611,   689,  6347,  3150,   705,\n",
      "         1320,   147,    82,  4112,   169,  1632,    17,  1085,   969,  2807,\n",
      "           87,  4352,     0,    46, 11879,  1333,  2989,   111,   537,  2803,\n",
      "          233,  1717,  1449,   898,   576,   134,   163,   354,     5,  2294,\n",
      "         1509, 11787,  7354,    26,   553,  3682,   864,   510,  1670,  1315],\n",
      "       device='cuda:0') Pred_data:  tensor(576, device='cuda:0')\n",
      "i: 14\n",
      "next word:  पूरा\n",
      "14 Values:  tensor([0.1297, 0.0960, 0.0862, 0.0424, 0.0374, 0.0363, 0.0349, 0.0303, 0.0293,\n",
      "        0.0280, 0.0264, 0.0230, 0.0227, 0.0205, 0.0179, 0.0177, 0.0173, 0.0168,\n",
      "        0.0128, 0.0125, 0.0123, 0.0121, 0.0118, 0.0118, 0.0117, 0.0116, 0.0112,\n",
      "        0.0109, 0.0106, 0.0105, 0.0097, 0.0091, 0.0086, 0.0083, 0.0082, 0.0081,\n",
      "        0.0080, 0.0079, 0.0077, 0.0073, 0.0072, 0.0072, 0.0068, 0.0067, 0.0066,\n",
      "        0.0064, 0.0062, 0.0060, 0.0060, 0.0056], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  10],\n",
      "        [7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576]], device='cuda:0') possible tokens:  tensor([ 253,   80,  140,    8, 1257,  381,  143, 1773,  111, 3800,    4,    3,\n",
      "         258,   30,  160,  985,   17,    1,  300,  246, 4061,  907,  917,  726,\n",
      "          38, 1887,    2, 1664,   12,   39,   34, 6488, 1367,   51,  318,  522,\n",
      "         147,   82,  689, 3112,  604,  614, 7980, 1029,    5, 1254,  531,  134,\n",
      "          46,    0], device='cuda:0') Pred_data:  tensor(253, device='cuda:0')\n",
      "i: 15\n",
      "next word:  भएको\n",
      "15 Values:  tensor([0.5728, 0.1028, 0.0423, 0.0297, 0.0244, 0.0235, 0.0201, 0.0149, 0.0137,\n",
      "        0.0108, 0.0102, 0.0091, 0.0089, 0.0069, 0.0065, 0.0063, 0.0062, 0.0058,\n",
      "        0.0056, 0.0054, 0.0048, 0.0047, 0.0046, 0.0043, 0.0034, 0.0033, 0.0031,\n",
      "        0.0027, 0.0027, 0.0026, 0.0025, 0.0024, 0.0024, 0.0024, 0.0023, 0.0023,\n",
      "        0.0023, 0.0022, 0.0021, 0.0019, 0.0019, 0.0018, 0.0016, 0.0016, 0.0015,\n",
      "        0.0015, 0.0014, 0.0014, 0.0013, 0.0013], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[7752],\n",
      "        [  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253]], device='cuda:0') possible tokens:  tensor([  10,   30, 1567,  417,    6,   13,   20,  172,  262,  899,   37,   56,\n",
      "        2330,   78, 1489,  133, 3164,  786, 2428, 5937,  297, 5044,   23,   36,\n",
      "          17,  545,  224, 7372,  272, 2246,  514, 1317,   11,  425,  287,  377,\n",
      "         559, 4410,   33,   68, 4500, 1101, 2526,  438, 1157,   86,    4, 2353,\n",
      "          27, 1164], device='cuda:0') Pred_data:  tensor(6, device='cuda:0')\n",
      "i: 16\n",
      "next word:  छैन\n",
      "16 Values:  tensor([0.4139, 0.2352, 0.0751, 0.0660, 0.0639, 0.0199, 0.0083, 0.0082, 0.0074,\n",
      "        0.0074, 0.0070, 0.0063, 0.0053, 0.0043, 0.0040, 0.0039, 0.0036, 0.0032,\n",
      "        0.0032, 0.0030, 0.0029, 0.0029, 0.0027, 0.0025, 0.0023, 0.0021, 0.0020,\n",
      "        0.0019, 0.0019, 0.0019, 0.0017, 0.0017, 0.0017, 0.0017, 0.0016, 0.0016,\n",
      "        0.0015, 0.0015, 0.0014, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0012,\n",
      "        0.0012, 0.0011, 0.0011, 0.0011, 0.0011], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  93],\n",
      "        [ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6]], device='cuda:0') possible tokens:  tensor([ 1934,    35,    68,     8,     4, 10662,    30,    42,     3,    37,\n",
      "         1773,   545,    12,  1184,    47,     2,     0,   786,   283,  2612,\n",
      "          342,   287,  4145,  1317,  1195,   117,  1310,    93,   160,   333,\n",
      "          376,  1157,   764,    45,    39,  1359,    18,  1869,    16,  2422,\n",
      "          481,  1091,  1257,     5,   294,   362,   231,  6111,  1632,    59],\n",
      "       device='cuda:0') Pred_data:  tensor(35, device='cuda:0')\n",
      "i: 17\n",
      "next word:  ।\n",
      "17 Values:  tensor([4.4716e-01, 3.6851e-01, 7.7062e-02, 3.7276e-02, 1.1743e-02, 8.9694e-03,\n",
      "        8.7721e-03, 4.6604e-03, 4.5580e-03, 2.9610e-03, 2.8332e-03, 2.1900e-03,\n",
      "        2.1528e-03, 2.0497e-03, 1.8778e-03, 1.2601e-03, 1.2516e-03, 1.0434e-03,\n",
      "        8.9935e-04, 6.4089e-04, 6.0740e-04, 5.8718e-04, 5.6699e-04, 5.4836e-04,\n",
      "        4.8500e-04, 4.5336e-04, 4.5107e-04, 4.5021e-04, 4.4179e-04, 4.3180e-04,\n",
      "        4.2127e-04, 4.0596e-04, 4.0021e-04, 3.9803e-04, 3.8131e-04, 3.7496e-04,\n",
      "        3.7218e-04, 3.6970e-04, 3.6207e-04, 3.6043e-04, 3.5170e-04, 3.4526e-04,\n",
      "        3.4232e-04, 3.4161e-04, 3.4009e-04, 3.3237e-04, 3.3215e-04, 2.9916e-04,\n",
      "        2.9343e-04, 2.9045e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 474],\n",
      "        [  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35]], device='cuda:0') possible tokens:  tensor([    1,    12,     2,     3,    93,    25,    42,   287,   121,   629,\n",
      "           38,    17,  1066,  1773,   141,    19,    59,   643, 10019,    80,\n",
      "         1257,   243,   318,  2437,    15,  1856,  1622,   497,   240,     8,\n",
      "           37,     0,   528,    34,    30,   246,  2207,  8348,  1887,   759,\n",
      "           90,  1754,    95,     4,   217,    97,   211,   603,    29,   489],\n",
      "       device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 18\n",
      "next word:  मैले\n",
      "18 Values:  tensor([0.0924, 0.0557, 0.0503, 0.0473, 0.0454, 0.0442, 0.0418, 0.0384, 0.0230,\n",
      "        0.0228, 0.0222, 0.0214, 0.0210, 0.0195, 0.0192, 0.0185, 0.0183, 0.0182,\n",
      "        0.0181, 0.0178, 0.0172, 0.0172, 0.0154, 0.0149, 0.0136, 0.0135, 0.0134,\n",
      "        0.0127, 0.0122, 0.0121, 0.0118, 0.0114, 0.0111, 0.0111, 0.0111, 0.0111,\n",
      "        0.0108, 0.0105, 0.0104, 0.0104, 0.0103, 0.0099, 0.0098, 0.0097, 0.0095,\n",
      "        0.0093, 0.0090, 0.0087, 0.0087, 0.0076], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  10],\n",
      "        [2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([ 1887,   160,  1254,  1773,  3800,  1257,   497,   211,    82,    83,\n",
      "         3112,    15,    34,  1664,   143,   111, 14767, 19270,    80,    19,\n",
      "          147,  3038, 23331,     0,   318,    90,  1856,   135,    41,  2993,\n",
      "         4994,  3769,   129,   102,   985,  3260,  2287,    46,   772,   726,\n",
      "           59,  1017,  2042,   680,   907,   604,  2362,    18,  7980,    87],\n",
      "       device='cuda:0') Pred_data:  tensor(160, device='cuda:0')\n",
      "i: 19\n",
      "next word:  नै\n",
      "19 Values:  tensor([0.1062, 0.0638, 0.0513, 0.0492, 0.0442, 0.0400, 0.0377, 0.0343, 0.0336,\n",
      "        0.0309, 0.0307, 0.0273, 0.0226, 0.0201, 0.0198, 0.0186, 0.0180, 0.0179,\n",
      "        0.0166, 0.0150, 0.0144, 0.0139, 0.0129, 0.0128, 0.0127, 0.0125, 0.0123,\n",
      "        0.0122, 0.0120, 0.0120, 0.0114, 0.0112, 0.0112, 0.0105, 0.0105, 0.0103,\n",
      "        0.0094, 0.0084, 0.0083, 0.0082, 0.0081, 0.0080, 0.0080, 0.0080, 0.0073,\n",
      "        0.0073, 0.0073, 0.0073, 0.0071, 0.0069], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[2067],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35],\n",
      "        [   1],\n",
      "        [ 160]], device='cuda:0') possible tokens:  tensor([   34,  1254,    80,  8685,  2084,   147,  3470,   246,  1974,    15,\n",
      "           17,     5,    82,   771,   772,  3769,  1461,  2182,   365,  1039,\n",
      "          281,    38,    46,  2287,    51,  6689,  1257,  4937,   530,  2362,\n",
      "          258,  9598,  6975,   517,    59,   985,    39, 10933,   531,    48,\n",
      "          111,   261,   598,   604,  1559,     0,  5484,   726,   102,  1578],\n",
      "       device='cuda:0') Pred_data:  tensor(17, device='cuda:0')\n",
      "i: 20\n",
      "next word:  आमाको\n",
      "20 Values:  tensor([0.0963, 0.0658, 0.0564, 0.0535, 0.0495, 0.0442, 0.0418, 0.0385, 0.0299,\n",
      "        0.0279, 0.0270, 0.0232, 0.0222, 0.0200, 0.0200, 0.0169, 0.0156, 0.0153,\n",
      "        0.0148, 0.0145, 0.0141, 0.0140, 0.0140, 0.0134, 0.0130, 0.0126, 0.0125,\n",
      "        0.0121, 0.0116, 0.0114, 0.0110, 0.0109, 0.0104, 0.0103, 0.0101, 0.0099,\n",
      "        0.0093, 0.0092, 0.0088, 0.0088, 0.0086, 0.0085, 0.0084, 0.0082, 0.0081,\n",
      "        0.0079, 0.0077, 0.0075, 0.0075, 0.0073], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   1],\n",
      "        [ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35],\n",
      "        [   1],\n",
      "        [ 160],\n",
      "        [  17]], device='cuda:0') possible tokens:  tensor([   34,    15,   147,   111,  1254,    82,    46,   246,   160,    87,\n",
      "          772,   281,  1469,  1887,   143,  3769,    10,  9598,  4679,    48,\n",
      "          530,   726,    80,  6689,   365,  3112,  1257,   110,    83,    59,\n",
      "          985,   648,    51,    17,    11,   547,  2362,    18,   134,   236,\n",
      "        10933,  3800,  1773,   578,  3611,  6347,  2287,  3470,   129,    26],\n",
      "       device='cuda:0') Pred_data:  tensor(2362, device='cuda:0')\n",
      "i: 21\n",
      "next word:  जीवनको\n",
      "21 Values:  tensor([0.1722, 0.0704, 0.0427, 0.0369, 0.0337, 0.0334, 0.0328, 0.0268, 0.0254,\n",
      "        0.0244, 0.0238, 0.0231, 0.0229, 0.0229, 0.0218, 0.0216, 0.0214, 0.0202,\n",
      "        0.0187, 0.0175, 0.0156, 0.0140, 0.0137, 0.0130, 0.0128, 0.0126, 0.0117,\n",
      "        0.0111, 0.0109, 0.0104, 0.0100, 0.0093, 0.0091, 0.0089, 0.0089, 0.0089,\n",
      "        0.0088, 0.0087, 0.0083, 0.0083, 0.0080, 0.0078, 0.0077, 0.0075, 0.0074,\n",
      "        0.0072, 0.0071, 0.0071, 0.0066, 0.0061], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 497],\n",
      "        [2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35],\n",
      "        [   1],\n",
      "        [ 160],\n",
      "        [  17],\n",
      "        [2362]], device='cuda:0') possible tokens:  tensor([ 9558,   441,   233,   249,   684,  2862,  1819,  1599,    26,   498,\n",
      "         2316,  7425,     0,  1813,  2504,    87,  2170,  1670,     9,   969,\n",
      "         1219,  2417,   339,  2148,  4712,  8486,   864,  2150,  3249, 26507,\n",
      "         1717,   386, 10207, 13202,   303,    53,  1807,    17,   135, 26597,\n",
      "           45,  4999,   537,   138,  2267,   384,  2792,  9083,  5417,  5816],\n",
      "       device='cuda:0') Pred_data:  tensor(1670, device='cuda:0')\n",
      "i: 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  एउटा\n",
      "22 Values:  tensor([0.1537, 0.0835, 0.0748, 0.0515, 0.0346, 0.0325, 0.0291, 0.0285, 0.0260,\n",
      "        0.0257, 0.0247, 0.0229, 0.0209, 0.0209, 0.0203, 0.0174, 0.0170, 0.0168,\n",
      "        0.0167, 0.0150, 0.0147, 0.0142, 0.0137, 0.0127, 0.0117, 0.0117, 0.0113,\n",
      "        0.0105, 0.0102, 0.0098, 0.0096, 0.0094, 0.0086, 0.0086, 0.0086, 0.0081,\n",
      "        0.0081, 0.0081, 0.0077, 0.0071, 0.0070, 0.0068, 0.0067, 0.0065, 0.0063,\n",
      "        0.0062, 0.0061, 0.0059, 0.0057, 0.0057], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[2287],\n",
      "        [1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35],\n",
      "        [   1],\n",
      "        [ 160],\n",
      "        [  17],\n",
      "        [2362],\n",
      "        [1670]], device='cuda:0') possible tokens:  tensor([    9,   435,   651,    87,   498,   576,   339,  5126,  2862,   236,\n",
      "         2294,  7732, 18298,  4693,  1058,   125,   441,   496,   303,   705,\n",
      "         3425,     0,  1449,   969,  2783,   801,  2108,  2422,    45,  2271,\n",
      "           82,  1819,  3032,   537,   169,  7314,    41,  1458,  5485,  2883,\n",
      "           18,   307,  4054,  5705,  6978,  1238,   590,  3488,   627,   833],\n",
      "       device='cuda:0') Pred_data:  tensor(82, device='cuda:0')\n",
      "i: 23\n",
      "next word:  हिस्सा\n",
      "23 Values:  tensor([0.3779, 0.0473, 0.0377, 0.0371, 0.0292, 0.0282, 0.0255, 0.0255, 0.0254,\n",
      "        0.0225, 0.0197, 0.0174, 0.0172, 0.0150, 0.0148, 0.0140, 0.0132, 0.0122,\n",
      "        0.0100, 0.0096, 0.0093, 0.0092, 0.0092, 0.0090, 0.0089, 0.0085, 0.0084,\n",
      "        0.0081, 0.0080, 0.0079, 0.0073, 0.0066, 0.0062, 0.0061, 0.0061, 0.0061,\n",
      "        0.0060, 0.0060, 0.0060, 0.0059, 0.0056, 0.0056, 0.0055, 0.0055, 0.0054,\n",
      "        0.0051, 0.0051, 0.0048, 0.0047, 0.0045], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[1813],\n",
      "        [  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35],\n",
      "        [   1],\n",
      "        [ 160],\n",
      "        [  17],\n",
      "        [2362],\n",
      "        [1670],\n",
      "        [  82]], device='cuda:0') possible tokens:  tensor([ 2862,  5097,    45,   651,  2883,   450,     0,    49,   125,  1449,\n",
      "          601,   905,   576,   969,  3151,  4000,   134,  3249,    79,  1846,\n",
      "         5356,  2792,   494,    26,  2783,   706,   496,  2155,    43,  1519,\n",
      "         2294,  2195,  6550, 21033,   313,  2156,  1123,  5126,   622,   647,\n",
      "           87,   435,  5897,  2925,   737,   129,  2159, 25439,   537,   502],\n",
      "       device='cuda:0') Pred_data:  tensor(2883, device='cuda:0')\n",
      "i: 24\n",
      "next word:  पायौं\n",
      "24 Values:  tensor([0.1056, 0.1021, 0.0874, 0.0706, 0.0563, 0.0439, 0.0405, 0.0379, 0.0301,\n",
      "        0.0276, 0.0221, 0.0218, 0.0217, 0.0192, 0.0190, 0.0179, 0.0170, 0.0166,\n",
      "        0.0159, 0.0152, 0.0147, 0.0127, 0.0113, 0.0104, 0.0096, 0.0090, 0.0090,\n",
      "        0.0079, 0.0077, 0.0077, 0.0075, 0.0072, 0.0069, 0.0067, 0.0066, 0.0064,\n",
      "        0.0060, 0.0060, 0.0056, 0.0055, 0.0054, 0.0051, 0.0050, 0.0048, 0.0048,\n",
      "        0.0046, 0.0046, 0.0045, 0.0043, 0.0042], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  30],\n",
      "        [   2],\n",
      "        [2042],\n",
      "        [  15],\n",
      "        [ 576],\n",
      "        [ 253],\n",
      "        [   6],\n",
      "        [  35],\n",
      "        [   1],\n",
      "        [ 160],\n",
      "        [  17],\n",
      "        [2362],\n",
      "        [1670],\n",
      "        [  82],\n",
      "        [2883]], device='cuda:0') possible tokens:  tensor([  531,   246,   306,     8,  7662,  4366,   469,     6,   140,  1359,\n",
      "          626,    17,    43,    80,     5,   138,  6541,   127,   454,   238,\n",
      "         1090,  3855,   267,     0,   113,   695,   166,    21,  8799,    20,\n",
      "          799,     3, 12806,  8402,  3281,   726,    12,    47,   488,   174,\n",
      "          670,    37,    16,    38,  1274,   519,   253,  1418,   160,  3755],\n",
      "       device='cuda:0') Pred_data:  tensor(12806, device='cuda:0')\n",
      "i: 25\n",
      "next word:  ।\n",
      "25 Values:  tensor([6.6168e-01, 1.3200e-01, 5.0103e-02, 3.7153e-02, 2.2321e-02, 2.0632e-02,\n",
      "        1.6005e-02, 8.1568e-03, 4.3353e-03, 4.2899e-03, 3.9405e-03, 3.3157e-03,\n",
      "        3.0793e-03, 2.5756e-03, 2.1687e-03, 2.0760e-03, 1.8889e-03, 1.5724e-03,\n",
      "        1.5516e-03, 1.4779e-03, 1.4138e-03, 1.2788e-03, 1.1491e-03, 1.1221e-03,\n",
      "        1.0309e-03, 1.0271e-03, 1.0217e-03, 8.1474e-04, 7.0974e-04, 6.3040e-04,\n",
      "        5.8163e-04, 5.8046e-04, 5.7843e-04, 5.7701e-04, 5.5032e-04, 5.4941e-04,\n",
      "        5.4913e-04, 5.1374e-04, 4.9943e-04, 4.8504e-04, 4.7050e-04, 4.5733e-04,\n",
      "        4.5443e-04, 4.1431e-04, 4.0691e-04, 3.7818e-04, 3.6958e-04, 3.5825e-04,\n",
      "        3.5410e-04, 3.5020e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[    2],\n",
      "        [ 2042],\n",
      "        [   15],\n",
      "        [  576],\n",
      "        [  253],\n",
      "        [    6],\n",
      "        [   35],\n",
      "        [    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806]], device='cuda:0') possible tokens:  tensor([   1,    2,    3,   12,   93,   25,  121,   19,  246,  240,  141,  629,\n",
      "          42,    5,  287, 1017,   59, 6199,   15,  160, 1754,   97, 1856, 1622,\n",
      "         810,  344,  603,   38,  245,  300, 4587,   83,  680,   17, 8005,   80,\n",
      "          34, 2248,   95,   90,  492,  643,   43,  413,  828, 4679,  138,  687,\n",
      "          18,    0], device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 26\n",
      "next word:  पुरुष\n",
      "26 Values:  tensor([0.0815, 0.0790, 0.0549, 0.0425, 0.0403, 0.0339, 0.0308, 0.0272, 0.0271,\n",
      "        0.0270, 0.0266, 0.0238, 0.0238, 0.0226, 0.0209, 0.0196, 0.0189, 0.0186,\n",
      "        0.0186, 0.0177, 0.0169, 0.0167, 0.0166, 0.0147, 0.0146, 0.0146, 0.0133,\n",
      "        0.0130, 0.0126, 0.0124, 0.0122, 0.0121, 0.0121, 0.0117, 0.0117, 0.0113,\n",
      "        0.0110, 0.0106, 0.0098, 0.0094, 0.0091, 0.0091, 0.0090, 0.0089, 0.0089,\n",
      "        0.0087, 0.0087, 0.0086, 0.0083, 0.0081], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 2042],\n",
      "        [   15],\n",
      "        [  576],\n",
      "        [  253],\n",
      "        [    6],\n",
      "        [   35],\n",
      "        [    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1]], device='cuda:0') possible tokens:  tensor([  160,   726,  2362,    15,   722,    19,   211,    83,    82,   147,\n",
      "         1456,   497,  1887,    59,  3800,   907,  1254,    34,    18,  2217,\n",
      "          129, 14767,   721,  3732,  3969,   680,     0,    87,    80,   985,\n",
      "        23331,   218,   135,  1856,   102,    54,   375,   143,  7253,  4994,\n",
      "          604,  4090,   111,   244,    41, 19270,  1670,  4199,    46,  1047],\n",
      "       device='cuda:0') Pred_data:  tensor(726, device='cuda:0')\n",
      "i: 27\n",
      "next word:  भएकै\n",
      "27 Values:  tensor([0.2244, 0.1607, 0.0997, 0.0635, 0.0529, 0.0449, 0.0368, 0.0368, 0.0233,\n",
      "        0.0227, 0.0139, 0.0131, 0.0116, 0.0100, 0.0100, 0.0093, 0.0092, 0.0074,\n",
      "        0.0074, 0.0069, 0.0068, 0.0064, 0.0061, 0.0060, 0.0058, 0.0058, 0.0057,\n",
      "        0.0056, 0.0053, 0.0053, 0.0053, 0.0052, 0.0050, 0.0049, 0.0044, 0.0044,\n",
      "        0.0044, 0.0042, 0.0041, 0.0041, 0.0038, 0.0034, 0.0031, 0.0031, 0.0030,\n",
      "        0.0029, 0.0029, 0.0029, 0.0028, 0.0028], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   15],\n",
      "        [  576],\n",
      "        [  253],\n",
      "        [    6],\n",
      "        [   35],\n",
      "        [    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726]], device='cuda:0') possible tokens:  tensor([ 2367,    17,     3,     2,    43,   138,   726,   246,   376,    14,\n",
      "        28529,  1359,    42,  1269,  5447,   684,   497,  6240,   240,   344,\n",
      "          492,  3146,    95,  1219,  4100,  3452,   635,    18,   643,  4090,\n",
      "          751,  5917,     5,    38,  9843,   898,  1115,   174, 10515,  6356,\n",
      "         1580,     0,    68,  4340,    84,   771,   722,    12,    82, 13545],\n",
      "       device='cuda:0') Pred_data:  tensor(2367, device='cuda:0')\n",
      "i: 28\n",
      "next word:  कारण\n",
      "28 Values:  tensor([9.6171e-01, 7.2844e-03, 5.8478e-03, 5.4762e-03, 3.4425e-03, 3.1047e-03,\n",
      "        1.7567e-03, 1.2935e-03, 9.1014e-04, 8.2827e-04, 7.0688e-04, 4.5989e-04,\n",
      "        4.0436e-04, 3.9516e-04, 3.8258e-04, 3.6784e-04, 3.5480e-04, 3.5348e-04,\n",
      "        2.9524e-04, 2.8195e-04, 2.4870e-04, 2.3582e-04, 2.2821e-04, 2.0017e-04,\n",
      "        1.9389e-04, 1.9381e-04, 1.7487e-04, 1.6913e-04, 1.6178e-04, 1.5735e-04,\n",
      "        1.5192e-04, 1.3837e-04, 1.3746e-04, 1.3246e-04, 1.3164e-04, 1.2892e-04,\n",
      "        1.2725e-04, 1.2479e-04, 1.2250e-04, 1.2217e-04, 1.1754e-04, 1.1233e-04,\n",
      "        1.1189e-04, 1.0637e-04, 1.0549e-04, 1.0355e-04, 1.0276e-04, 9.9952e-05,\n",
      "        9.9877e-05, 9.8258e-05], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[  576],\n",
      "        [  253],\n",
      "        [    6],\n",
      "        [   35],\n",
      "        [    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367]], device='cuda:0') possible tokens:  tensor([   49,    47,   568,   241,  6290,   179,  2399,    45,   202,  2178,\n",
      "           35,    92,    18,   313,    68,     8,  1632,     5,   116,   119,\n",
      "          923,  1934,   551,  2989,   333,  1188,   165,  2156,  5368,    52,\n",
      "          567,   177, 16643,    37,  2438,    57,     4,   103,    94,   117,\n",
      "            2,  2475,     0,     9,   287,   651,    87,  1732,   946,   242],\n",
      "       device='cuda:0') Pred_data:  tensor(49, device='cuda:0')\n",
      "i: 29\n",
      "next word:  अरू\n",
      "29 Values:  tensor([0.1073, 0.0799, 0.0530, 0.0423, 0.0421, 0.0413, 0.0408, 0.0382, 0.0382,\n",
      "        0.0295, 0.0289, 0.0281, 0.0259, 0.0231, 0.0207, 0.0163, 0.0160, 0.0157,\n",
      "        0.0147, 0.0147, 0.0144, 0.0142, 0.0133, 0.0118, 0.0118, 0.0114, 0.0112,\n",
      "        0.0103, 0.0103, 0.0102, 0.0100, 0.0097, 0.0095, 0.0093, 0.0090, 0.0090,\n",
      "        0.0088, 0.0085, 0.0083, 0.0083, 0.0080, 0.0079, 0.0075, 0.0074, 0.0074,\n",
      "        0.0073, 0.0073, 0.0072, 0.0071, 0.0070], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  253],\n",
      "        [    6],\n",
      "        [   35],\n",
      "        [    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49]], device='cuda:0') possible tokens:  tensor([ 160,  726,   80,    5, 4090, 2362,   15, 1456,  907,  722,    0,    2,\n",
      "         925,   17,  985,  218,   34,   59,  129,  147, 1461, 3800,   51,  112,\n",
      "          83,  246,  105, 1272,  550, 3732,   46, 9971,  132,  684,  751, 4329,\n",
      "           3,  648,   18,   82,   90, 3969, 2659,  721,   41, 1210, 3371, 2940,\n",
      "           1,  244], device='cuda:0') Pred_data:  tensor(985, device='cuda:0')\n",
      "i: 30\n",
      "next word:  नै\n",
      "30 Values:  tensor([0.3006, 0.0882, 0.0699, 0.0599, 0.0585, 0.0342, 0.0331, 0.0331, 0.0266,\n",
      "        0.0249, 0.0216, 0.0192, 0.0171, 0.0143, 0.0140, 0.0100, 0.0099, 0.0085,\n",
      "        0.0083, 0.0078, 0.0075, 0.0073, 0.0070, 0.0070, 0.0064, 0.0058, 0.0058,\n",
      "        0.0057, 0.0055, 0.0049, 0.0049, 0.0047, 0.0044, 0.0043, 0.0043, 0.0042,\n",
      "        0.0041, 0.0039, 0.0039, 0.0039, 0.0037, 0.0036, 0.0035, 0.0035, 0.0034,\n",
      "        0.0034, 0.0034, 0.0034, 0.0034, 0.0034], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[    6],\n",
      "        [   35],\n",
      "        [    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985]], device='cuda:0') possible tokens:  tensor([   17,   726,  1432,    41,     5,  1869,    51,    46,    80,   702,\n",
      "         3969,   546,  1457,   985,  2940,  4090,     0,    39,    45, 14098,\n",
      "         1456,   762, 10195,   898,    26,  4930,   722,  9971, 19270,   132,\n",
      "         5283,  1817,   916,  1272,  1310,   582,  1846,   907, 14767,  1819,\n",
      "          100,    38,   751,  5302,   138,   935,   297,  6202,  1283,  5581],\n",
      "       device='cuda:0') Pred_data:  tensor(17, device='cuda:0')\n",
      "i: 31\n",
      "next word:  उसको\n",
      "31 Values:  tensor([0.1781, 0.1363, 0.0479, 0.0345, 0.0333, 0.0290, 0.0259, 0.0257, 0.0255,\n",
      "        0.0218, 0.0217, 0.0210, 0.0197, 0.0188, 0.0181, 0.0174, 0.0165, 0.0156,\n",
      "        0.0151, 0.0146, 0.0144, 0.0137, 0.0132, 0.0124, 0.0122, 0.0119, 0.0116,\n",
      "        0.0103, 0.0102, 0.0102, 0.0097, 0.0094, 0.0092, 0.0087, 0.0083, 0.0082,\n",
      "        0.0078, 0.0072, 0.0070, 0.0069, 0.0067, 0.0064, 0.0063, 0.0063, 0.0061,\n",
      "        0.0061, 0.0061, 0.0059, 0.0058, 0.0056], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   35],\n",
      "        [    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17]], device='cuda:0') possible tokens:  tensor([  35,  726,  297,   37,  335,   64,    7,    8, 1272,    5, 4733,   30,\n",
      "         481,  347,  545, 2650,  935,  907,  762,   34,  342,   20,  132,   10,\n",
      "         134, 1468, 2940,  394,    4,   46, 4090,  300, 2427,  985,   26,  661,\n",
      "        2227,  648,  283,  117, 2393,  111,   78,   51, 1869,  328,  582,   16,\n",
      "        1273,   15], device='cuda:0') Pred_data:  tensor(648, device='cuda:0')\n",
      "i: 32\n",
      "next word:  आफ्नै\n",
      "32 Values:  tensor([0.1235, 0.0613, 0.0407, 0.0392, 0.0352, 0.0342, 0.0335, 0.0334, 0.0314,\n",
      "        0.0268, 0.0226, 0.0224, 0.0219, 0.0215, 0.0206, 0.0205, 0.0188, 0.0185,\n",
      "        0.0175, 0.0169, 0.0158, 0.0156, 0.0153, 0.0152, 0.0151, 0.0144, 0.0137,\n",
      "        0.0135, 0.0120, 0.0120, 0.0118, 0.0111, 0.0111, 0.0109, 0.0104, 0.0102,\n",
      "        0.0102, 0.0102, 0.0100, 0.0096, 0.0096, 0.0095, 0.0093, 0.0091, 0.0091,\n",
      "        0.0090, 0.0090, 0.0089, 0.0089, 0.0088], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[    1],\n",
      "        [  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648]], device='cuda:0') possible tokens:  tensor([ 1599,     0,  1219,   249,   233,  1670,    34,  2504,   441,   135,\n",
      "         1813, 17283,    43,   138,  1699,  7535,  2362,   499,   282,   722,\n",
      "        17236,   386,     9,  1819,  4149,  5816,  4190,  1585,  9971,   352,\n",
      "            5,  2862,   299,   498,    46,  2591,  3803,    17,  3085,  6813,\n",
      "          747,  1831,    82,  2792,   213,  1330,  4712,   520, 12854,    59],\n",
      "       device='cuda:0') Pred_data:  tensor(282, device='cuda:0')\n",
      "i: 33\n",
      "next word:  मात्र\n",
      "33 Values:  tensor([0.2143, 0.0576, 0.0409, 0.0366, 0.0289, 0.0283, 0.0256, 0.0239, 0.0232,\n",
      "        0.0231, 0.0218, 0.0207, 0.0205, 0.0181, 0.0172, 0.0172, 0.0172, 0.0172,\n",
      "        0.0157, 0.0145, 0.0139, 0.0137, 0.0137, 0.0135, 0.0128, 0.0128, 0.0125,\n",
      "        0.0118, 0.0117, 0.0110, 0.0106, 0.0105, 0.0105, 0.0105, 0.0103, 0.0103,\n",
      "        0.0102, 0.0099, 0.0097, 0.0096, 0.0096, 0.0095, 0.0094, 0.0091, 0.0089,\n",
      "        0.0088, 0.0087, 0.0082, 0.0081, 0.0076], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  160],\n",
      "        [   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648],\n",
      "        [  282]], device='cuda:0') possible tokens:  tensor([ 1599,   135,     0,   352,   722,   441,  6088,     8,  1813,  2862,\n",
      "           43, 19852,  4190,  2504,  1670,  2362,   621,  1114,  2361,   233,\n",
      "          618,  5931,  5498, 16045,  3732,   520,  2217,   333,   138,  3085,\n",
      "        20960,   498,  4411,  1146,  7199,  9971,   249,    37,   642,     9,\n",
      "         1819,  1398,   499,  2591,   721, 17213,   898, 26403,  3969, 16691],\n",
      "       device='cuda:0') Pred_data:  tensor(43, device='cuda:0')\n",
      "i: 34\n",
      "next word:  नभई\n",
      "34 Values:  tensor([0.0902, 0.0861, 0.0802, 0.0520, 0.0498, 0.0491, 0.0379, 0.0371, 0.0366,\n",
      "        0.0346, 0.0232, 0.0208, 0.0180, 0.0178, 0.0171, 0.0161, 0.0160, 0.0153,\n",
      "        0.0137, 0.0131, 0.0130, 0.0124, 0.0119, 0.0117, 0.0113, 0.0107, 0.0105,\n",
      "        0.0105, 0.0103, 0.0097, 0.0093, 0.0092, 0.0092, 0.0089, 0.0088, 0.0087,\n",
      "        0.0086, 0.0086, 0.0085, 0.0082, 0.0081, 0.0079, 0.0079, 0.0078, 0.0077,\n",
      "        0.0077, 0.0073, 0.0070, 0.0070, 0.0069], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   17],\n",
      "        [ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648],\n",
      "        [  282],\n",
      "        [   43]], device='cuda:0') possible tokens:  tensor([ 117,  911,    8,   26,   37,  297,  545, 1269,   30,   20,   16, 2663,\n",
      "           0, 1506, 2862,  108,    6,  684,  135,    5, 2103,  249,   49, 2217,\n",
      "        5547,   68,   34,    4,  651,   24,   57, 1201,   43, 3618,   10, 7535,\n",
      "        2361,  376,  384, 1050,   35,  833, 1247,  222,  342,  959,   78,    2,\n",
      "          45,    1], device='cuda:0') Pred_data:  tensor(911, device='cuda:0')\n",
      "i: 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  उनीहरूको\n",
      "35 Values:  tensor([0.1253, 0.0741, 0.0600, 0.0528, 0.0490, 0.0377, 0.0344, 0.0343, 0.0301,\n",
      "        0.0298, 0.0271, 0.0270, 0.0247, 0.0226, 0.0209, 0.0180, 0.0178, 0.0165,\n",
      "        0.0160, 0.0140, 0.0127, 0.0121, 0.0115, 0.0109, 0.0108, 0.0106, 0.0105,\n",
      "        0.0103, 0.0102, 0.0102, 0.0099, 0.0099, 0.0098, 0.0098, 0.0093, 0.0088,\n",
      "        0.0082, 0.0082, 0.0081, 0.0074, 0.0074, 0.0074, 0.0069, 0.0068, 0.0068,\n",
      "        0.0068, 0.0068, 0.0067, 0.0065, 0.0063], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 2362],\n",
      "        [ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648],\n",
      "        [  282],\n",
      "        [   43],\n",
      "        [  911]], device='cuda:0') possible tokens:  tensor([ 1460,   726,   907,     2,    34,   100,   594,   985,     0,   751,\n",
      "         1356,  2940,   684,  1456,   648,  9971,  4090,   132,  2362,  1518,\n",
      "         3787,   422,    41,   935,    15,  7655,   297,  2504,   160,  3581,\n",
      "         1902,    46,  1461,   289,   125,  2266,  5683,   521,  2195,  1050,\n",
      "          722,  5671, 10356,   244,   425, 13503,    39,   925, 11009,  2063],\n",
      "       device='cuda:0') Pred_data:  tensor(907, device='cuda:0')\n",
      "i: 36\n",
      "next word:  हित\n",
      "36 Values:  tensor([0.0664, 0.0609, 0.0489, 0.0388, 0.0379, 0.0365, 0.0363, 0.0355, 0.0303,\n",
      "        0.0279, 0.0270, 0.0259, 0.0242, 0.0236, 0.0206, 0.0206, 0.0201, 0.0188,\n",
      "        0.0187, 0.0170, 0.0165, 0.0150, 0.0148, 0.0143, 0.0140, 0.0139, 0.0137,\n",
      "        0.0137, 0.0136, 0.0132, 0.0131, 0.0130, 0.0128, 0.0127, 0.0125, 0.0121,\n",
      "        0.0118, 0.0113, 0.0108, 0.0107, 0.0105, 0.0104, 0.0103, 0.0103, 0.0103,\n",
      "        0.0102, 0.0101, 0.0096, 0.0095, 0.0094], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 1670],\n",
      "        [   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648],\n",
      "        [  282],\n",
      "        [   43],\n",
      "        [  911],\n",
      "        [  907]], device='cuda:0') possible tokens:  tensor([ 1599,  7118,   249,     0,  3085,   537,  1437,  1462,    43,  2267,\n",
      "          138,  1460,   594,  2361,  1332,   299,    81,   183,  3413,     5,\n",
      "          499,   861,  2453, 13883,  2939,   233,  1222,   618,  2504,    55,\n",
      "         5612,  1351,  4685,   642,  1219,     9,  1050,   213,   784,   222,\n",
      "         1330,   441,   386,   498,  6049,   169,  1799,  9558,  4798,  1819],\n",
      "       device='cuda:0') Pred_data:  tensor(2453, device='cuda:0')\n",
      "i: 37\n",
      "next word:  गर्ने\n",
      "37 Values:  tensor([0.1748, 0.0891, 0.0876, 0.0826, 0.0765, 0.0453, 0.0407, 0.0395, 0.0355,\n",
      "        0.0352, 0.0283, 0.0226, 0.0212, 0.0209, 0.0178, 0.0166, 0.0132, 0.0125,\n",
      "        0.0099, 0.0079, 0.0075, 0.0067, 0.0066, 0.0064, 0.0057, 0.0057, 0.0054,\n",
      "        0.0050, 0.0049, 0.0046, 0.0043, 0.0042, 0.0041, 0.0037, 0.0036, 0.0034,\n",
      "        0.0034, 0.0032, 0.0032, 0.0032, 0.0030, 0.0030, 0.0030, 0.0029, 0.0029,\n",
      "        0.0029, 0.0027, 0.0025, 0.0025, 0.0024], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   82],\n",
      "        [ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648],\n",
      "        [  282],\n",
      "        [   43],\n",
      "        [  911],\n",
      "        [  907],\n",
      "        [ 2453]], device='cuda:0') possible tokens:  tensor([    3,    13,    10,    20,    30,     5,    17,     2,     6,  2416,\n",
      "          297,   425,   412,  1878,    37,    11,  5188,   272,    78,   133,\n",
      "            8,   262,  2687,   377,   134,   753,    55,   417,   268,   328,\n",
      "           25,   617,  2958,   172,  1643,  2979,  5248,     4,   623,   545,\n",
      "         1928,   231,   600,   258,   557,    12, 11383,   287,     0,   359],\n",
      "       device='cuda:0') Pred_data:  tensor(13, device='cuda:0')\n",
      "i: 38\n",
      "next word:  मात्र\n",
      "38 Values:  tensor([0.1359, 0.0704, 0.0703, 0.0507, 0.0449, 0.0375, 0.0329, 0.0304, 0.0255,\n",
      "        0.0233, 0.0233, 0.0200, 0.0185, 0.0183, 0.0181, 0.0178, 0.0169, 0.0148,\n",
      "        0.0148, 0.0145, 0.0141, 0.0140, 0.0139, 0.0136, 0.0128, 0.0127, 0.0125,\n",
      "        0.0121, 0.0114, 0.0108, 0.0107, 0.0105, 0.0104, 0.0102, 0.0101, 0.0099,\n",
      "        0.0091, 0.0088, 0.0087, 0.0087, 0.0087, 0.0085, 0.0079, 0.0077, 0.0075,\n",
      "        0.0075, 0.0072, 0.0071, 0.0070, 0.0070], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 2883],\n",
      "        [12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648],\n",
      "        [  282],\n",
      "        [   43],\n",
      "        [  911],\n",
      "        [  907],\n",
      "        [ 2453],\n",
      "        [   13]], device='cuda:0') possible tokens:  tensor([   8,   26,  494,  622,    2,  303,   45,    3,   17,  651,  169,  121,\n",
      "        2043,   61,   11, 1325,  165,  618,  529,  105,  859,   43, 1384, 1289,\n",
      "          33, 2159,  313, 1770,    4,  114, 2407, 2295, 2556, 1659,  599,  220,\n",
      "        1217,  163,  117,  197,  407,   25,    5,    0, 3298, 2118,  111,  633,\n",
      "         875,  164], device='cuda:0') Pred_data:  tensor(43, device='cuda:0')\n",
      "i: 39\n",
      "next word:  हुँदैन\n",
      "39 Values:  tensor([0.2555, 0.1465, 0.0988, 0.0799, 0.0577, 0.0436, 0.0288, 0.0259, 0.0147,\n",
      "        0.0142, 0.0132, 0.0127, 0.0125, 0.0124, 0.0120, 0.0110, 0.0105, 0.0098,\n",
      "        0.0091, 0.0090, 0.0082, 0.0081, 0.0062, 0.0062, 0.0061, 0.0057, 0.0056,\n",
      "        0.0054, 0.0054, 0.0048, 0.0044, 0.0039, 0.0038, 0.0038, 0.0037, 0.0036,\n",
      "        0.0033, 0.0033, 0.0033, 0.0030, 0.0028, 0.0028, 0.0025, 0.0025, 0.0025,\n",
      "        0.0024, 0.0024, 0.0023, 0.0022, 0.0022], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[12806],\n",
      "        [    1],\n",
      "        [  726],\n",
      "        [ 2367],\n",
      "        [   49],\n",
      "        [  985],\n",
      "        [   17],\n",
      "        [  648],\n",
      "        [  282],\n",
      "        [   43],\n",
      "        [  911],\n",
      "        [  907],\n",
      "        [ 2453],\n",
      "        [   13],\n",
      "        [   43]], device='cuda:0') possible tokens:  tensor([ 117,    8,  911,  297,    6, 1269,   37, 3316,   20, 6758,   78,   30,\n",
      "         892,  342,  545,    5,    4, 9240, 1247,   35,  172,  335,   10,  651,\n",
      "          68,   26,   57, 1201,   11,   21,  377, 4733,   16,  231,  133,  690,\n",
      "          17,  328,  494,   45,  425,  503,  283, 1846,  878,  417,  267,  376,\n",
      "        2081,   13], device='cuda:0') Pred_data:  tensor(297, device='cuda:0')\n",
      "i: 40\n",
      "next word:  ।\n",
      "40 Values:  tensor([8.1901e-01, 1.1653e-01, 1.5546e-02, 1.4967e-02, 9.9997e-03, 3.0699e-03,\n",
      "        2.5548e-03, 1.9521e-03, 1.8099e-03, 1.5274e-03, 9.0180e-04, 8.2819e-04,\n",
      "        7.9569e-04, 5.7197e-04, 5.3739e-04, 4.3460e-04, 4.2707e-04, 4.1682e-04,\n",
      "        3.9178e-04, 3.8303e-04, 3.7790e-04, 3.5284e-04, 3.3103e-04, 3.2555e-04,\n",
      "        3.1087e-04, 2.9251e-04, 2.8430e-04, 2.7022e-04, 2.6586e-04, 2.6164e-04,\n",
      "        2.6068e-04, 2.5962e-04, 2.4788e-04, 2.3733e-04, 2.3474e-04, 2.2556e-04,\n",
      "        2.2257e-04, 2.1916e-04, 2.1669e-04, 2.1252e-04, 2.1143e-04, 2.0670e-04,\n",
      "        1.9276e-04, 1.8995e-04, 1.8968e-04, 1.8912e-04, 1.8859e-04, 1.8822e-04,\n",
      "        1.8820e-04, 1.8425e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[   1],\n",
      "        [ 726],\n",
      "        [2367],\n",
      "        [  49],\n",
      "        [ 985],\n",
      "        [  17],\n",
      "        [ 648],\n",
      "        [ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297]], device='cuda:0') possible tokens:  tensor([   1,    2,   25,  121,    3,   93,   12,   19,   17,  287,   29, 2248,\n",
      "          15,   42,   38,  141,   59,  907, 1017,  413,    5,  603,  629,  246,\n",
      "        1856,  240,  925,  111,  245, 7306,  929,  105,  680,   34,    4,   16,\n",
      "          90,    8,   18,  492,  212, 4587,  211,  396,    0,  528, 1423,  112,\n",
      "         132, 1622], device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 41\n",
      "next word:  यदि\n",
      "41 Values:  tensor([0.0966, 0.0642, 0.0419, 0.0419, 0.0404, 0.0386, 0.0363, 0.0307, 0.0304,\n",
      "        0.0237, 0.0231, 0.0219, 0.0209, 0.0209, 0.0199, 0.0181, 0.0180, 0.0175,\n",
      "        0.0171, 0.0166, 0.0164, 0.0162, 0.0156, 0.0152, 0.0147, 0.0146, 0.0145,\n",
      "        0.0144, 0.0143, 0.0141, 0.0140, 0.0129, 0.0128, 0.0126, 0.0123, 0.0113,\n",
      "        0.0110, 0.0105, 0.0104, 0.0104, 0.0101, 0.0100, 0.0098, 0.0096, 0.0093,\n",
      "        0.0091, 0.0090, 0.0089, 0.0088, 0.0087], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 726],\n",
      "        [2367],\n",
      "        [  49],\n",
      "        [ 985],\n",
      "        [  17],\n",
      "        [ 648],\n",
      "        [ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([  19,  211,  289,  726,   15,  129,  907,  497,   34, 1017,   82, 1040,\n",
      "        1856, 4994,  152, 1254,  878, 1964,  143, 1640,    0,  110,  751,  715,\n",
      "          41,  111,  160,   54,  120,  266,  594,   59,  552,   90,  680,   46,\n",
      "        2078, 1461, 7266,  222,  148, 1246,   80,  925,   37,   18,   48,  604,\n",
      "          87,  244], device='cuda:0') Pred_data:  tensor(497, device='cuda:0')\n",
      "i: 42\n",
      "next word:  उनीहरूले\n",
      "42 Values:  tensor([0.1524, 0.1007, 0.0496, 0.0490, 0.0392, 0.0380, 0.0356, 0.0328, 0.0321,\n",
      "        0.0314, 0.0303, 0.0233, 0.0223, 0.0218, 0.0209, 0.0185, 0.0173, 0.0168,\n",
      "        0.0158, 0.0153, 0.0141, 0.0129, 0.0128, 0.0126, 0.0125, 0.0123, 0.0104,\n",
      "        0.0087, 0.0086, 0.0084, 0.0076, 0.0076, 0.0074, 0.0074, 0.0073, 0.0069,\n",
      "        0.0067, 0.0067, 0.0061, 0.0061, 0.0060, 0.0060, 0.0059, 0.0056, 0.0054,\n",
      "        0.0052, 0.0052, 0.0049, 0.0048, 0.0047], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[2367],\n",
      "        [  49],\n",
      "        [ 985],\n",
      "        [  17],\n",
      "        [ 648],\n",
      "        [ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497]], device='cuda:0') possible tokens:  tensor([ 318, 1773, 1040, 3800, 1257,  160, 1254,  726, 1887, 2042, 1664,   41,\n",
      "          15,  129,  143,  289, 1047,  907,   34,   59, 2993,  878,   90,    2,\n",
      "         546,  303,  578,    0,   82,  702, 2182,  147, 4090, 9716, 5283, 2287,\n",
      "        1432,  852,  111, 9476, 3449,  550, 6095,  121, 5302,  281,   46,   26,\n",
      "        1045,  120], device='cuda:0') Pred_data:  tensor(852, device='cuda:0')\n",
      "i: 43\n",
      "next word:  आफू\n",
      "43 Values:  tensor([0.2045, 0.0530, 0.0467, 0.0440, 0.0351, 0.0324, 0.0297, 0.0289, 0.0262,\n",
      "        0.0249, 0.0239, 0.0213, 0.0197, 0.0195, 0.0174, 0.0167, 0.0164, 0.0155,\n",
      "        0.0145, 0.0141, 0.0141, 0.0137, 0.0133, 0.0127, 0.0123, 0.0122, 0.0116,\n",
      "        0.0109, 0.0108, 0.0106, 0.0103, 0.0098, 0.0094, 0.0093, 0.0093, 0.0093,\n",
      "        0.0092, 0.0090, 0.0089, 0.0084, 0.0083, 0.0083, 0.0083, 0.0082, 0.0081,\n",
      "        0.0080, 0.0080, 0.0078, 0.0077, 0.0076], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  49],\n",
      "        [ 985],\n",
      "        [  17],\n",
      "        [ 648],\n",
      "        [ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852]], device='cuda:0') possible tokens:  tensor([  34,   17,  530, 8685,  517,    5,  907,   15,  281,  771, 2084,  598,\n",
      "          82,   80,  392,  878, 1461, 1254,  111,  860,  110,  289,  218,  147,\n",
      "          26,  258,   59,   46, 1039,  604,  465,   39,  522,   10, 1559, 3947,\n",
      "         129,  282,  222,  726,  434,  435,  123,  152,   41,  528,  751,   28,\n",
      "        1257,    0], device='cuda:0') Pred_data:  tensor(517, device='cuda:0')\n",
      "i: 44\n",
      "next word:  सचेत\n",
      "44 Values:  tensor([0.1351, 0.0871, 0.0473, 0.0468, 0.0385, 0.0319, 0.0300, 0.0290, 0.0282,\n",
      "        0.0277, 0.0277, 0.0242, 0.0232, 0.0227, 0.0191, 0.0173, 0.0166, 0.0164,\n",
      "        0.0163, 0.0152, 0.0147, 0.0141, 0.0137, 0.0136, 0.0127, 0.0118, 0.0117,\n",
      "        0.0116, 0.0107, 0.0105, 0.0104, 0.0103, 0.0101, 0.0099, 0.0098, 0.0091,\n",
      "        0.0090, 0.0090, 0.0088, 0.0087, 0.0086, 0.0085, 0.0084, 0.0078, 0.0077,\n",
      "        0.0077, 0.0077, 0.0076, 0.0076, 0.0076], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 985],\n",
      "        [  17],\n",
      "        [ 648],\n",
      "        [ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517]], device='cuda:0') possible tokens:  tensor([   17,     3,  3297,     5,  4694,   703,  4686,  2979,     2, 19819,\n",
      "         4205,    34,  1578,  3947,  2010,   582,  1559,  4576,   606,   588,\n",
      "         4172,     0,  1204,  3728,    42,  1254,  1110,  1431,   222,  5754,\n",
      "          774,   138,   994,  3803,   258,  5989,   424,    43,  1333,  5425,\n",
      "          444,  4177,  7535,  4941,  4908, 14051,  2724,  4340,   400,  1759],\n",
      "       device='cuda:0') Pred_data:  tensor(1431, device='cuda:0')\n",
      "i: 45\n",
      "next word:  हुन\n",
      "45 Values:  tensor([0.1514, 0.1198, 0.0797, 0.0665, 0.0608, 0.0493, 0.0401, 0.0372, 0.0293,\n",
      "        0.0219, 0.0210, 0.0194, 0.0185, 0.0165, 0.0139, 0.0130, 0.0125, 0.0123,\n",
      "        0.0111, 0.0106, 0.0097, 0.0097, 0.0092, 0.0088, 0.0083, 0.0083, 0.0081,\n",
      "        0.0079, 0.0079, 0.0079, 0.0077, 0.0077, 0.0075, 0.0074, 0.0074, 0.0062,\n",
      "        0.0061, 0.0057, 0.0052, 0.0049, 0.0049, 0.0048, 0.0048, 0.0044, 0.0044,\n",
      "        0.0043, 0.0042, 0.0041, 0.0040, 0.0040], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  17],\n",
      "        [ 648],\n",
      "        [ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431]], device='cuda:0') possible tokens:  tensor([    6,    37,   174,   376,    21,   764,    17,    20,   545,   328,\n",
      "         6022,   224,   559,   206,  1090,    68,   166,   380,   456,   671,\n",
      "            3,   519,  1596,  3452,   626, 11339,  2963, 13824,   583,   505,\n",
      "          425,  2779,   294,   197,    10,     2,    40,   297,  1269,  7493,\n",
      "         8881,  5406,  2367, 11294,  1665,  7853,  3368,  1157,   493,  2381],\n",
      "       device='cuda:0') Pred_data:  tensor(37, device='cuda:0')\n",
      "i: 46\n",
      "next word:  सक्छन्\n",
      "46 Values:  tensor([0.3210, 0.0913, 0.0559, 0.0390, 0.0329, 0.0296, 0.0242, 0.0231, 0.0215,\n",
      "        0.0210, 0.0193, 0.0170, 0.0164, 0.0164, 0.0160, 0.0145, 0.0138, 0.0125,\n",
      "        0.0118, 0.0117, 0.0117, 0.0112, 0.0100, 0.0100, 0.0090, 0.0087, 0.0087,\n",
      "        0.0083, 0.0082, 0.0067, 0.0066, 0.0062, 0.0061, 0.0054, 0.0054, 0.0054,\n",
      "        0.0053, 0.0052, 0.0050, 0.0050, 0.0049, 0.0045, 0.0045, 0.0044, 0.0043,\n",
      "        0.0043, 0.0043, 0.0040, 0.0039, 0.0038], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 648],\n",
      "        [ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37]], device='cuda:0') possible tokens:  tensor([  655,  2920,   777,   216,   306,  1381,    12,   185,   617,   913,\n",
      "         2084,  3565,  7677,    42,  6056,  3493,     3,  7760,     2,   659,\n",
      "         2132,  8356, 10214,  1827,   474, 16053,   977,  1795,     5,   850,\n",
      "         3314,   828,  8489,  6616,  4771,  9452,  1636, 10101,   862,  6820,\n",
      "          145,  4790,   425,  2046,  3498, 14568,    38,  1531,   979,  2765],\n",
      "       device='cuda:0') Pred_data:  tensor(655, device='cuda:0')\n",
      "i: 47\n",
      "next word:  र\n",
      "47 Values:  tensor([6.0744e-01, 1.6398e-01, 8.7040e-02, 4.2859e-02, 2.1669e-02, 2.0905e-02,\n",
      "        1.2511e-02, 6.1495e-03, 5.6362e-03, 4.3253e-03, 2.7184e-03, 2.6610e-03,\n",
      "        2.4914e-03, 1.8260e-03, 1.7744e-03, 1.7513e-03, 1.0169e-03, 9.7413e-04,\n",
      "        8.5330e-04, 6.0352e-04, 5.5588e-04, 5.5153e-04, 5.3197e-04, 5.2588e-04,\n",
      "        5.0224e-04, 4.9876e-04, 4.5357e-04, 4.4898e-04, 4.1629e-04, 4.0918e-04,\n",
      "        3.7268e-04, 3.7094e-04, 3.6306e-04, 3.4531e-04, 3.2593e-04, 3.2122e-04,\n",
      "        3.1152e-04, 3.1094e-04, 3.0651e-04, 3.0631e-04, 3.0132e-04, 2.9883e-04,\n",
      "        2.8792e-04, 2.8194e-04, 2.6417e-04, 2.5650e-04, 2.3671e-04, 2.3168e-04,\n",
      "        2.1508e-04, 2.1248e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 282],\n",
      "        [  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655]], device='cuda:0') possible tokens:  tensor([   12,     1,     2,     3,    42,   121,    25,   629,    93,    38,\n",
      "         6199,  1066,   643,    19,    17,   141,    59,    34,   243,   318,\n",
      "          497,  2770,     0,   492,  7074,  1257,    37,   603,  1773,  1317,\n",
      "         1017,   246,   828, 12838,  7932,   907,  1040,  4587, 10019,  7306,\n",
      "          303,   759,    15,   528,  2248,   240,   680,   390,  1856,    18],\n",
      "       device='cuda:0') Pred_data:  tensor(3, device='cuda:0')\n",
      "i: 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  त्यस्तो\n",
      "48 Values:  tensor([0.1516, 0.1316, 0.0593, 0.0581, 0.0486, 0.0420, 0.0373, 0.0265, 0.0263,\n",
      "        0.0207, 0.0171, 0.0163, 0.0156, 0.0142, 0.0140, 0.0138, 0.0131, 0.0131,\n",
      "        0.0123, 0.0116, 0.0116, 0.0115, 0.0114, 0.0111, 0.0109, 0.0101, 0.0099,\n",
      "        0.0099, 0.0098, 0.0097, 0.0095, 0.0083, 0.0083, 0.0083, 0.0082, 0.0082,\n",
      "        0.0081, 0.0081, 0.0078, 0.0076, 0.0075, 0.0075, 0.0074, 0.0070, 0.0069,\n",
      "        0.0069, 0.0068, 0.0063, 0.0062, 0.0062], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  43],\n",
      "        [ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3]], device='cuda:0') possible tokens:  tensor([   34,   907,  1461,  1040,   281,   243,     2,   289,  6095,   528,\n",
      "         1253,    59,    15,   985,   578,   392,   517,  2034,   303,  4259,\n",
      "           48,   598,  1308,   929,   497,  1254,   157,   878,   390,  2063,\n",
      "           41,    46,   100,    26,   160,    89,   925,    90,   258,   852,\n",
      "        19270,   102,   266,   530,   648,   167,     0,   434,  2633,   751],\n",
      "       device='cuda:0') Pred_data:  tensor(578, device='cuda:0')\n",
      "i: 49\n",
      "next word:  पनि\n",
      "49 Values:  tensor([0.1055, 0.0842, 0.0637, 0.0507, 0.0505, 0.0372, 0.0345, 0.0229, 0.0225,\n",
      "        0.0218, 0.0218, 0.0209, 0.0191, 0.0185, 0.0179, 0.0172, 0.0170, 0.0170,\n",
      "        0.0168, 0.0162, 0.0158, 0.0150, 0.0145, 0.0142, 0.0135, 0.0135, 0.0132,\n",
      "        0.0131, 0.0122, 0.0118, 0.0118, 0.0114, 0.0112, 0.0112, 0.0111, 0.0103,\n",
      "        0.0096, 0.0096, 0.0094, 0.0091, 0.0089, 0.0089, 0.0086, 0.0085, 0.0085,\n",
      "        0.0082, 0.0082, 0.0076, 0.0076, 0.0074], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 911],\n",
      "        [ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578]], device='cuda:0') possible tokens:  tensor([ 1285,   164,   494,  2082,   163,    26,   303,    37,   481,    10,\n",
      "          114,   465,  1452,   529,  1458,   106,  1163,   380,   275,   222,\n",
      "          985,   599,  4420,   123,   169,   622,   435,  1455,  1310,    45,\n",
      "         3395,  5917,     0,  8690,  1265,  3327,  8960,  1330,  6235,   297,\n",
      "            5,   878,   324,   541,  2103,    41, 12598,   404,   769,   333],\n",
      "       device='cuda:0') Pred_data:  tensor(5, device='cuda:0')\n",
      "i: 50\n",
      "next word:  हुँदैन\n",
      "50 Values:  tensor([0.1872, 0.1067, 0.0484, 0.0404, 0.0355, 0.0296, 0.0274, 0.0262, 0.0261,\n",
      "        0.0251, 0.0247, 0.0245, 0.0217, 0.0209, 0.0199, 0.0166, 0.0133, 0.0130,\n",
      "        0.0124, 0.0124, 0.0118, 0.0115, 0.0114, 0.0113, 0.0112, 0.0110, 0.0110,\n",
      "        0.0108, 0.0105, 0.0097, 0.0095, 0.0094, 0.0094, 0.0087, 0.0087, 0.0087,\n",
      "        0.0086, 0.0085, 0.0082, 0.0081, 0.0081, 0.0078, 0.0077, 0.0076, 0.0073,\n",
      "        0.0072, 0.0062, 0.0061, 0.0060, 0.0059], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 907],\n",
      "        [2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5]], device='cuda:0') possible tokens:  tensor([  10,  297,   37,  907,    7,   26,   35,  655,  133,  106,  377,  222,\n",
      "        1401,  117,  417,   61,   39,   65,  111,    4,  425,   13,  303, 4733,\n",
      "          23,   11,  185,   92,  985,  541,  134,  252,   34,    8,   45,  114,\n",
      "         283,  510, 1827,  335,    6,  590,  545,  892,   30, 4476,  435,   20,\n",
      "        9679,   86], device='cuda:0') Pred_data:  tensor(297, device='cuda:0')\n",
      "i: 51\n",
      "next word:  ।\n",
      "51 Values:  tensor([7.6971e-01, 4.8640e-02, 4.0640e-02, 3.0339e-02, 2.6139e-02, 2.4131e-02,\n",
      "        1.9146e-02, 5.7373e-03, 2.6364e-03, 2.5101e-03, 2.0825e-03, 2.0245e-03,\n",
      "        2.0061e-03, 1.9186e-03, 1.6812e-03, 1.5356e-03, 1.4816e-03, 1.2674e-03,\n",
      "        9.6664e-04, 9.1789e-04, 7.6963e-04, 7.2072e-04, 7.1645e-04, 7.0907e-04,\n",
      "        7.0335e-04, 6.8240e-04, 6.7026e-04, 6.6052e-04, 6.4401e-04, 6.0814e-04,\n",
      "        5.8242e-04, 5.5999e-04, 5.4672e-04, 5.1835e-04, 4.7098e-04, 4.4189e-04,\n",
      "        3.9356e-04, 3.8602e-04, 3.7182e-04, 3.5987e-04, 3.3640e-04, 3.3632e-04,\n",
      "        3.1550e-04, 3.0417e-04, 2.9832e-04, 2.9819e-04, 2.8515e-04, 2.7191e-04,\n",
      "        2.6552e-04, 2.6072e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[2453],\n",
      "        [  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297]], device='cuda:0') possible tokens:  tensor([    1,     2,    25,    12,     3,    93,   121,   629,   287,    42,\n",
      "           17,   141,    19,  2248,   828,   759,  1017,  7306,   603,   246,\n",
      "         1622,  4679,    97,   955,    59,    29,   413,    16,  1856,  2437,\n",
      "          240,   245,  4587,     4,    38,    15,    24,  1066,     8,   396,\n",
      "         2532,  3375,   211,  2207,   217,   212, 12838,    34,   490,   643],\n",
      "       device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 52\n",
      "next word:  यस्तो\n",
      "52 Values:  tensor([0.0727, 0.0666, 0.0594, 0.0534, 0.0433, 0.0424, 0.0414, 0.0391, 0.0370,\n",
      "        0.0320, 0.0271, 0.0262, 0.0254, 0.0237, 0.0231, 0.0213, 0.0184, 0.0176,\n",
      "        0.0176, 0.0170, 0.0158, 0.0149, 0.0130, 0.0126, 0.0113, 0.0112, 0.0109,\n",
      "        0.0106, 0.0106, 0.0106, 0.0104, 0.0101, 0.0100, 0.0098, 0.0097, 0.0094,\n",
      "        0.0093, 0.0090, 0.0088, 0.0087, 0.0083, 0.0083, 0.0083, 0.0081, 0.0078,\n",
      "        0.0077, 0.0077, 0.0075, 0.0075, 0.0074], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  13],\n",
      "        [  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([   19,  1040,  1254,   289,   129,   211,    34,   497,    41,   907,\n",
      "         1017,  1773,  1257, 19270,    82,  4994,   281,  1047,    15,    80,\n",
      "          738,  1856,   985,   143,   111,    46,   222,   314,  1664,   598,\n",
      "          370,   415,   578,  1640,   181,     0,    87,   266,    39,  2078,\n",
      "         1063,    18,  3926,    89,   155,   453,   604,  1525,  1291,    37],\n",
      "       device='cuda:0') Pred_data:  tensor(129, device='cuda:0')\n",
      "i: 53\n",
      "next word:  अवस्थामा\n",
      "53 Values:  tensor([0.4694, 0.0591, 0.0449, 0.0338, 0.0263, 0.0245, 0.0242, 0.0191, 0.0147,\n",
      "        0.0139, 0.0126, 0.0109, 0.0099, 0.0096, 0.0094, 0.0094, 0.0091, 0.0091,\n",
      "        0.0089, 0.0088, 0.0088, 0.0084, 0.0073, 0.0073, 0.0073, 0.0072, 0.0070,\n",
      "        0.0068, 0.0062, 0.0060, 0.0059, 0.0058, 0.0058, 0.0055, 0.0053, 0.0052,\n",
      "        0.0051, 0.0050, 0.0050, 0.0049, 0.0049, 0.0048, 0.0048, 0.0047, 0.0047,\n",
      "        0.0046, 0.0046, 0.0046, 0.0046, 0.0045], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  43],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129]], device='cuda:0') possible tokens:  tensor([  164,   114,  4832,   169,  4420,   163,   106,    26,   846,    45,\n",
      "           37,  2966,  6235,  1310,   202,  2043,  1458,  1620,  2103, 12598,\n",
      "         3810,  1003,   275,   179,   541,   575,  1382,  1453,  1632,   599,\n",
      "         1285,   246,  1157,  2989,     0,   756,   303,  1163,  2360,   222,\n",
      "         8960,    92,   859,  8690,  2082,  1862,  1819,  1360,  6912,   300],\n",
      "       device='cuda:0') Pred_data:  tensor(164, device='cuda:0')\n",
      "i: 54\n",
      "next word:  हामी\n",
      "54 Values:  tensor([0.0832, 0.0632, 0.0577, 0.0486, 0.0411, 0.0395, 0.0385, 0.0385, 0.0349,\n",
      "        0.0276, 0.0273, 0.0255, 0.0226, 0.0225, 0.0197, 0.0173, 0.0168, 0.0167,\n",
      "        0.0161, 0.0157, 0.0155, 0.0151, 0.0147, 0.0135, 0.0132, 0.0132, 0.0129,\n",
      "        0.0125, 0.0124, 0.0120, 0.0120, 0.0112, 0.0112, 0.0111, 0.0107, 0.0103,\n",
      "        0.0102, 0.0102, 0.0100, 0.0098, 0.0097, 0.0096, 0.0094, 0.0093, 0.0083,\n",
      "        0.0082, 0.0077, 0.0077, 0.0076, 0.0075], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 297],\n",
      "        [   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164]], device='cuda:0') possible tokens:  tensor([ 1773,    34,    41,  1040,  1257,    80,     2,   281,   985,     5,\n",
      "          907,    17,  2182,   318,    39,   415,   143,   578,    90,    82,\n",
      "           42,     0,   738,  1254,   289, 19270,    46,    18,   111,   129,\n",
      "          160,   100,   497,    51,   222,   689,  2911,    15,  3567,    37,\n",
      "         1664,    38,    43,   891,   925,  1461,   598,    26,   258,    20],\n",
      "       device='cuda:0') Pred_data:  tensor(90, device='cuda:0')\n",
      "i: 55\n",
      "next word:  धेरै\n",
      "55 Values:  tensor([0.1118, 0.0740, 0.0625, 0.0473, 0.0452, 0.0407, 0.0301, 0.0288, 0.0240,\n",
      "        0.0240, 0.0236, 0.0225, 0.0213, 0.0208, 0.0183, 0.0179, 0.0166, 0.0164,\n",
      "        0.0162, 0.0161, 0.0154, 0.0147, 0.0144, 0.0142, 0.0138, 0.0135, 0.0131,\n",
      "        0.0129, 0.0120, 0.0118, 0.0111, 0.0111, 0.0108, 0.0103, 0.0102, 0.0102,\n",
      "        0.0102, 0.0101, 0.0100, 0.0098, 0.0096, 0.0094, 0.0085, 0.0081, 0.0080,\n",
      "        0.0079, 0.0079, 0.0077, 0.0077, 0.0076], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   1],\n",
      "        [ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90]], device='cuda:0') possible tokens:  tensor([  34, 2182, 1578, 8685, 1040,   46,    5,  907, 1580,  281,   17, 1254,\n",
      "        1431, 2724, 4686,  243, 1559,  258, 1257, 1461,  916,   51,  381, 3728,\n",
      "        2010,   80,  802,   41,   18,  771, 1333,  738,  528,   15,  289,  795,\n",
      "         517,   39,  703,  214,  129,  303, 4415,  215, 5989,  530, 1029,  111,\n",
      "          59,  135], device='cuda:0') Pred_data:  tensor(51, device='cuda:0')\n",
      "i: 56\n",
      "next word:  छौं\n",
      "56 Values:  tensor([0.1409, 0.0411, 0.0387, 0.0376, 0.0371, 0.0371, 0.0358, 0.0337, 0.0325,\n",
      "        0.0298, 0.0293, 0.0238, 0.0231, 0.0193, 0.0190, 0.0178, 0.0177, 0.0176,\n",
      "        0.0175, 0.0165, 0.0163, 0.0159, 0.0156, 0.0139, 0.0135, 0.0133, 0.0129,\n",
      "        0.0127, 0.0126, 0.0125, 0.0122, 0.0119, 0.0119, 0.0108, 0.0104, 0.0103,\n",
      "        0.0097, 0.0096, 0.0096, 0.0096, 0.0094, 0.0092, 0.0091, 0.0090, 0.0089,\n",
      "        0.0088, 0.0088, 0.0087, 0.0085, 0.0084], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 497],\n",
      "        [ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51]], device='cuda:0') possible tokens:  tensor([  17, 3174,  209, 1333,  615, 6212, 4686,   45, 1431, 1366, 4245, 1759,\n",
      "        3599,  106,  635,  303, 3687, 1781,  317,  859,  134,  762, 5500,  582,\n",
      "          28,  380, 2301,  125, 8907, 4480, 3745, 2381,  213,    5, 7617, 1416,\n",
      "          51, 4713,  608, 5012,  726,  976,   95, 2010, 3787, 1665,  575,    7,\n",
      "        5768, 1071], device='cuda:0') Pred_data:  tensor(317, device='cuda:0')\n",
      "i: 57\n",
      "next word:  ।\n",
      "57 Values:  tensor([7.8268e-01, 7.7485e-02, 2.8737e-02, 2.7265e-02, 1.9997e-02, 1.5900e-02,\n",
      "        9.3741e-03, 4.3381e-03, 2.6568e-03, 2.2415e-03, 2.1022e-03, 1.9092e-03,\n",
      "        1.8435e-03, 1.7753e-03, 1.5685e-03, 1.3097e-03, 1.0916e-03, 1.0888e-03,\n",
      "        9.4922e-04, 9.2057e-04, 8.9412e-04, 7.7213e-04, 7.1189e-04, 6.3717e-04,\n",
      "        6.1601e-04, 5.9605e-04, 5.7392e-04, 5.6733e-04, 5.2042e-04, 5.1204e-04,\n",
      "        5.1028e-04, 4.9583e-04, 4.9463e-04, 4.7471e-04, 4.4702e-04, 4.3966e-04,\n",
      "        4.3482e-04, 4.3276e-04, 4.3173e-04, 4.2960e-04, 4.1516e-04, 4.1213e-04,\n",
      "        4.0381e-04, 3.9510e-04, 3.7385e-04, 3.6792e-04, 3.6608e-04, 3.6063e-04,\n",
      "        3.5508e-04, 3.2365e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 852],\n",
      "        [ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317]], device='cuda:0') possible tokens:  tensor([    1,     2,    12,   121,    93,     3,    25,    19,   287,   603,\n",
      "          245,   828,    42,   246,   413,   141,  1017,     5,    17,   240,\n",
      "          629,  4679,   907,    59,   111,    29,   759,    90,    15,   143,\n",
      "         1856,   680, 12838,  7932,   492,  7306,  1622,  1423,  6199,    38,\n",
      "          810,    95,    18,  2248,  2532,    97,   318,    34,   211,    46],\n",
      "       device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 58\n",
      "next word:  यस्तो\n",
      "58 Values:  tensor([0.1065, 0.0567, 0.0459, 0.0449, 0.0391, 0.0375, 0.0370, 0.0361, 0.0329,\n",
      "        0.0260, 0.0258, 0.0242, 0.0237, 0.0209, 0.0208, 0.0201, 0.0196, 0.0182,\n",
      "        0.0166, 0.0159, 0.0152, 0.0146, 0.0144, 0.0139, 0.0138, 0.0131, 0.0128,\n",
      "        0.0126, 0.0126, 0.0125, 0.0122, 0.0119, 0.0112, 0.0107, 0.0106, 0.0106,\n",
      "        0.0106, 0.0101, 0.0100, 0.0096, 0.0092, 0.0091, 0.0091, 0.0091, 0.0090,\n",
      "        0.0090, 0.0087, 0.0086, 0.0084, 0.0083], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 517],\n",
      "        [1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([  19,  143,  211,  111,  497,   90, 1773, 1017, 1254,  289, 1040,  129,\n",
      "          82, 1257, 4994,   15,   34, 1640,  738, 1856,  604,   80,  318,   41,\n",
      "         102,  907, 1063,   87,  155,   46,  266, 2078,  985,   39, 1047,   59,\n",
      "          18, 3926,   54,  314, 1525,  415, 1664,   51,   37, 3120,  726,   48,\n",
      "         110,  181], device='cuda:0') Pred_data:  tensor(129, device='cuda:0')\n",
      "i: 59\n",
      "next word:  प्रवृत्ति\n",
      "59 Values:  tensor([0.4965, 0.0815, 0.0375, 0.0330, 0.0309, 0.0212, 0.0206, 0.0164, 0.0163,\n",
      "        0.0117, 0.0103, 0.0100, 0.0100, 0.0096, 0.0096, 0.0088, 0.0088, 0.0087,\n",
      "        0.0084, 0.0076, 0.0073, 0.0066, 0.0064, 0.0064, 0.0059, 0.0057, 0.0054,\n",
      "        0.0052, 0.0051, 0.0050, 0.0049, 0.0049, 0.0047, 0.0046, 0.0045, 0.0045,\n",
      "        0.0044, 0.0043, 0.0042, 0.0041, 0.0040, 0.0040, 0.0039, 0.0039, 0.0039,\n",
      "        0.0039, 0.0038, 0.0037, 0.0037, 0.0037], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[1431],\n",
      "        [  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129]], device='cuda:0') possible tokens:  tensor([  164,   169,  4420,   114,  2966,  4832,   106,    37,   163,    45,\n",
      "          202,    26,   846,  1453,  3810,  2043,  1632,   859,  6235,   575,\n",
      "         1382,  1620,   179,  7037,   246,  2360,  2989,  1261,  8690,    10,\n",
      "         1157,   275,     5,   756,  1310,  3397,     4,  1360,  1217,    38,\n",
      "        12598,    46,    17,  1659,  1458,   590,   303,     0,  1003,    92],\n",
      "       device='cuda:0') Pred_data:  tensor(2043, device='cuda:0')\n",
      "i: 60\n",
      "next word:  अब\n",
      "60 Values:  tensor([0.1228, 0.1087, 0.1076, 0.0534, 0.0433, 0.0400, 0.0258, 0.0248, 0.0248,\n",
      "        0.0212, 0.0211, 0.0202, 0.0172, 0.0170, 0.0167, 0.0164, 0.0163, 0.0156,\n",
      "        0.0154, 0.0138, 0.0126, 0.0123, 0.0123, 0.0116, 0.0113, 0.0112, 0.0107,\n",
      "        0.0098, 0.0098, 0.0093, 0.0090, 0.0090, 0.0087, 0.0084, 0.0084, 0.0083,\n",
      "        0.0079, 0.0077, 0.0077, 0.0077, 0.0075, 0.0073, 0.0070, 0.0064, 0.0063,\n",
      "        0.0062, 0.0059, 0.0059, 0.0058, 0.0058], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  37],\n",
      "        [ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043]], device='cuda:0') possible tokens:  tensor([    3,     2,  1408,     5,    90,   793,   801,   168,   876,   111,\n",
      "         4700,   300,   325,   331,    38,    80,  2268,   381, 28096,   120,\n",
      "        11079,    17,   215,  1373,   689,   604,   102,  2762,   254,   110,\n",
      "           54,   246,   258,    39,     0,    40,   125,   742,  4086,   663,\n",
      "            4,    34,  1580,   143,   400,    51,   688,   129,  4027,  2387],\n",
      "       device='cuda:0') Pred_data:  tensor(102, device='cuda:0')\n",
      "i: 61\n",
      "next word:  लागू\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Values:  tensor([0.0727, 0.0687, 0.0416, 0.0415, 0.0393, 0.0324, 0.0317, 0.0309, 0.0299,\n",
      "        0.0297, 0.0277, 0.0266, 0.0234, 0.0233, 0.0229, 0.0213, 0.0205, 0.0198,\n",
      "        0.0197, 0.0189, 0.0181, 0.0174, 0.0170, 0.0167, 0.0152, 0.0143, 0.0143,\n",
      "        0.0141, 0.0133, 0.0132, 0.0132, 0.0123, 0.0122, 0.0113, 0.0113, 0.0110,\n",
      "        0.0108, 0.0104, 0.0099, 0.0098, 0.0098, 0.0097, 0.0095, 0.0092, 0.0092,\n",
      "        0.0091, 0.0090, 0.0087, 0.0087, 0.0086], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 655],\n",
      "        [   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102]], device='cuda:0') possible tokens:  tensor([  80,   90,  258,  111,    5, 2213,   59,  143,  381,  646,   39,   34,\n",
      "         689,   38,  318,  168,   12,  562,   15,   51,  243,   46,   20,  489,\n",
      "        2066,   41,  300, 1773,  752,  801, 1257,  400, 2387,   30, 2042,   67,\n",
      "         134, 2182, 2635,  528,  662,  129,  615,   82,  742, 1664,  297,  125,\n",
      "        1040,   18], device='cuda:0') Pred_data:  tensor(646, device='cuda:0')\n",
      "i: 62\n",
      "next word:  होला\n",
      "62 Values:  tensor([0.1970, 0.1542, 0.1132, 0.1051, 0.0460, 0.0323, 0.0309, 0.0290, 0.0225,\n",
      "        0.0222, 0.0158, 0.0150, 0.0141, 0.0128, 0.0126, 0.0108, 0.0105, 0.0098,\n",
      "        0.0097, 0.0088, 0.0076, 0.0074, 0.0067, 0.0063, 0.0062, 0.0058, 0.0055,\n",
      "        0.0055, 0.0048, 0.0048, 0.0047, 0.0045, 0.0044, 0.0043, 0.0040, 0.0039,\n",
      "        0.0038, 0.0037, 0.0037, 0.0035, 0.0034, 0.0032, 0.0028, 0.0028, 0.0027,\n",
      "        0.0025, 0.0024, 0.0024, 0.0023, 0.0022], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   3],\n",
      "        [ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646]], device='cuda:0') possible tokens:  tensor([   30,    10,     6,    20,    37,    13,  2428,   224,   297,   262,\n",
      "          172,   545,    40,    27,   417,   133,   514,    78,  3452,   559,\n",
      "          219,  4250,   225,   174,   287,  1663, 11202,    11,  8604,  2246,\n",
      "          376,   798,  5937,  7372,  5406,   899,   786,     4,    17,  1157,\n",
      "         4119,   425,  2867,    56,   764,  5044,    68,   278,  5188,    85],\n",
      "       device='cuda:0') Pred_data:  tensor(287, device='cuda:0')\n",
      "i: 63\n",
      "next word:  ।\n",
      "63 Values:  tensor([0.5364, 0.1495, 0.1241, 0.0265, 0.0256, 0.0236, 0.0200, 0.0190, 0.0055,\n",
      "        0.0053, 0.0051, 0.0050, 0.0047, 0.0039, 0.0038, 0.0030, 0.0030, 0.0025,\n",
      "        0.0024, 0.0022, 0.0019, 0.0018, 0.0015, 0.0015, 0.0014, 0.0013, 0.0013,\n",
      "        0.0012, 0.0012, 0.0012, 0.0011, 0.0011, 0.0009, 0.0009, 0.0009, 0.0008,\n",
      "        0.0008, 0.0008, 0.0008, 0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006,\n",
      "        0.0006, 0.0006, 0.0006, 0.0006, 0.0005], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 578],\n",
      "        [   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287]], device='cuda:0') possible tokens:  tensor([   1,  121,    2,   93,    3,   12,   25,   19,   38,  810,  240,   42,\n",
      "         287,   59,  413,  643,   15,  603, 1017, 6199,  629, 1423,  245, 1066,\n",
      "         141,  111, 7306, 1773,   95,   90,   80, 1040, 1035,  243,  492, 5820,\n",
      "         246,  528,   34, 2248, 3375,   17,   22,  497,  318,  143,   35,  160,\n",
      "          48,   82], device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 64\n",
      "next word:  हामी\n",
      "64 Values:  tensor([0.1871, 0.0413, 0.0402, 0.0369, 0.0348, 0.0340, 0.0294, 0.0290, 0.0286,\n",
      "        0.0265, 0.0258, 0.0213, 0.0203, 0.0200, 0.0198, 0.0188, 0.0182, 0.0181,\n",
      "        0.0176, 0.0147, 0.0141, 0.0132, 0.0131, 0.0128, 0.0127, 0.0125, 0.0123,\n",
      "        0.0120, 0.0118, 0.0118, 0.0114, 0.0112, 0.0109, 0.0108, 0.0107, 0.0104,\n",
      "        0.0100, 0.0098, 0.0093, 0.0093, 0.0093, 0.0091, 0.0091, 0.0091, 0.0087,\n",
      "        0.0086, 0.0085, 0.0084, 0.0083, 0.0083], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   5],\n",
      "        [ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([  19,  143,  211,  111,   90, 1017, 1254,   15,  738,  497,  102,  289,\n",
      "         129, 1040, 1773,  266,   59,   82,   41,   39,  604,   51,   80,   67,\n",
      "          46,   54, 1856,   34, 2078,  155,  415, 3926,  120,  715,  157,   18,\n",
      "         110, 1063,  222,   48,  123,   37, 1047,  742,  318,   22,  234,   50,\n",
      "         398,  985], device='cuda:0') Pred_data:  tensor(90, device='cuda:0')\n",
      "i: 65\n",
      "next word:  तपाईंलाई\n",
      "65 Values:  tensor([0.0956, 0.0567, 0.0450, 0.0440, 0.0430, 0.0424, 0.0369, 0.0310, 0.0271,\n",
      "        0.0237, 0.0220, 0.0214, 0.0198, 0.0195, 0.0183, 0.0181, 0.0176, 0.0169,\n",
      "        0.0165, 0.0156, 0.0155, 0.0150, 0.0150, 0.0145, 0.0144, 0.0144, 0.0140,\n",
      "        0.0140, 0.0138, 0.0135, 0.0134, 0.0131, 0.0128, 0.0127, 0.0125, 0.0123,\n",
      "        0.0123, 0.0118, 0.0116, 0.0111, 0.0110, 0.0107, 0.0104, 0.0103, 0.0102,\n",
      "        0.0101, 0.0099, 0.0096, 0.0096, 0.0092], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 297],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90]], device='cuda:0') possible tokens:  tensor([  46, 2182, 1040,    5, 1578,   34,   51, 2724, 1580,  381,  738,   38,\n",
      "         102,   17,   54,  771,   18,  802,  916, 2992, 6149, 1029, 4480,  528,\n",
      "         243,  111,   39,   15,   80, 1254,  214,  489,  795,  258,   50,   59,\n",
      "        3728,  604, 4415,   81, 2213,  289, 1257, 2042, 1781,   41,  731,  742,\n",
      "        1290,  129], device='cuda:0') Pred_data:  tensor(2182, device='cuda:0')\n",
      "i: 66\n",
      "next word:  थाहा\n",
      "66 Values:  tensor([0.1220, 0.0840, 0.0811, 0.0704, 0.0377, 0.0348, 0.0345, 0.0304, 0.0250,\n",
      "        0.0220, 0.0217, 0.0213, 0.0193, 0.0192, 0.0184, 0.0167, 0.0158, 0.0146,\n",
      "        0.0144, 0.0132, 0.0128, 0.0126, 0.0125, 0.0119, 0.0117, 0.0116, 0.0116,\n",
      "        0.0115, 0.0114, 0.0108, 0.0106, 0.0103, 0.0097, 0.0096, 0.0095, 0.0089,\n",
      "        0.0089, 0.0089, 0.0088, 0.0087, 0.0081, 0.0076, 0.0075, 0.0075, 0.0072,\n",
      "        0.0072, 0.0070, 0.0067, 0.0065, 0.0062], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   1],\n",
      "        [ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182]], device='cuda:0') possible tokens:  tensor([  34,   80,  689,    5,   17,   18,   81,   82,  258,  125,  575,  235,\n",
      "          41,   15,  283,  111,  435,  214,  123,  365,  738,  381,  615,  415,\n",
      "        1431,   59, 1333,  134,   38,   39,  976,  860, 1559, 1180,   50, 3856,\n",
      "          51,   46, 1441, 2010,  147,  129, 4415,  346,  360,  483,  614,  106,\n",
      "         100,  562], device='cuda:0') Pred_data:  tensor(283, device='cuda:0')\n",
      "i: 67\n",
      "next word:  छ\n",
      "67 Values:  tensor([0.3223, 0.2729, 0.1099, 0.0600, 0.0326, 0.0172, 0.0143, 0.0116, 0.0100,\n",
      "        0.0094, 0.0086, 0.0080, 0.0079, 0.0071, 0.0069, 0.0064, 0.0063, 0.0056,\n",
      "        0.0051, 0.0049, 0.0048, 0.0045, 0.0044, 0.0038, 0.0036, 0.0035, 0.0034,\n",
      "        0.0033, 0.0032, 0.0026, 0.0025, 0.0022, 0.0022, 0.0021, 0.0021, 0.0020,\n",
      "        0.0019, 0.0018, 0.0018, 0.0018, 0.0017, 0.0017, 0.0016, 0.0016, 0.0016,\n",
      "        0.0015, 0.0015, 0.0015, 0.0015, 0.0014], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 129],\n",
      "        [ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283]], device='cuda:0') possible tokens:  tensor([   35,     4,    30,    17,     6,   172,   545,    20,  4461,    78,\n",
      "           37,  2367,   690, 16278,   297,   328,   695, 48247,  4250,   287,\n",
      "         2650,  2428,    68,    16,  3368, 12834,   342,  2612,   376,   786,\n",
      "          306,  3849,     0,    40,   224,  7678,    10,  6728, 10613,     5,\n",
      "          425, 15243,  1712,  1317,  4367, 13274,     7,   174,   764,  5147],\n",
      "       device='cuda:0') Pred_data:  tensor(4, device='cuda:0')\n",
      "i: 68\n",
      "next word:  ।\n",
      "68 Values:  tensor([0.4075, 0.2306, 0.1729, 0.0729, 0.0131, 0.0107, 0.0086, 0.0082, 0.0058,\n",
      "        0.0057, 0.0052, 0.0048, 0.0039, 0.0031, 0.0030, 0.0028, 0.0027, 0.0021,\n",
      "        0.0021, 0.0021, 0.0019, 0.0019, 0.0017, 0.0017, 0.0016, 0.0016, 0.0015,\n",
      "        0.0014, 0.0014, 0.0013, 0.0013, 0.0012, 0.0012, 0.0011, 0.0010, 0.0010,\n",
      "        0.0008, 0.0008, 0.0008, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0006,\n",
      "        0.0006, 0.0006, 0.0006, 0.0006, 0.0006], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 164],\n",
      "        [  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4]], device='cuda:0') possible tokens:  tensor([ 121,    1,   12,    2,    3, 1423,   38,  318,   42,   93,  810,   19,\n",
      "        1773,   17,   25,  643,  820, 1257,   35,   34,  287,   90,   59, 1017,\n",
      "         143,  492,   46, 6199,  111,  497,   15, 2182,   80,  680,   95, 1085,\n",
      "         522,  245,   18,  160,  246,  141,   41,    0,  413,   82,  907,   51,\n",
      "         258,  490], device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 69\n",
      "next word:  सबै\n",
      "69 Values:  tensor([0.0966, 0.0922, 0.0591, 0.0541, 0.0476, 0.0428, 0.0425, 0.0367, 0.0351,\n",
      "        0.0341, 0.0227, 0.0210, 0.0203, 0.0202, 0.0193, 0.0170, 0.0153, 0.0145,\n",
      "        0.0142, 0.0139, 0.0137, 0.0133, 0.0131, 0.0125, 0.0119, 0.0118, 0.0117,\n",
      "        0.0110, 0.0106, 0.0106, 0.0101, 0.0100, 0.0094, 0.0093, 0.0090, 0.0083,\n",
      "        0.0082, 0.0082, 0.0081, 0.0080, 0.0078, 0.0078, 0.0077, 0.0076, 0.0071,\n",
      "        0.0071, 0.0071, 0.0067, 0.0066, 0.0066], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  90],\n",
      "        [  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([1773,   19,  143, 1257,  318,   90,  211,  111, 1017,  497,  738,   80,\n",
      "         102,   82, 1254,   34,   15,   41,  604,  289, 2182, 2993, 1664,  129,\n",
      "          46, 1040,   18, 3926,   51,   39,  155,   59, 2042,  985, 3635, 3120,\n",
      "        1640,  314,  141,   37,  742, 2078,  266, 1856,    0,  680, 1047,  100,\n",
      "         160,  381], device='cuda:0') Pred_data:  tensor(46, device='cuda:0')\n",
      "i: 70\n",
      "next word:  पुरुष\n",
      "70 Values:  tensor([0.0896, 0.0524, 0.0515, 0.0406, 0.0318, 0.0290, 0.0267, 0.0267, 0.0255,\n",
      "        0.0254, 0.0253, 0.0241, 0.0216, 0.0211, 0.0210, 0.0206, 0.0205, 0.0204,\n",
      "        0.0195, 0.0181, 0.0173, 0.0171, 0.0168, 0.0166, 0.0165, 0.0164, 0.0163,\n",
      "        0.0161, 0.0156, 0.0156, 0.0150, 0.0143, 0.0138, 0.0134, 0.0134, 0.0121,\n",
      "        0.0117, 0.0116, 0.0114, 0.0105, 0.0104, 0.0103, 0.0099, 0.0098, 0.0097,\n",
      "        0.0097, 0.0095, 0.0095, 0.0091, 0.0091], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  51],\n",
      "        [ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46]], device='cuda:0') possible tokens:  tensor([  45,   95, 1665,  859,  303, 1290,  333, 3766,   26,  123,  106, 2018,\n",
      "         423,  599,  380, 7758,  738, 2381,   91, 1632,    0,  575,  726, 1957,\n",
      "        1380,   46,  295,  762, 3439,  100,   70,  132, 2989, 5118, 1532, 1781,\n",
      "        6642,  177,  122, 1005,   65, 1310,  111, 2295,  226, 1414, 2156,  221,\n",
      "          18,  415], device='cuda:0') Pred_data:  tensor(726, device='cuda:0')\n",
      "i: 71\n",
      "next word:  मात्रै\n",
      "71 Values:  tensor([0.1155, 0.0854, 0.0837, 0.0827, 0.0667, 0.0660, 0.0382, 0.0371, 0.0348,\n",
      "        0.0236, 0.0229, 0.0192, 0.0178, 0.0162, 0.0159, 0.0152, 0.0127, 0.0126,\n",
      "        0.0116, 0.0114, 0.0106, 0.0098, 0.0097, 0.0097, 0.0093, 0.0091, 0.0086,\n",
      "        0.0082, 0.0081, 0.0072, 0.0071, 0.0068, 0.0067, 0.0066, 0.0066, 0.0066,\n",
      "        0.0065, 0.0064, 0.0063, 0.0061, 0.0060, 0.0058, 0.0058, 0.0057, 0.0055,\n",
      "        0.0053, 0.0053, 0.0052, 0.0051, 0.0050], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 317],\n",
      "        [   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726]], device='cuda:0') possible tokens:  tensor([  17,    3,   42,   14,    2, 2301, 1110, 1594, 1132, 4340,   43,  555,\n",
      "         635,  726,  376, 4100,  138,  174,   46,    7,  141,  380, 4190, 4941,\n",
      "        3452,  614,   34,    1,   95,  923, 2367, 5989,  344, 8907,   18, 1905,\n",
      "          37, 1115, 5213,  643,   32, 1665, 9301,  492,  545,  566,    5, 1302,\n",
      "        1366,  815], device='cuda:0') Pred_data:  tensor(138, device='cuda:0')\n",
      "i: 72\n",
      "next word:  राम्रो\n",
      "72 Values:  tensor([0.0674, 0.0559, 0.0549, 0.0541, 0.0368, 0.0357, 0.0341, 0.0341, 0.0328,\n",
      "        0.0276, 0.0265, 0.0214, 0.0208, 0.0177, 0.0177, 0.0176, 0.0176, 0.0168,\n",
      "        0.0167, 0.0165, 0.0163, 0.0158, 0.0153, 0.0151, 0.0149, 0.0148, 0.0144,\n",
      "        0.0143, 0.0139, 0.0139, 0.0137, 0.0132, 0.0129, 0.0126, 0.0124, 0.0121,\n",
      "        0.0119, 0.0117, 0.0116, 0.0115, 0.0112, 0.0108, 0.0108, 0.0108, 0.0108,\n",
      "        0.0106, 0.0103, 0.0102, 0.0098, 0.0094], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   1],\n",
      "        [ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 138]], device='cuda:0') possible tokens:  tensor([ 2301,   726,   117,     7,   318,  4733,  3316,    34,     1,     8,\n",
      "          134,  1132,  5547,   132,  5213,   111,    80,    57,     2,   303,\n",
      "          300,   335,    18,  1247,   911,    20, 14098,  1445,   129,    46,\n",
      "          907,  1257,   347,    90,   753,  2010,  4340,   545,  1040,    82,\n",
      "            5,  1758,  2081,   283,  1139,  1269,    37,   479,   380,    24],\n",
      "       device='cuda:0') Pred_data:  tensor(134, device='cuda:0')\n",
      "i: 73\n",
      "next word:  हुनेछ\n",
      "73 Values:  tensor([0.1293, 0.1195, 0.0811, 0.0574, 0.0535, 0.0447, 0.0364, 0.0305, 0.0254,\n",
      "        0.0222, 0.0214, 0.0206, 0.0198, 0.0186, 0.0184, 0.0166, 0.0166, 0.0163,\n",
      "        0.0149, 0.0143, 0.0142, 0.0132, 0.0130, 0.0122, 0.0119, 0.0103, 0.0100,\n",
      "        0.0083, 0.0083, 0.0083, 0.0076, 0.0075, 0.0072, 0.0072, 0.0064, 0.0059,\n",
      "        0.0058, 0.0058, 0.0056, 0.0054, 0.0054, 0.0051, 0.0050, 0.0050, 0.0049,\n",
      "        0.0048, 0.0046, 0.0046, 0.0045, 0.0045], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 129],\n",
      "        [2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134]], device='cuda:0') possible tokens:  tensor([  30,  172,    4,   10,   20,   26,   37,    6,    7,  545,  671,    8,\n",
      "          35,  231,  377,  287,    3,   17, 1139,    1,  297, 4733,  786,   45,\n",
      "           2, 9931, 1707,   13,    5,  174,   78,  959,  347,   68,  335, 1708,\n",
      "          80, 1506,  122,  164,  726,  376, 1265, 3452,  134, 9558,  219, 2428,\n",
      "         626,  222], device='cuda:0') Pred_data:  tensor(172, device='cuda:0')\n",
      "i: 74\n",
      "next word:  ।\n",
      "74 Values:  tensor([7.6766e-01, 7.3831e-02, 4.0036e-02, 3.2335e-02, 1.9843e-02, 1.6355e-02,\n",
      "        1.5971e-02, 6.3091e-03, 2.9502e-03, 2.2094e-03, 2.0344e-03, 1.4293e-03,\n",
      "        1.4017e-03, 1.0584e-03, 1.0502e-03, 8.9981e-04, 7.7363e-04, 6.9973e-04,\n",
      "        6.9339e-04, 6.8686e-04, 6.8208e-04, 6.6295e-04, 6.3341e-04, 6.0947e-04,\n",
      "        6.0905e-04, 6.0527e-04, 4.9946e-04, 4.6776e-04, 4.2729e-04, 4.2522e-04,\n",
      "        3.9782e-04, 3.9369e-04, 3.9314e-04, 3.8775e-04, 3.8362e-04, 3.7414e-04,\n",
      "        3.5339e-04, 3.5127e-04, 3.3474e-04, 3.1394e-04, 3.0343e-04, 2.9212e-04,\n",
      "        2.6298e-04, 2.5499e-04, 2.4632e-04, 2.4362e-04, 2.2899e-04, 2.2305e-04,\n",
      "        2.1851e-04, 1.9495e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[2043],\n",
      "        [ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172]], device='cuda:0') possible tokens:  tensor([   1,    2,   12,    3,  121,   25,   93,   19,   42,  629, 1017,  287,\n",
      "         141,   38,  643, 1856,  240,  490,  245,   17, 2248, 1066,  759,   59,\n",
      "         211,  318,  246,  680,  603,  413,   95,  492,   15,   29,   34, 2800,\n",
      "         497, 1754,  111, 1622,  955, 6199, 1423, 1257,  687,   18, 3375,  810,\n",
      "          90, 2682], device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 75\n",
      "next word:  एउटा\n",
      "75 Values:  tensor([0.0883, 0.0809, 0.0792, 0.0701, 0.0548, 0.0544, 0.0296, 0.0286, 0.0253,\n",
      "        0.0240, 0.0238, 0.0232, 0.0231, 0.0212, 0.0205, 0.0189, 0.0177, 0.0171,\n",
      "        0.0165, 0.0159, 0.0153, 0.0145, 0.0131, 0.0117, 0.0098, 0.0098, 0.0093,\n",
      "        0.0093, 0.0093, 0.0091, 0.0089, 0.0089, 0.0089, 0.0089, 0.0088, 0.0085,\n",
      "        0.0083, 0.0080, 0.0079, 0.0077, 0.0076, 0.0074, 0.0073, 0.0073, 0.0072,\n",
      "        0.0070, 0.0070, 0.0069, 0.0066, 0.0064], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 102],\n",
      "        [ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1]], device='cuda:0') possible tokens:  tensor([  726,  1257,   211,    19,   497,   318,   907,  1017,    34,    80,\n",
      "          129,    90,  1773,   143,   111,  3926,    46,   289,  1040,   132,\n",
      "         1856,    41,  1254,   925,  4090, 19270, 14767,  3375,   102,    51,\n",
      "          160,   443,  1664,   141,   266,    82,    37,   985,    15,  2078,\n",
      "         1640,   148,  2010,  1047,  2182,    87,     0,   181,   680,   604],\n",
      "       device='cuda:0') Pred_data:  tensor(82, device='cuda:0')\n",
      "i: 76\n",
      "next word:  हातमा\n",
      "76 Values:  tensor([0.1390, 0.1167, 0.0566, 0.0489, 0.0297, 0.0268, 0.0267, 0.0255, 0.0254,\n",
      "        0.0246, 0.0244, 0.0235, 0.0218, 0.0213, 0.0204, 0.0190, 0.0185, 0.0182,\n",
      "        0.0164, 0.0157, 0.0156, 0.0150, 0.0145, 0.0137, 0.0126, 0.0124, 0.0122,\n",
      "        0.0119, 0.0112, 0.0108, 0.0099, 0.0098, 0.0088, 0.0086, 0.0084, 0.0082,\n",
      "        0.0074, 0.0073, 0.0072, 0.0072, 0.0071, 0.0071, 0.0070, 0.0069, 0.0069,\n",
      "        0.0069, 0.0068, 0.0066, 0.0063, 0.0063], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 646],\n",
      "        [ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82]], device='cuda:0') possible tokens:  tensor([  43,  129,   45,  726, 2653, 1310,  138,  408,  601,  134,  891, 2195,\n",
      "           0,  738, 2010, 1139,   50,  575,  502, 1456, 4090,  132, 1546, 1869,\n",
      "          49, 1519,  753,  762,    2,  481, 1330,  599, 6059,  217, 1096, 4930,\n",
      "         135, 2940, 4199, 1846, 1599,  313,  905,  582, 3181,  125,  352,  118,\n",
      "          47,   26], device='cuda:0') Pred_data:  tensor(1599, device='cuda:0')\n",
      "i: 77\n",
      "next word:  लिएर\n",
      "77 Values:  tensor([0.0763, 0.0739, 0.0664, 0.0588, 0.0536, 0.0532, 0.0349, 0.0348, 0.0323,\n",
      "        0.0255, 0.0235, 0.0225, 0.0220, 0.0215, 0.0209, 0.0203, 0.0193, 0.0172,\n",
      "        0.0169, 0.0163, 0.0162, 0.0160, 0.0158, 0.0146, 0.0130, 0.0110, 0.0110,\n",
      "        0.0108, 0.0107, 0.0100, 0.0100, 0.0098, 0.0093, 0.0093, 0.0089, 0.0087,\n",
      "        0.0085, 0.0083, 0.0081, 0.0080, 0.0074, 0.0073, 0.0073, 0.0072, 0.0071,\n",
      "        0.0071, 0.0071, 0.0071, 0.0071, 0.0071], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 287],\n",
      "        [   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599]], device='cuda:0') possible tokens:  tensor([2450, 5454,   34,  238,  520,  321,  726, 1120,  738,   38,    2, 3072,\n",
      "        1456,   43,   30,   82,    0,  138,   17,  714,   41,  614,   39,   18,\n",
      "          46,   80,   40, 1460, 2237,  160,    3,   21, 3406, 1599,    5, 2611,\n",
      "          20,  258,   12, 1001,   92, 2201,   26,    4,   42, 4452, 1773,  135,\n",
      "         182,  381], device='cuda:0') Pred_data:  tensor(238, device='cuda:0')\n",
      "i: 78\n",
      "next word:  वा\n",
      "78 Values:  tensor([0.0652, 0.0643, 0.0506, 0.0448, 0.0368, 0.0366, 0.0333, 0.0333, 0.0327,\n",
      "        0.0304, 0.0283, 0.0267, 0.0260, 0.0256, 0.0255, 0.0225, 0.0217, 0.0213,\n",
      "        0.0192, 0.0175, 0.0173, 0.0170, 0.0162, 0.0156, 0.0154, 0.0153, 0.0151,\n",
      "        0.0139, 0.0139, 0.0138, 0.0137, 0.0122, 0.0120, 0.0115, 0.0104, 0.0102,\n",
      "        0.0099, 0.0096, 0.0090, 0.0086, 0.0085, 0.0085, 0.0081, 0.0080, 0.0077,\n",
      "        0.0076, 0.0075, 0.0071, 0.0071, 0.0069], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   1],\n",
      "        [  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238]], device='cuda:0') possible tokens:  tensor([  43,   34,  318,   42, 1773,  907,  479,  726,    2,   82,   66, 1079,\n",
      "         138,   90, 1257,  925,  181, 5196,   18,   51,  205,   26,    0, 2182,\n",
      "          80,  135,  160,  237,   17,  985, 1112, 1696,   41,   39, 1228,  444,\n",
      "          46,   83,    5, 1230,  258, 4313,  143,  381,  147,  111, 3019,  588,\n",
      "        4316, 1081], device='cuda:0') Pred_data:  tensor(42, device='cuda:0')\n",
      "i: 79\n",
      "next word:  फरक\n",
      "79 Values:  tensor([0.0974, 0.0714, 0.0562, 0.0509, 0.0448, 0.0431, 0.0418, 0.0342, 0.0305,\n",
      "        0.0288, 0.0285, 0.0242, 0.0234, 0.0228, 0.0213, 0.0151, 0.0150, 0.0145,\n",
      "        0.0145, 0.0136, 0.0133, 0.0132, 0.0132, 0.0131, 0.0127, 0.0113, 0.0113,\n",
      "        0.0108, 0.0108, 0.0107, 0.0106, 0.0105, 0.0105, 0.0105, 0.0105, 0.0102,\n",
      "        0.0100, 0.0099, 0.0098, 0.0096, 0.0092, 0.0090, 0.0090, 0.0087, 0.0085,\n",
      "        0.0083, 0.0083, 0.0083, 0.0082, 0.0081], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  90],\n",
      "        [2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238],\n",
      "        [  42]], device='cuda:0') possible tokens:  tensor([   34,   726,   181,  1389,   985,     0,    41,  1079,  4090,   100,\n",
      "          364,    82,   891,  1599,   168,  2393,   521,    18,  6059,  2413,\n",
      "         4794,   601,    51, 19270,   125,   614,  1460,    52,   703,   907,\n",
      "         2940,  1257,  1773,   282,   318,    64,   352,    46,    59,   520,\n",
      "          394,  6129,  1432,   946,  4097,  1402, 13465,  9233,   648,    39],\n",
      "       device='cuda:0') Pred_data:  tensor(394, device='cuda:0')\n",
      "i: 80\n",
      "next word:  व्यवहार\n",
      "80 Values:  tensor([0.0944, 0.0742, 0.0515, 0.0504, 0.0416, 0.0343, 0.0342, 0.0307, 0.0288,\n",
      "        0.0282, 0.0269, 0.0266, 0.0263, 0.0259, 0.0252, 0.0246, 0.0220, 0.0216,\n",
      "        0.0181, 0.0181, 0.0181, 0.0151, 0.0142, 0.0139, 0.0137, 0.0136, 0.0124,\n",
      "        0.0115, 0.0110, 0.0108, 0.0107, 0.0106, 0.0105, 0.0102, 0.0085, 0.0083,\n",
      "        0.0081, 0.0081, 0.0079, 0.0079, 0.0078, 0.0077, 0.0075, 0.0074, 0.0074,\n",
      "        0.0073, 0.0068, 0.0065, 0.0064, 0.0064], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[2182],\n",
      "        [ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238],\n",
      "        [  42],\n",
      "        [ 394]], device='cuda:0') possible tokens:  tensor([  394,  1191,  1483,  1285,   333,  1708,    37, 11410, 12557,     0,\n",
      "         1498,   347,  4918,    30,  2018,  1502,  2295,   182,  1360,  1659,\n",
      "           26,   726,  5450,   144,    45,    84,  2851,  3745,     2,  3966,\n",
      "          737,    20, 16688,  1460,     4,   376,   174,   784,   313,  1024,\n",
      "        19802,  2382,  2053,  3766, 17696,   875,   481, 13142,  2967, 15550],\n",
      "       device='cuda:0') Pred_data:  tensor(1285, device='cuda:0')\n",
      "i: 81\n",
      "next word:  गर्नुपर्छ\n",
      "81 Values:  tensor([0.2595, 0.0483, 0.0451, 0.0394, 0.0378, 0.0375, 0.0347, 0.0294, 0.0284,\n",
      "        0.0238, 0.0226, 0.0224, 0.0223, 0.0207, 0.0188, 0.0177, 0.0175, 0.0171,\n",
      "        0.0166, 0.0147, 0.0135, 0.0134, 0.0131, 0.0130, 0.0103, 0.0100, 0.0097,\n",
      "        0.0096, 0.0089, 0.0077, 0.0077, 0.0074, 0.0071, 0.0068, 0.0066, 0.0066,\n",
      "        0.0064, 0.0064, 0.0063, 0.0061, 0.0056, 0.0056, 0.0055, 0.0051, 0.0051,\n",
      "        0.0049, 0.0044, 0.0044, 0.0042, 0.0042], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 283],\n",
      "        [   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238],\n",
      "        [  42],\n",
      "        [ 394],\n",
      "        [1285]], device='cuda:0') possible tokens:  tensor([   10,   377,    13,   417,   133,  1563,  5631,  1567,    30,     5,\n",
      "            0,     2,     3,    20,   583,  2428,  1707,    40,   347,    37,\n",
      "         2309,   400,  2526,   545,    27,     6,    56,   262,   108,   134,\n",
      "         2585,  2237,   584,  2337,    42,    11, 10591,  6012,     8,  2260,\n",
      "         3519,    68,   476,  1977,    17,   253,  5903,   231,    43, 13082],\n",
      "       device='cuda:0') Pred_data:  tensor(417, device='cuda:0')\n",
      "i: 82\n",
      "next word:  ?\n",
      "82 Values:  tensor([8.6137e-01, 3.6672e-02, 2.9260e-02, 2.1668e-02, 1.3598e-02, 7.4946e-03,\n",
      "        3.4983e-03, 2.5066e-03, 2.1887e-03, 1.7982e-03, 1.3771e-03, 1.3400e-03,\n",
      "        1.3278e-03, 1.3225e-03, 1.3100e-03, 8.9076e-04, 6.8275e-04, 6.0936e-04,\n",
      "        5.8674e-04, 5.8383e-04, 5.4048e-04, 5.1987e-04, 4.8496e-04, 4.4855e-04,\n",
      "        4.2417e-04, 4.1288e-04, 4.0828e-04, 3.8592e-04, 3.8590e-04, 3.8547e-04,\n",
      "        3.7924e-04, 3.7027e-04, 3.6303e-04, 3.3442e-04, 3.3392e-04, 3.1597e-04,\n",
      "        2.9535e-04, 2.8662e-04, 2.5552e-04, 2.5396e-04, 2.5083e-04, 2.5055e-04,\n",
      "        2.4838e-04, 2.3945e-04, 2.3242e-04, 2.2600e-04, 2.2439e-04, 2.2374e-04,\n",
      "        2.1876e-04, 2.1597e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[   4],\n",
      "        [   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238],\n",
      "        [  42],\n",
      "        [ 394],\n",
      "        [1285],\n",
      "        [ 417]], device='cuda:0') possible tokens:  tensor([   1,    2,   25,    3,   12,   93,  121,   42,   19,  413, 1017,  629,\n",
      "        2248,  245,  141,  759,  240,  492, 1856,   15,   59,  828,   18,   34,\n",
      "          17,  955, 1419,   95, 1622,  211,  287,   29,  680,  497,  318,    0,\n",
      "         246,  603,   38,   97,   14, 1066, 4587, 2800,  643,  396, 1142,    5,\n",
      "         528, 8136], device='cuda:0') Pred_data:  tensor(2, device='cuda:0')\n",
      "i: 83\n",
      "next word:  तपाईं\n",
      "83 Values:  tensor([0.0688, 0.0646, 0.0566, 0.0469, 0.0427, 0.0394, 0.0363, 0.0339, 0.0319,\n",
      "        0.0315, 0.0298, 0.0276, 0.0250, 0.0244, 0.0237, 0.0173, 0.0169, 0.0155,\n",
      "        0.0155, 0.0154, 0.0153, 0.0150, 0.0141, 0.0138, 0.0134, 0.0133, 0.0132,\n",
      "        0.0131, 0.0130, 0.0130, 0.0126, 0.0124, 0.0116, 0.0114, 0.0113, 0.0109,\n",
      "        0.0107, 0.0105, 0.0104, 0.0104, 0.0102, 0.0092, 0.0091, 0.0090, 0.0088,\n",
      "        0.0085, 0.0083, 0.0082, 0.0081, 0.0078], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   1],\n",
      "        [  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238],\n",
      "        [  42],\n",
      "        [ 394],\n",
      "        [1285],\n",
      "        [ 417],\n",
      "        [   2]], device='cuda:0') possible tokens:  tensor([ 726,  907,   19,  497,   34, 1040,  318, 1419,  211,  413,   41,  443,\n",
      "         492,    0, 1257,    3, 3926,  550,  925,  184, 4090,  181, 1773,   82,\n",
      "        1856,  672,  680,  289,  160,  852, 1017,   80, 1035,   18,  141,   29,\n",
      "          59,  129,  929,   90,  245,   42, 2207,   15,   26,  143,  528,  637,\n",
      "          46,  266], device='cuda:0') Pred_data:  tensor(318, device='cuda:0')\n",
      "i: 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  आफ्नो\n",
      "84 Values:  tensor([0.2614, 0.0601, 0.0566, 0.0496, 0.0367, 0.0278, 0.0267, 0.0241, 0.0212,\n",
      "        0.0191, 0.0184, 0.0174, 0.0165, 0.0160, 0.0155, 0.0152, 0.0142, 0.0138,\n",
      "        0.0135, 0.0131, 0.0127, 0.0125, 0.0122, 0.0106, 0.0103, 0.0102, 0.0102,\n",
      "        0.0102, 0.0101, 0.0098, 0.0097, 0.0095, 0.0087, 0.0086, 0.0084, 0.0083,\n",
      "        0.0082, 0.0080, 0.0078, 0.0077, 0.0076, 0.0076, 0.0072, 0.0071, 0.0070,\n",
      "        0.0068, 0.0068, 0.0065, 0.0064, 0.0064], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  46],\n",
      "        [ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238],\n",
      "        [  42],\n",
      "        [ 394],\n",
      "        [1285],\n",
      "        [ 417],\n",
      "        [   2],\n",
      "        [ 318]], device='cuda:0') possible tokens:  tensor([  34,   80,   17,    5,  726,  891,  258,  140, 1578,   46,    2,   43,\n",
      "          18, 1559, 2010,  907,  111,    3,   82,  134,   41, 1079,  703, 3055,\n",
      "        1892,  820, 1029,  635,  381,   32,  689, 2809,  281, 3803,   38,  100,\n",
      "          42,  253, 4937,   26,   51,  662,   15, 1460,   10, 8685,  695,  282,\n",
      "          87,  479], device='cuda:0') Pred_data:  tensor(34, device='cuda:0')\n",
      "i: 85\n",
      "next word:  अनुकूलको\n",
      "85 Values:  tensor([0.0888, 0.0744, 0.0422, 0.0411, 0.0355, 0.0319, 0.0316, 0.0278, 0.0258,\n",
      "        0.0239, 0.0228, 0.0218, 0.0210, 0.0206, 0.0206, 0.0189, 0.0180, 0.0174,\n",
      "        0.0165, 0.0165, 0.0165, 0.0163, 0.0163, 0.0162, 0.0152, 0.0150, 0.0149,\n",
      "        0.0143, 0.0141, 0.0138, 0.0136, 0.0136, 0.0135, 0.0129, 0.0128, 0.0126,\n",
      "        0.0126, 0.0126, 0.0125, 0.0125, 0.0123, 0.0110, 0.0110, 0.0107, 0.0103,\n",
      "        0.0093, 0.0093, 0.0092, 0.0091, 0.0088], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 726],\n",
      "        [ 138],\n",
      "        [ 134],\n",
      "        [ 172],\n",
      "        [   1],\n",
      "        [  82],\n",
      "        [1599],\n",
      "        [ 238],\n",
      "        [  42],\n",
      "        [ 394],\n",
      "        [1285],\n",
      "        [ 417],\n",
      "        [   2],\n",
      "        [ 318],\n",
      "        [  34]], device='cuda:0') possible tokens:  tensor([ 1813,  1599,  2792,     0,  1670,  2783,   386,   971,  6955,  2361,\n",
      "           34,  9558,   299,   135,   520,   233,   784,   213, 43765,   499,\n",
      "         3085,   642,  1807,  5816,  1460,  4352,  2535,   303,   756,  9971,\n",
      "         1585,  5931,  1219, 12995,  4190,  3745,    26,  2504, 12996,  1441,\n",
      "          618, 19819,   737,   622,   352,  7972,    87,   122,   134,  8157],\n",
      "       device='cuda:0') Pred_data:  tensor(19819, device='cuda:0')\n",
      "i: 86\n",
      "next word:  बाटो\n",
      "86 Values:  tensor([0.1611, 0.0798, 0.0649, 0.0461, 0.0421, 0.0413, 0.0377, 0.0343, 0.0307,\n",
      "        0.0269, 0.0244, 0.0239, 0.0231, 0.0223, 0.0204, 0.0187, 0.0183, 0.0180,\n",
      "        0.0147, 0.0136, 0.0130, 0.0124, 0.0119, 0.0109, 0.0108, 0.0106, 0.0102,\n",
      "        0.0100, 0.0091, 0.0087, 0.0076, 0.0076, 0.0074, 0.0074, 0.0074, 0.0073,\n",
      "        0.0072, 0.0071, 0.0067, 0.0067, 0.0063, 0.0062, 0.0061, 0.0058, 0.0057,\n",
      "        0.0057, 0.0057, 0.0056, 0.0053, 0.0052], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  138],\n",
      "        [  134],\n",
      "        [  172],\n",
      "        [    1],\n",
      "        [   82],\n",
      "        [ 1599],\n",
      "        [  238],\n",
      "        [   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819]], device='cuda:0') possible tokens:  tensor([   26,   738,    42,    92,   435,  4097,   222,   163,     3,    30,\n",
      "          846,     0,   705,    34,  1460,    37,  4109,  6124,   784,   108,\n",
      "          756,  2967,  1187,     2,  9558,   599,     6,  2316,    20,  1265,\n",
      "         1303,   122,  4577,  1659,   400, 15769,    45,  5619,    81,   386,\n",
      "         3762,   293,  1069,   169,  5080,   384,   134,  4389,   100,   545],\n",
      "       device='cuda:0') Pred_data:  tensor(435, device='cuda:0')\n",
      "i: 87\n",
      "next word:  रोजे\n",
      "87 Values:  tensor([0.0829, 0.0805, 0.0536, 0.0447, 0.0379, 0.0318, 0.0298, 0.0264, 0.0258,\n",
      "        0.0255, 0.0251, 0.0230, 0.0230, 0.0213, 0.0205, 0.0196, 0.0181, 0.0170,\n",
      "        0.0170, 0.0168, 0.0167, 0.0156, 0.0155, 0.0155, 0.0145, 0.0145, 0.0145,\n",
      "        0.0141, 0.0137, 0.0136, 0.0126, 0.0123, 0.0123, 0.0120, 0.0116, 0.0115,\n",
      "        0.0113, 0.0110, 0.0110, 0.0106, 0.0103, 0.0101, 0.0101, 0.0100, 0.0095,\n",
      "        0.0095, 0.0093, 0.0090, 0.0088, 0.0087], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  134],\n",
      "        [  172],\n",
      "        [    1],\n",
      "        [   82],\n",
      "        [ 1599],\n",
      "        [  238],\n",
      "        [   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435]], device='cuda:0') possible tokens:  tensor([14705,    42, 18067,  6322,     3,   978,     2, 12027,     0, 26104,\n",
      "         2038, 46827,  2095,  1578,  1522,    30, 19901,   166,  3904, 32620,\n",
      "           17,  6012, 14302,  5409,  2585,  7525, 11373,    43,  2671,   583,\n",
      "         1535, 29712,  7099,  2141,    34,  2566, 18321,   186,   253,  2260,\n",
      "        10785,   459,  1026,  9608,  1037, 19114,  2033,  4527,  3099,     6],\n",
      "       device='cuda:0') Pred_data:  tensor(18067, device='cuda:0')\n",
      "i: 88\n",
      "next word:  वा\n",
      "88 Values:  tensor([0.4027, 0.2547, 0.0951, 0.0649, 0.0390, 0.0310, 0.0172, 0.0147, 0.0079,\n",
      "        0.0076, 0.0053, 0.0051, 0.0048, 0.0044, 0.0043, 0.0027, 0.0023, 0.0022,\n",
      "        0.0022, 0.0021, 0.0021, 0.0021, 0.0021, 0.0020, 0.0020, 0.0019, 0.0014,\n",
      "        0.0013, 0.0013, 0.0012, 0.0010, 0.0009, 0.0009, 0.0008, 0.0007, 0.0007,\n",
      "        0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  172],\n",
      "        [    1],\n",
      "        [   82],\n",
      "        [ 1599],\n",
      "        [  238],\n",
      "        [   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067]], device='cuda:0') possible tokens:  tensor([   1,   12,    2,    3,  121,   42,   93,  141,    5,  240,  643,   25,\n",
      "         344,  287,   19,  629,   43,  810,   30,   95, 1773, 6199, 1017, 1066,\n",
      "        1928,   38, 3023, 2207,  318, 3417,   59,  167,  413, 1291,  246, 5559,\n",
      "        2770,  300,   34, 1856,  138, 1754,  687,  603,   82,   90,  689,   80,\n",
      "         211,  245], device='cuda:0') Pred_data:  tensor(42, device='cuda:0')\n",
      "i: 89\n",
      "next word:  अरू\n",
      "89 Values:  tensor([0.2155, 0.0799, 0.0581, 0.0443, 0.0393, 0.0370, 0.0344, 0.0258, 0.0247,\n",
      "        0.0229, 0.0216, 0.0193, 0.0167, 0.0157, 0.0156, 0.0156, 0.0144, 0.0144,\n",
      "        0.0141, 0.0131, 0.0128, 0.0124, 0.0120, 0.0117, 0.0112, 0.0105, 0.0104,\n",
      "        0.0099, 0.0098, 0.0095, 0.0092, 0.0091, 0.0091, 0.0090, 0.0080, 0.0079,\n",
      "        0.0077, 0.0075, 0.0072, 0.0070, 0.0070, 0.0069, 0.0067, 0.0066, 0.0065,\n",
      "        0.0065, 0.0065, 0.0065, 0.0064, 0.0063], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[    1],\n",
      "        [   82],\n",
      "        [ 1599],\n",
      "        [  238],\n",
      "        [   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42]], device='cuda:0') possible tokens:  tensor([   34,   985,   100,     0,  1257,    41,  1773,   282,   907,    59,\n",
      "           82,   181,    39,   281,    46,  4259,  2393,   147,  2754,   726,\n",
      "         8685,   314,  1079,  6095,   521,   243,   390,  4097,  8690,   648,\n",
      "         2536,   235,    92,    26, 19270,   318,   598,   326,   891,   528,\n",
      "         6129,  4361,   578,  4401,  3602,  1460,    18, 41812,  3228,   561],\n",
      "       device='cuda:0') Pred_data:  tensor(985, device='cuda:0')\n",
      "i: 90\n",
      "next word:  कुनै\n",
      "90 Values:  tensor([0.2835, 0.1139, 0.0653, 0.0564, 0.0525, 0.0375, 0.0356, 0.0322, 0.0210,\n",
      "        0.0181, 0.0178, 0.0164, 0.0154, 0.0123, 0.0120, 0.0119, 0.0113, 0.0111,\n",
      "        0.0104, 0.0099, 0.0091, 0.0088, 0.0086, 0.0076, 0.0073, 0.0070, 0.0069,\n",
      "        0.0067, 0.0066, 0.0065, 0.0060, 0.0057, 0.0055, 0.0052, 0.0045, 0.0044,\n",
      "        0.0043, 0.0040, 0.0040, 0.0039, 0.0036, 0.0035, 0.0035, 0.0034, 0.0033,\n",
      "        0.0032, 0.0031, 0.0031, 0.0031, 0.0031], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   82],\n",
      "        [ 1599],\n",
      "        [  238],\n",
      "        [   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985]], device='cuda:0') possible tokens:  tensor([   41,  1432,    17,    46,   546,     5,   985,  1457,  1908,   100,\n",
      "           39,   702,    45,    10,  6202,    80,    26,  1111,   898,    34,\n",
      "           92,   726,  1139,   179,    95,     0,  1846,    51,   859,  4259,\n",
      "         3897,   599,   891,  1817,   435,  1869,   326,  1122,   738,    38,\n",
      "        10195,  8685,   875,   510,    52,   907,  1257,   762,   318,   771],\n",
      "       device='cuda:0') Pred_data:  tensor(41, device='cuda:0')\n",
      "i: 91\n",
      "next word:  हालतमा\n",
      "91 Values:  tensor([0.3000, 0.0712, 0.0380, 0.0343, 0.0294, 0.0275, 0.0267, 0.0263, 0.0245,\n",
      "        0.0217, 0.0207, 0.0179, 0.0164, 0.0163, 0.0153, 0.0153, 0.0149, 0.0145,\n",
      "        0.0138, 0.0136, 0.0129, 0.0120, 0.0108, 0.0107, 0.0106, 0.0104, 0.0102,\n",
      "        0.0101, 0.0098, 0.0095, 0.0093, 0.0093, 0.0084, 0.0083, 0.0082, 0.0072,\n",
      "        0.0070, 0.0069, 0.0067, 0.0066, 0.0063, 0.0062, 0.0061, 0.0060, 0.0058,\n",
      "        0.0056, 0.0054, 0.0052, 0.0050, 0.0049], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 1599],\n",
      "        [  238],\n",
      "        [   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41]], device='cuda:0') possible tokens:  tensor([    5,    92,  4860,   179,   891,   217,     0,    26,    47,   202,\n",
      "          333,   235,   726,   100,   599,    18,   738,  1310,   496,   875,\n",
      "         2361,  1244,    50,    41, 11667,    49,  1139,   435,   163,  1360,\n",
      "         5897,    65,    45,    82,  1265, 26657,  1197,   481, 17644,   859,\n",
      "          568,  1846,  3088, 15538,  2940,  1532,  2989,   737,   762,    84],\n",
      "       device='cuda:0') Pred_data:  tensor(4860, device='cuda:0')\n",
      "i: 92\n",
      "next word:  पनि\n",
      "92 Values:  tensor([0.1581, 0.1385, 0.0588, 0.0429, 0.0405, 0.0307, 0.0284, 0.0275, 0.0211,\n",
      "        0.0185, 0.0183, 0.0180, 0.0175, 0.0169, 0.0161, 0.0156, 0.0147, 0.0146,\n",
      "        0.0140, 0.0130, 0.0125, 0.0124, 0.0119, 0.0117, 0.0113, 0.0111, 0.0110,\n",
      "        0.0105, 0.0099, 0.0098, 0.0094, 0.0093, 0.0092, 0.0092, 0.0091, 0.0091,\n",
      "        0.0091, 0.0091, 0.0089, 0.0089, 0.0085, 0.0082, 0.0080, 0.0080, 0.0072,\n",
      "        0.0070, 0.0068, 0.0067, 0.0065, 0.0065], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  238],\n",
      "        [   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860]], device='cuda:0') possible tokens:  tensor([  34,    5, 3803,   17,    2, 1079, 1578,  297,   43,   37,   42, 1254,\n",
      "           0,  195,  435, 5547,  907,   10, 1257,  140, 1026,  583, 1892,   41,\n",
      "          59,   38,  281,   26, 1270,  134,  866,    1,  545, 5196, 7881,  578,\n",
      "        2696, 1040, 1367,  479,   30,  163, 1588,    3,  282, 4733,   82, 2090,\n",
      "         614,  318], device='cuda:0') Pred_data:  tensor(5, device='cuda:0')\n",
      "i: 93\n",
      "next word:  पुरुष\n",
      "93 Values:  tensor([0.1919, 0.0592, 0.0554, 0.0462, 0.0340, 0.0305, 0.0294, 0.0246, 0.0232,\n",
      "        0.0205, 0.0200, 0.0192, 0.0186, 0.0186, 0.0175, 0.0169, 0.0160, 0.0152,\n",
      "        0.0149, 0.0145, 0.0141, 0.0138, 0.0134, 0.0129, 0.0120, 0.0118, 0.0118,\n",
      "        0.0116, 0.0113, 0.0113, 0.0111, 0.0111, 0.0111, 0.0108, 0.0105, 0.0100,\n",
      "        0.0097, 0.0096, 0.0096, 0.0095, 0.0093, 0.0093, 0.0092, 0.0088, 0.0087,\n",
      "        0.0085, 0.0084, 0.0083, 0.0082, 0.0081], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   42],\n",
      "        [  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860],\n",
      "        [    5]], device='cuda:0') possible tokens:  tensor([  34, 1257,   10,  297,  134,   26,  435,   30,  907,  726,    1,   37,\n",
      "        3803,   41, 1040,  738,   92,  163,  703, 1780,  976,  762, 1254, 1079,\n",
      "           0,   35,  281,   59,  599, 2182, 1773,  318,  303,   18,  528,  111,\n",
      "        2147,   15,   82, 4733,  833, 2783,  283,   39, 1588,  578,  705,  400,\n",
      "        4097,  133], device='cuda:0') Pred_data:  tensor(726, device='cuda:0')\n",
      "i: 94\n",
      "next word:  नै\n",
      "94 Values:  tensor([0.2525, 0.1493, 0.1178, 0.0346, 0.0280, 0.0278, 0.0235, 0.0213, 0.0209,\n",
      "        0.0190, 0.0185, 0.0181, 0.0179, 0.0149, 0.0143, 0.0134, 0.0132, 0.0112,\n",
      "        0.0112, 0.0089, 0.0083, 0.0082, 0.0079, 0.0075, 0.0075, 0.0073, 0.0069,\n",
      "        0.0067, 0.0067, 0.0066, 0.0061, 0.0056, 0.0056, 0.0053, 0.0052, 0.0052,\n",
      "        0.0047, 0.0046, 0.0044, 0.0044, 0.0043, 0.0043, 0.0041, 0.0041, 0.0040,\n",
      "        0.0037, 0.0036, 0.0036, 0.0036, 0.0035], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  394],\n",
      "        [ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860],\n",
      "        [    5],\n",
      "        [  726]], device='cuda:0') possible tokens:  tensor([ 4100,    17,    42,    37,   643,     1,   344,  2301,   545,    43,\n",
      "          297,     2,     3,  4733,  1359,   519,   376,  5606,  1594,  2127,\n",
      "         2367,   635,  4190,    14, 19270,   671,   726, 21651,    95,  1115,\n",
      "          138,  9301,   364,   347,   172,    68,    34,    10,  2427,    32,\n",
      "         3548,   174,   695, 29454,  1269, 11339,   425,   533,  2770,   240],\n",
      "       device='cuda:0') Pred_data:  tensor(17, device='cuda:0')\n",
      "i: 95\n",
      "next word:  हुनुहुन्छ\n",
      "95 Values:  tensor([0.0990, 0.0786, 0.0458, 0.0453, 0.0434, 0.0372, 0.0353, 0.0343, 0.0342,\n",
      "        0.0316, 0.0313, 0.0307, 0.0302, 0.0271, 0.0236, 0.0230, 0.0215, 0.0198,\n",
      "        0.0181, 0.0180, 0.0157, 0.0154, 0.0130, 0.0117, 0.0111, 0.0110, 0.0106,\n",
      "        0.0105, 0.0102, 0.0102, 0.0102, 0.0100, 0.0099, 0.0098, 0.0094, 0.0091,\n",
      "        0.0082, 0.0078, 0.0071, 0.0069, 0.0069, 0.0067, 0.0067, 0.0066, 0.0065,\n",
      "        0.0064, 0.0064, 0.0060, 0.0060, 0.0059], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[ 1285],\n",
      "        [  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860],\n",
      "        [    5],\n",
      "        [  726],\n",
      "        [   17]], device='cuda:0') possible tokens:  tensor([   37,  2301,   347,    20,  4733,  1079,    57,     1,   671,   545,\n",
      "         1506,     7,   300,     8,  5658,   297,    34,   726,    30,    35,\n",
      "          335,   519,    24,   134,  5213,   753,  1707,   172,     4,  5547,\n",
      "          703,     2,    10,  6962,  7099,    64,   959,   376,   764,  3395,\n",
      "         1132,  1836,  2427,  1191,  2569,   396,  3803, 14169,    17,  1712],\n",
      "       device='cuda:0') Pred_data:  tensor(671, device='cuda:0')\n",
      "i: 96\n",
      "next word:  ।\n",
      "96 Values:  tensor([6.7097e-01, 1.1108e-01, 6.6194e-02, 4.1603e-02, 2.9363e-02, 1.7867e-02,\n",
      "        1.1393e-02, 1.0583e-02, 6.7666e-03, 3.8733e-03, 2.6910e-03, 2.5097e-03,\n",
      "        2.1520e-03, 1.9447e-03, 1.1913e-03, 1.1065e-03, 9.6541e-04, 9.6514e-04,\n",
      "        9.3436e-04, 9.3313e-04, 8.9779e-04, 8.4936e-04, 8.3142e-04, 6.8188e-04,\n",
      "        6.7307e-04, 6.5783e-04, 6.4967e-04, 5.8321e-04, 5.6374e-04, 5.4653e-04,\n",
      "        5.1801e-04, 4.9334e-04, 4.7216e-04, 4.7071e-04, 4.6744e-04, 4.6702e-04,\n",
      "        4.4263e-04, 4.3583e-04, 4.0490e-04, 3.9825e-04, 3.9745e-04, 3.7172e-04,\n",
      "        3.4782e-04, 3.4290e-04, 3.3496e-04, 3.3261e-04, 3.2704e-04, 3.2479e-04,\n",
      "        3.1592e-04, 3.1449e-04], device='cuda:0', grad_fn=<SelectBackward0>) Gen_data:  tensor([[  417],\n",
      "        [    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860],\n",
      "        [    5],\n",
      "        [  726],\n",
      "        [   17],\n",
      "        [  671]], device='cuda:0') possible tokens:  tensor([    1,     2,    12,     3,    93,    25,    42,   629,   121,   287,\n",
      "          643,   141,   413,    19,    38,  1017,   492,    59,  2207,    17,\n",
      "         1066,   603,   240,   680,  8348,  2248,    15,   211,   759, 10019,\n",
      "           34,  1856,    80,   245,  1622,  1754,   318,  3998, 12838,  1773,\n",
      "         4679,  3375,    97,   497,  2437,    95,  6199,  4388,    29,   490],\n",
      "       device='cuda:0') Pred_data:  tensor(1, device='cuda:0')\n",
      "i: 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word:  यस्तो\n",
      "97 Values:  tensor([0.1616, 0.0497, 0.0467, 0.0452, 0.0426, 0.0416, 0.0403, 0.0373, 0.0302,\n",
      "        0.0299, 0.0297, 0.0260, 0.0224, 0.0220, 0.0203, 0.0194, 0.0162, 0.0158,\n",
      "        0.0157, 0.0144, 0.0138, 0.0128, 0.0127, 0.0123, 0.0123, 0.0122, 0.0122,\n",
      "        0.0121, 0.0109, 0.0107, 0.0102, 0.0100, 0.0100, 0.0085, 0.0080, 0.0080,\n",
      "        0.0080, 0.0079, 0.0076, 0.0074, 0.0072, 0.0069, 0.0068, 0.0066, 0.0064,\n",
      "        0.0064, 0.0063, 0.0062, 0.0062, 0.0062], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[    2],\n",
      "        [  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860],\n",
      "        [    5],\n",
      "        [  726],\n",
      "        [   17],\n",
      "        [  671],\n",
      "        [    1]], device='cuda:0') possible tokens:  tensor([  726,  5283,   211,    80,   907,  1257,   497, 19270,   129,    41,\n",
      "         1254,    19,   318,  1773,  1017,    34,  1047,   132,    46,   289,\n",
      "          925, 14767,  1040,   985,    82,  4994,    15,  4090,  1640,   852,\n",
      "          604,  1856,   680,  2477,  6881,   111,    18,  8080,   143,  3800,\n",
      "         2078,  1016,   281,     0,   578,  3926,    37, 23331,  1887,   181],\n",
      "       device='cuda:0') Pred_data:  tensor(129, device='cuda:0')\n",
      "i: 98\n",
      "next word:  कुरालाई\n",
      "98 Values:  tensor([0.4829, 0.0441, 0.0248, 0.0245, 0.0241, 0.0238, 0.0194, 0.0185, 0.0154,\n",
      "        0.0148, 0.0144, 0.0139, 0.0131, 0.0127, 0.0123, 0.0112, 0.0111, 0.0107,\n",
      "        0.0101, 0.0094, 0.0091, 0.0087, 0.0086, 0.0083, 0.0076, 0.0076, 0.0075,\n",
      "        0.0069, 0.0067, 0.0065, 0.0064, 0.0063, 0.0063, 0.0061, 0.0061, 0.0060,\n",
      "        0.0059, 0.0059, 0.0059, 0.0057, 0.0055, 0.0055, 0.0053, 0.0051, 0.0051,\n",
      "        0.0050, 0.0048, 0.0048, 0.0048, 0.0048], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[  318],\n",
      "        [   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860],\n",
      "        [    5],\n",
      "        [  726],\n",
      "        [   17],\n",
      "        [  671],\n",
      "        [    1],\n",
      "        [  129]], device='cuda:0') possible tokens:  tensor([  164,   169,  2989, 12598,  4420,    45,    37,   179,  1453,   106,\n",
      "          202,   163,   114,    46,   859,  2103,    92,  6235,  1632,  1310,\n",
      "         2966,   502,    26,    38,  4832,   246,  1003,   846,    10,  1261,\n",
      "          985,  3682,  4097,   333,     2,  1819,     0,    41,   300,  2043,\n",
      "        11879,    80,  1285,    82,  1458,    85,  2360,    18,   726,   575],\n",
      "       device='cuda:0') Pred_data:  tensor(1632, device='cuda:0')\n",
      "i: 99\n",
      "next word:  <unk>\n",
      "99 Values:  tensor([0.1192, 0.1010, 0.0418, 0.0349, 0.0344, 0.0308, 0.0300, 0.0271, 0.0269,\n",
      "        0.0266, 0.0243, 0.0242, 0.0222, 0.0220, 0.0200, 0.0196, 0.0191, 0.0190,\n",
      "        0.0172, 0.0167, 0.0147, 0.0144, 0.0140, 0.0140, 0.0133, 0.0132, 0.0130,\n",
      "        0.0129, 0.0125, 0.0117, 0.0114, 0.0112, 0.0109, 0.0108, 0.0108, 0.0103,\n",
      "        0.0099, 0.0097, 0.0096, 0.0096, 0.0095, 0.0093, 0.0090, 0.0089, 0.0087,\n",
      "        0.0086, 0.0079, 0.0078, 0.0077, 0.0075], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>) Gen_data:  tensor([[   34],\n",
      "        [19819],\n",
      "        [  435],\n",
      "        [18067],\n",
      "        [   42],\n",
      "        [  985],\n",
      "        [   41],\n",
      "        [ 4860],\n",
      "        [    5],\n",
      "        [  726],\n",
      "        [   17],\n",
      "        [  671],\n",
      "        [    1],\n",
      "        [  129],\n",
      "        [ 1632]], device='cuda:0') possible tokens:  tensor([ 160,    5,  143,  258,  907,  651,   34,   90,   80,    0, 3718,  552,\n",
      "         111, 2783, 1257, 5283,   83,   39,  726, 7935,  925,   46,  832, 1773,\n",
      "         318, 3099,  768,   17, 1397, 3222, 1335,   41,  876,  238,  546, 1514,\n",
      "        2663,   38, 2010,  235, 8489,  756, 2182,  605,  852, 1664,  300,  168,\n",
      "         615, 4124], device='cuda:0') Pred_data:  tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "z__ = nonnaive_generator(best_model, st_i,no_words = 100,k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dba78ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'म भारत भ्रमण गर्न चाहन्छु र के गर्ने हो भने तपाईले गर्ने भनेको नै धेरै हदसम्म जनताले आफ्नो इच्छा र आवश्यकता कहाँ छ ? तपाईं आफ्नो पक्षमा मत दिएको थाहा भएकै हो । त्यसैले अहिले राजनीतिक सहमति प्राप्त भएको छ । पार्टी र संसदीय समितिका सदस्यहरूले पनि अब पार्टी एकता संयोजन समितिमा बसेको बैठकले तय गर्नेछ । केन्द्रीय समितिको बैठक बुधबार बसेको बैठकले तय गरेको प्रतिवेदनमाथि सम्बोधन गर्दै उनले भने ? उक्त छलफलमा जनताले धेरै राजनीतिक दलहरू विश्वस्त छ ? यसलाई पूरा गर्दै अब दलित समुदायले दलित समुदायलाई नै विकास गर्न सक्ने योजना अगाडि आएको छ । दलित समुदायको न्यूनतम तलब ? शिक्षक ?'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(st)+' ' +' '.join(z__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f851ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['।']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39556386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['चाहन्छु',\n",
       " 'र',\n",
       " 'के',\n",
       " 'गर्ने',\n",
       " 'हो',\n",
       " 'भने',\n",
       " 'तपाईले',\n",
       " 'गर्ने',\n",
       " 'भनेको',\n",
       " 'नै',\n",
       " 'धेरै',\n",
       " 'हदसम्म',\n",
       " 'जनताले',\n",
       " 'आफ्नो',\n",
       " 'इच्छा',\n",
       " 'र',\n",
       " 'आवश्यकता',\n",
       " 'कहाँ',\n",
       " 'छ',\n",
       " '?',\n",
       " 'तपाईं',\n",
       " 'आफ्नो',\n",
       " 'पक्षमा',\n",
       " 'मत',\n",
       " 'दिएको',\n",
       " 'थाहा',\n",
       " 'भएकै',\n",
       " 'हो',\n",
       " '।',\n",
       " 'त्यसैले',\n",
       " 'अहिले',\n",
       " 'राजनीतिक',\n",
       " 'सहमति',\n",
       " 'प्राप्त',\n",
       " 'भएको',\n",
       " 'छ',\n",
       " '।',\n",
       " 'पार्टी',\n",
       " 'र',\n",
       " 'संसदीय',\n",
       " 'समितिका',\n",
       " 'सदस्यहरूले',\n",
       " 'पनि',\n",
       " 'अब',\n",
       " 'पार्टी',\n",
       " 'एकता',\n",
       " 'संयोजन',\n",
       " 'समितिमा',\n",
       " 'बसेको',\n",
       " 'बैठकले',\n",
       " 'तय',\n",
       " 'गर्नेछ',\n",
       " '।',\n",
       " 'केन्द्रीय',\n",
       " 'समितिको',\n",
       " 'बैठक',\n",
       " 'बुधबार',\n",
       " 'बसेको',\n",
       " 'बैठकले',\n",
       " 'तय',\n",
       " 'गरेको',\n",
       " 'प्रतिवेदनमाथि',\n",
       " 'सम्बोधन',\n",
       " 'गर्दै',\n",
       " 'उनले',\n",
       " 'भने',\n",
       " '?',\n",
       " 'उक्त',\n",
       " 'छलफलमा',\n",
       " 'जनताले',\n",
       " 'धेरै',\n",
       " 'राजनीतिक',\n",
       " 'दलहरू',\n",
       " 'विश्वस्त',\n",
       " 'छ',\n",
       " '?',\n",
       " 'यसलाई',\n",
       " 'पूरा',\n",
       " 'गर्दै',\n",
       " 'अब',\n",
       " 'दलित',\n",
       " 'समुदायले',\n",
       " 'दलित',\n",
       " 'समुदायलाई',\n",
       " 'नै',\n",
       " 'विकास',\n",
       " 'गर्न',\n",
       " 'सक्ने',\n",
       " 'योजना',\n",
       " 'अगाडि',\n",
       " 'आएको',\n",
       " 'छ',\n",
       " '।',\n",
       " 'दलित',\n",
       " 'समुदायको',\n",
       " 'न्यूनतम',\n",
       " 'तलब',\n",
       " '?',\n",
       " 'शिक्षक',\n",
       " '?']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf58bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
