{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# A neural probabilistic language model\n","Step 1: get the corpus and split into paragraphs, into sentence, into words\n","\n","Step 2: get the training data, i.e, certain no. of paragraphs (say 95%)\n","\n","Step 3: get the word frequency in hash_map for each words in training data\n","\n","Step 4: prepare a vocabulary. in which keep the word whose frequency is greater than some cut-off frequency\n","\n","Step 5: create word-to-index and index-to-word mapping for each words [One-Hot Encoding]\\\n","\n","Step 6: prepare the training and testing data with one-hot encoding\n","\n","Step 7: build a neural probabilistic language model\n","\n","Step 8: create training, testing and train loop and train the model\n","\n","Step 9: evaluate the model with own input\n","\n","Step 10: Use language generation metrics to evaluate the model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Import Required packages"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671411850063,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"HeJE5kRc4SCE"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/niranjan/miniconda3/envs/envpytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import multiprocessing\n","\n","import time"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n"]}],"source":["print(\"GPU is\", \"available\" if torch.cuda.is_available() else \"NOT AVAILABLE\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Dec 21 16:44:38 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n","|  0%   34C    P8    19W / 170W |    867MiB / 12045MiB |     37%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1469      G   /usr/lib/xorg/Xorg                 18MiB |\n","|    0   N/A  N/A      2162      G   /usr/bin/gnome-shell               69MiB |\n","|    0   N/A  N/A      5232      G   ...RendererForSitePerProcess      122MiB |\n","|    0   N/A  N/A      5916      G   /usr/lib/xorg/Xorg                269MiB |\n","|    0   N/A  N/A      6047      G   /usr/bin/gnome-shell               67MiB |\n","|    0   N/A  N/A      6650      G   ...AAAAAAAAA= --shared-files       35MiB |\n","|    0   N/A  N/A      7148      G   /usr/lib/firefox/firefox          274MiB |\n","|    0   N/A  N/A     30077      G   /usr/bin/nvidia-settings            2MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'1.13.1'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["torch.manual_seed(42)\n","torch.cuda.manual_seed(42)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: nvidia-sim: command not found\n"]}],"source":["!nvidia-sim"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DEVICE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Import data file corpus and explore"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671411443192,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"w_fgT5oV4aR5"},"outputs":[],"source":["oscar_dedup_file_path = \"./ne_dedup.txt\""]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":13528,"status":"ok","timestamp":1671411476348,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"WaDfuSv84p--"},"outputs":[],"source":["with open(oscar_dedup_file_path) as f:\n","  lines = f.readlines()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1671411482669,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"yk_eGbkB4u1u","outputId":"5bbe54aa-e5a3-4504-f6e9-4c65b073679e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'list'> 1490594\n"]},{"data":{"text/plain":["['सरसफाइको प्रतिक भनेर चिनिने हरियो रंङले गाउँ हरियाली बनेको छ । सुकिदह गाउँपालिका वडा नं. ३ मा करिब ७ सय घरहरु रहेका छन । ती घरमध्ये ८७ प्रतिशत घरमा हरियो रङ लगाइसकेको वडा सचिब कमलराज वलीले जानकारी दिए ।\\n',\n"," 'गाउँको फरक परिचान बनाउनको लागि यो अभियानको सुरुवात गरेको सुकिदह ३ का वडाअध्यक्ष जलेशकुमार केसीले बताए । पूर्ण सरसफाइमा सहयोग पु¥याउने उद्देश्यका साथ यो अभियानको सुरुवात गरिएको हो । गाउँका पुरै घर एउटै रङ लगाउँदा निकै सुन्दर देखिएको स्थानीयहरुले बताएका छन । उनीहरु वडापालिकामा एउटै रङ लगाउने कुरामा सहमत भएका थिए ।\\n',\n"," 'सरकारकै इशारामा चल्ने किसिमको कर्मचारी संयन्त्र बनाउन आफूहरुले छलफल गरिरहेको संकेत गर्दै उनले कर्मचारीहरु सिंहदरबारबाट प्रदेश र गाउँपालिकामा जान नमान्ने नियति छिट्टै अन्त्य हुने बताए ।\\n',\n"," 'पोखरा । नेपाल कम्युनिष्ट पार्टी (नेकपा) अध्यक्ष पुष्पकमल दाहालले कर्मचारीकै कारण सरकारले जनता सामु गरेका बाचा पुरा गर्न नसकेको बताएका छन् ।\\n',\n"," 'प्रेस सेन्टर र प्रेस चौतारी गण्डकी प्रदेशले पोखरामा बुधबार आयोजना गरेको पत्रकार सम्मेलनमा दाहालले राजनीतिक परिवर्तनका लागि दलहरु सफल भए पनि कर्मचारी संयन्त्रका कारण सरकार असफल भएको आरोप लगाए ।\\n']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["print(type(lines), len(lines))\n","lines[100:105]"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":775,"status":"ok","timestamp":1671411560895,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"7eEnyPlv4zQB","outputId":"4e5101e0-2007-4c1c-d4e4-f75a506a7b25"},"outputs":[{"data":{"text/plain":["1490594"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(lines)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671411568354,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"gYdYLC485GiR","outputId":"4caed862-cfbf-4376-8203-f275c6606114"},"outputs":[{"data":{"text/plain":["'– नियमित रुपमा कक्षाहरुमा भाग लिने, सबै गृहकार्यहरु समयमा गर्ने, नोटहरु राम्रोसँग तयार गर्ने, स्कुल कलेजको हरेक कार्यमा संलग्न हुने। एकटक पढिसकेको कुरा फेरी दोहोर्याएर पढ्नलाई पर्याप्त समय छुट्टाउनुहोस् जसकारण परिक्षाको समयमा आतिनु नपरोस। अन्तिम समयमा आएर पढ्ने बानी हटाउनुहोस्।\\n'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["lines[10000]"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1671411786109,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"yNbrSpuc5IeJ","outputId":"320c9824-5ef5-4f19-d128-5535c202164d"},"outputs":[{"name":"stdout","output_type":"stream","text":["['–', 'नियमित', 'रुपमा', 'कक्षाहरुमा', 'भाग', 'लिने,', 'सबै', 'गृहकार्यहरु', 'समयमा', 'गर्ने,', 'नोटहरु', 'राम्रोसँग', 'तयार', 'गर्ने,', 'स्कुल', 'कलेजको', 'हरेक', 'कार्यमा', 'संलग्न', 'हुने।', 'एकटक', 'पढिसकेको', 'कुरा', 'फेरी', 'दोहोर्याएर', 'पढ्नलाई', 'पर्याप्त', 'समय', 'छुट्टाउनुहोस्', 'जसकारण', 'परिक्षाको', 'समयमा', 'आतिनु', 'नपरोस।', 'अन्तिम', 'समयमा', 'आएर', 'पढ्ने', 'बानी', 'हटाउनुहोस्।']\n"]}],"source":["words = lines[10000].split()\n","print(words)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c301ed80e5ea4f2282febcf1b57030b8","9edc7bff45494ebf9eb67363a1d205f1","40cca0631be246b099ac2040409c7a6c","a8ac993552544d7ca839d9a2ef9ed010","798a3ba041ac4d4bb5807effa8617d98","ac9071277e5c472fa456716cdf1d5c12","d1accea3dc174e3bbef02f408929b3d4","7c016a590c064c5da40f59c58172e421","6a7370125cdc411cbdf3cac9459d49f2","5d6a26bf692647348344cbf1f24532ba","8f4a575277f6427d984dca5dd25db5d4"]},"executionInfo":{"elapsed":23060,"status":"ok","timestamp":1671411878992,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"ZTzpWxBg5MC8","outputId":"8edccb31-28e9-48a9-a4c0-9542b7b16f80"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1490594/1490594 [00:09<00:00, 165093.84it/s]\n"]}],"source":["oscar_paras = []\n","for sentence in tqdm(lines):\n","  words = sentence.split()\n","  oscar_paras.append(words) "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671411920417,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"vCsbekvW6GIc","outputId":"36ffe7c7-c539-474b-8bd4-62f65e725af0"},"outputs":[{"name":"stdout","output_type":"stream","text":["['–', 'नियमित', 'रुपमा', 'कक्षाहरुमा', 'भाग', 'लिने,', 'सबै', 'गृहकार्यहरु', 'समयमा', 'गर्ने,', 'नोटहरु', 'राम्रोसँग', 'तयार', 'गर्ने,', 'स्कुल', 'कलेजको', 'हरेक', 'कार्यमा', 'संलग्न', 'हुने।', 'एकटक', 'पढिसकेको', 'कुरा', 'फेरी', 'दोहोर्याएर', 'पढ्नलाई', 'पर्याप्त', 'समय', 'छुट्टाउनुहोस्', 'जसकारण', 'परिक्षाको', 'समयमा', 'आतिनु', 'नपरोस।', 'अन्तिम', 'समयमा', 'आएर', 'पढ्ने', 'बानी', 'हटाउनुहोस्।']\n"]}],"source":["print(oscar_paras[10000])"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1671412015773,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"cTPOr6X96Wkq","outputId":"3444bb3f-d24d-4f3d-ee73-4db0d8c56369"},"outputs":[{"data":{"text/plain":["1490594"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(oscar_paras)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671412052651,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"Vg4xCQ2S61kI","outputId":"f2c3dee2-aff0-427f-f35d-d5066a53817b"},"outputs":[{"data":{"text/plain":["1416064"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["train_size = int(len(oscar_paras) * 0.95)\n","train_size"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1671412359216,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"qCo_Ss_p7_az","outputId":"0975de10-34b1-46f7-921b-4ec1faf1cd2f"},"outputs":[{"data":{"text/plain":["60"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["len(oscar_paras[6993])"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":21095,"status":"ok","timestamp":1671412407461,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"hZpmC3DQ7Gyl"},"outputs":[{"name":"stderr","output_type":"stream","text":["1490594it [00:10, 149035.97it/s]\n"]}],"source":["# create the corpus from training dataset\n","train_corpus = []\n","test_corpus = []\n","for idx, sentence in tqdm(enumerate(oscar_paras)):\n","  words = []\n","  for word in sentence:\n","    words.append(word)\n","  if(idx < train_size):\n","    train_corpus.append(words)\n","  else:\n","    test_corpus.append(words)\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","True\n"]}],"source":["print(train_corpus[0] == oscar_paras[0])\n","print(test_corpus[0] == oscar_paras[len(train_corpus)])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["len(train_corpus) + len(test_corpus) == len(oscar_paras)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["634e8980ccdf4558b70fa2c8493cbfeb","2495f504600c4122baf45c8a542e9ef0","56fcc3197b094d4ba627b3f5303312e3","609cbbfdf5c44e64b8e5e0fbe264e1a8","91d14e6d018b46c3bd2aac9b5cb8c698","b14c905e48db48a599352f98ac994cbe","e34510ed0fdf403195aa7f774b4d041e","d48510d17f074319a1fcdbc48ef4797d","745a7becf1514ba9be3f65aeae8d31fe","543d7325b4594708acabcd64a8e96701","b798b3a08edc4674961683f88cd21869"]},"executionInfo":{"elapsed":38337,"status":"ok","timestamp":1671412602342,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"3ypDF_sO8W1t","outputId":"5baf2c4a-878f-4168-bb9e-030704ea19f4"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1416064/1416064 [00:15<00:00, 90939.43it/s]\n"]}],"source":["words_term_frequency_train = {}\n","for sentence in tqdm(train_corpus):\n","  for word in sentence:\n","    words_term_frequency_train[word] = words_term_frequency_train.get(word, 0) + 1"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Analyze on word frequency in the corpus"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1671412655584,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"},"user_tz":-345},"id":"AMF9dx0g8uJZ","outputId":"3ce57b04-7004-4cd9-825b-93eefa7778b0"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2130172/2130172 [00:00<00:00, 4341773.79it/s]\n"]},{"data":{"text/plain":["(2130172, 40544, 2089628)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# get all the words with frequency less than cutoff_frequency\n","cutoff_word_frequency = 100\n","exceed_cutoff_word_count = 0\n","total_words = len(words_term_frequency_train)\n","for word, freq in tqdm(words_term_frequency_train.items()):\n","    if (freq > cutoff_word_frequency):\n","        exceed_cutoff_word_count += 1\n","total_words, exceed_cutoff_word_count, total_words - exceed_cutoff_word_count\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Vocabulary Preparation"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"3VYKd4N19SIx"},"outputs":[{"data":{"text/plain":["{'<UNK>'}"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# keep all the words which have frequency higher that the cutoff_word_frequency in the vocab\n","UNK_SYMBOL = \"<UNK>\"\n","vocab = set([UNK_SYMBOL])\n","vocab"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'list'>\n","[['बर्दिबास', 'नगरपालिकाको', 'तेस्रो', 'नगर', 'परिषदबाट', 'पारित', 'आ.व.२०७३।७४', 'को', 'संशोधित', 'र', '२०७४।७५', 'को', 'प्रस्तावित', 'नीति,', 'कार्यक्रम', 'तथा', 'बजेट'], ['अार्थिक', 'वर्ष', '२०७५/७६', 'काे', 'नदिजन्य', 'पदार्थकाे', 'उत्खनन्', 'गरी', 'बिक्रि', 'वितरण', 'तथा', 'अान्तरिक', 'निकासी', 'गर्ने', 'कार्यकाे', 'बाेलपत्र', 'सम्बन्धी', 'सुचना']]\n"]}],"source":["print(type(train_corpus))\n","print(train_corpus[0:2])"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1416064/1416064 [00:16<00:00, 87101.79it/s]\n"]}],"source":["# create vocabulary\n","for sentence in tqdm(train_corpus):\n","    for word in sentence:\n","        if words_term_frequency_train.get(word, 0) >= cutoff_word_frequency:\n","            vocab.add(word)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["(40832, set, ['कीर्ति', 'फ्रान्सको', 'रहरले', 'बेजोड', 'तट'])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab), type(vocab), list(vocab)[0:5]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### One Hot Encoding and Data Loader"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Procedure\n","Step 1 : Create the batch generator for the corpus sentences using \"generate_batch\" function\n","Step 2 : Create the one-hot generator for the batch sentences using \"one-hot generator\" function\n","Step 3 : "]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# create word-to-id and id-to-word mapping\n","word_to_id_mappings = {}\n","id_to_word_mappings = {}\n","for idx, word in enumerate(vocab):\n","    word_to_id_mappings[word] = idx\n","    id_to_word_mappings[idx] = word\n","\n","# return the id of word if found in vocab else, return id for unknown word\n","def get_id_of_word(word):\n","    return word_to_id_mappings.get(word, word_to_id_mappings[UNK_SYMBOL])"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def oneHotEncoding(sentence):\n","    \"\"\"\n","    Take an input sentence and generate one hot vectors of certain context size = 3 in this particular case\n","    Arguments:\n","        sentence: any sentences\n","    \n","    Output : \n","        tensor of shape (x, 3) where 3 is the size of one hot vector and x is the number of one hot vectors\n","    \"\"\"\n","    batch_list = []\n","    for i, word in enumerate(sentence):\n","        if i+2 >= len(sentence):\n","            break\n","        context_words = [get_id_of_word(word), get_id_of_word(sentence[i + 1]), get_id_of_word(sentence[i + 2])]\n","        batch_list.append(context_words)\n","    return torch.LongTensor(batch_list)\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def concatenate_batch_corpus(batch_corpus):\n","    \"\"\" \n","    Concatenate the sentences presented in the form of list in batch_corpus\n","    Arguments:\n","        batch_corpus : list of sentences of certain batch size\n","    Output:\n","        string : concatenation of all sentences in the batch_corpus into a single sentence\n","    \"\"\"\n","    string = \"\"\n","    for sentence in batch_corpus:\n","        sen = ' '.join(sentence)\n","        sen += \" \"\n","        string += sen\n","    return string\n","    "]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# generate batch for train corpus\n","def generate_batch(corpus, batch_size=32):\n","    \"\"\" \n","    Generate batch of sentences from the corpus\n","    Arguments:\n","        corpus : list of sentences where sentences is the list of words. (Corpus is 2 dimensional list)\n","        batch_size : number of sentences to be yielded for a batch\n","    \"\"\"\n","    for i in range(0, len(corpus), batch_size):\n","        yield corpus[i:i + batch_size]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["def one_hot_generator(generator):\n","    \"\"\" \n","    Generate one hot vector for a batch of sentences from the corpus\n","    Arguments:\n","        generator : a function to generate the batch of input sentences\n","        \n","    Output : \n","        batch_one_hot : batch of one-hot vectors for the input batch of sentences\n","    \"\"\"\n","    for idx, batch in enumerate(generator):\n","        batch_concatenated_string = concatenate_batch_corpus(batch)\n","        batch_one_hot = oneHotEncoding(batch_concatenated_string)\n","        yield batch_one_hot"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[14384,  6318,  5620],\n","        [ 6318,  5620,  4888],\n","        [ 5620,  4888,  5620],\n","        ...,\n","        [ 9600, 34232, 39777],\n","        [34232, 39777,  5620],\n","        [39777,  5620,  5620]])\n","tensor([[14384,  6318],\n","        [ 6318,  5620],\n","        [ 5620,  4888],\n","        ...,\n","        [ 9600, 34232],\n","        [34232, 39777],\n","        [39777,  5620]])\n","tensor([ 5620,  4888,  5620, 14384,  9600, 12336,  5620,   722, 30190,  6318,\n","        33168,  9600, 28879,  5620, 39777,  9600, 39777,  5620,  5620,  6440,\n","        23495, 12336,  5620,  6318,  5620,  5620,   722, 30190,  6318,  5620,\n","        33168,  6318,  5620, 34984,  4888, 14384,  9600, 38683,  5620, 33168,\n","         9600,  6318,  5620,  6440,  5620, 16331, 39907, 30478, 39907, 11086,\n","         1562, 22731, 20917, 15529, 22731, 25914,  5620, 39777,  5620,  5620,\n","        12336,  4541,  4330,  5620, 22728,  5620,  6440,  5620,  6318,  5620,\n","        11086,  1562, 22731, 25914, 15529, 22731,  8424,  5620, 39777,  5620,\n","         5620, 33168,  5620,  6318, 12336,  5620,  6440,  9600, 30478,  5620,\n","         6440,  5620,   722,  5620,  6440,  5620, 33926,  5620, 39777,  9600,\n","         6318,  5620,   851, 39777,  5620,  6318,   799,  5620,  6440, 27301,\n","         9600,  5620, 14384, 39575, 23495, 38683,  5620, 39051,  9600,  6318,\n","         5620, 27301,  5620, 39777,  5620, 30478,  6318,  5620, 34984,  5620,\n","        11086,  1562, 22731,  8424,  4561, 22731, 10015,  5620, 39777,  9600,\n","        23495,  5620,   722,  4888,  5620, 39575,   722,  5620,   851,  5620,\n","        33168,  4888,  9600,  6318,  5620, 27301, 39777,  9600, 23495,  5620,\n","        15530,  6440,  5620, 21453,   722,   722,  5620,  5620, 30190,  6318,\n","         5620,  5620, 14384,  5620, 39777,  5620,  6318,  5620,  5620, 30478,\n","         5620,  6440,  6318,  1859,  5620,  6440, 27301,  9600,  5620, 39051,\n","         9600,   722,  5620,  6440,  6318,  5620, 39777,  5620,   722,  5620,\n","        39777,  9600, 12336,  5620,  5620, 30190,  6318,  5620,   722, 23495,\n","         5620, 39777,  9600,  6318,  5620,   851, 39777,  9600, 23495,  5620,\n","        14384,  9600, 23495, 28879, 33168,  6440,  5620,  6318,  5620, 12336,\n","          799,  5620, 14384,   722,  5620, 22728,  5620,  5620, 12336,  5620,\n","        30311,   722,  9600,  5620, 12336, 39777,  5620, 34984,  9600,  6318,\n","         5620, 12336, 33168,  5620,  6440,  6318,  5620,  5620, 39051, 17925,\n","         5620,   851,  9600,   722,   799,  9600,  5620, 12336, 33168,  5620,\n","         6440,  6318,  5620, 14384,  9600, 12336,  5620,  5620, 12336,   799,\n","         5620, 33168,  5620,  6318,  5620,  1859,  5620, 12336,  6318,  5620,\n","        39777,  9600,  6318, 30478,  9600, 28879,  9600, 40537,  6318,  5620,\n","        39777,  5620,  5620, 12336, 40537,   851,  5620, 30190,  5620,  6318,\n","         5620, 12336, 40537, 17925,  9600, 30190,  5620,  6440,  9600, 39777,\n","         9600,  5620,  5620, 28879,  9600, 30190,  5620,  5620, 39051,   722,\n","         5620,  6318,  9600,  5620, 22728,  5620, 16532,  5620, 15529, 34348,\n","        34348,  5620, 12336,  9600,   799,  5620,  4888,  9600,   851,  5620,\n","        39777,  5620, 39051, 22728,  5620,   851,   851,   722,  5620, 39777,\n","        23495,   722,  5620,  4888,  5620,  6318, 40537,  6318,  5620, 39777,\n","         5620,  5620,   722, 30478,  5620, 39777,  6318,  1859,  5620, 12336,\n","          799,  5620, 14384,   722,  5620, 22728,   799,  9600,  5620, 15529,\n","        34348, 34348,  5620, 39777,  9600,  5620,   799,  9600,  6383,  5620,\n","         4541, 33926,  5620,  2730, 11086,  5620, 39777,  9600,  6440,  5620,\n","        39777,  5620, 15529,  5620,  6318,  9600, 34984,  5620, 38683,  5620,\n","         6318, 33168,  6440,  5620,  5620, 30478,  5620,  4888,  5620,   851,\n","         9600,  4888, 23495, 30478,  5620,  5620, 17925,  1859,  5620,  6383,\n","         9600,  6318,  5620,  5620,   799,  5620,  6440,  5620,  6318,  6318,\n","         9600, 34984,  5620, 38683,  5620,  6318,  5620, 39777,  6440,  9600,\n","         6318, 39777,  5620,  5620, 30311,  9600,  6318,  5620,  4888,  5620,\n","        30478, 12336,  5620,   851,  5620, 11166, 33168, 30311,  9600,  6318,\n","         5620, 39777,  5620, 17925,  5620,  6318,   799,  1859,   799,  9600,\n","         5620, 16331, 39575,  5620,  6440,  5620,   851, 12336,  6440,  6318,\n","         5620,  5620,  5620, 33168,  5620,  6318, 12336,  5620, 27301,  9600,\n","          722,  5620, 30190,  6318, 23495, 39777,  5620,  5620, 16532,   722,\n","         5620,  5620, 15529,  5620,  6318,  9600, 34984,  5620, 38683,  5620,\n","         6318, 33168,  6440,  5620,  5620, 30478,  5620,  4888,  5620,   851,\n","         9600,  4888, 23495, 30478,  5620,  5620, 17925,  1859,  5620,  6383,\n","         9600,  6318,  5620,  5620, 39777,  6440,  9600,  6318, 39777,  9600,\n","         5620, 39051,   799,  5620,  6318,  5620,  4330, 23495, 21453,  5620,\n","        40537,   799,  9600,  4888,  5620, 14384,  5620,   722,  5620, 21453,\n","         9600, 28879,  5620,  4888,  9600,  5620, 39051, 28879,  5620, 27301,\n","         9600,   722,  5620, 39777,  5620,  5620,   799,  5620,  6440,  5620,\n","         6318,  5620, 33168,  5620,  6318,  5620,  1859,  5620,   722,  5620,\n","          799,   722,  5620,  6440,  5620,  6318,  1859,  9600,   799,  9600,\n","         5620, 30311,  9600,  6318,  5620,  4888,  5620, 30478, 12336,  5620,\n","          851,  5620, 11166, 33168, 30311,  9600,  6318,  5620, 39777,  5620,\n","        39777,  9600,  5620,   799,  9600,  6383,  5620,  5620, 33926,  5620,\n","        11086, 10015,  5620, 39777,  9600,  6440,  5620,  6440,  5620, 39777,\n","         5620, 15529,  5620, 12336,  6318, 39777,  9600,  6318, 28879, 23495,\n","         5620, 12336,  5620,  5620,  2182, 33926,  5620, 33168,  5620,  6318,\n","         4888, 23495,  4330,  5620,  6318,  5620, 12336,  5620, 27301,  9600,\n","          722,  5620,   851,  5620,  6440, 40537,   799,  9600,  5620, 39777,\n","         6318,  5620,   799, 30311,  9600,  6318,  5620,  5620, 12336,   799,\n","         9600,   851,  5620, 39575,   722,  5620, 30190,  6318,  5620,   722,\n","        39777,  9600,  5620, 28879,  9600, 30190,  5620,  5620, 15810, 39777,\n","         6318,  5620,   799, 30311,  9600,  6318,  5620,  5620, 12336,   799,\n","         9600,   851,  5620, 39575,   722,  5620, 39051, 22728,  5620,   851,\n","         9600,  4888, 23495,  4330, 39845, 11086,  1562, 22731,  8424, 32529,\n","         5620, 28879,  5620,   851,  9600, 15530,   722, 23495,  5620,  6440,\n","          851,  9600,  6318,  5620,  5620, 30190,  6318, 23495, 39777,  5620,\n","         5620, 16532,  5620, 15529,  5620, 12336,  6318, 39777,  9600,  6318,\n","        28879, 23495,  5620,   851, 12336, 39051,  2182,  5620,  5620, 28879,\n","         5620,   851,  9600, 34232, 39777,  5620,  5620])\n"]}],"source":["train_generator = generate_batch(train_corpus, batch_size=5)\n","train_one_hot_generator = one_hot_generator(train_generator)\n","\n","for i, one_hot in enumerate(train_one_hot_generator):\n","    print(one_hot)\n","    print(one_hot[:, 0:2])\n","    print(one_hot[:, 2])\n","    if i > -1:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["(40832, 1490594, 1416064, 74530)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab), len(oscar_paras), train_size, len(lines) - train_size"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Model Development"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["class TrigramNNmodel(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim = 300, context_size = 2):\n","    super(TrigramNNmodel, self).__init__()\n","    self.context_size = context_size\n","    self.embedding_dim = embedding_dim\n","    \n","    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","    self.linear1 = nn.Linear(context_size * embedding_dim, 124)\n","    self.linear2 = nn.Linear(124, vocab_size, bias = False)\n","\n","  def forward(self, inputs):\n","    # compute x' : concatenation of x1 and x2 embeddings\n","    embeds = self.embeddings(inputs).view((-1, self.context_size * self.embedding_dim))\n","    # compute h: tanh(W_1. x' + b)\n","    out = torch.tanh(self.linear1(embeds))\n","    # compute W_2.h\n","    out = self.linear2(out)\n","    # compute y: log_softmax(W_2.h)\n","    log_prods = F.log_softmax(out, dim = 1)\n","    # return log probabilites\n","    # log_prods : BATCH_SIZE x len(vocab)\n","    return log_prods"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model Summary"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["VOCAB_SIZE = len(vocab)\n","EMBEDDING_DIM = 300\n","CONTEXT_SIZE = 2\n","model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","      Layer (type)        Output Shape         Param #     Tr. Param #\n","=======================================================================\n","       Embedding-1       [807, 2, 300]      12,249,600      12,249,600\n","          Linear-2          [807, 124]          74,524          74,524\n","          Linear-3        [807, 40832]       5,063,168       5,063,168\n","=======================================================================\n","Total params: 17,387,292\n","Trainable params: 17,387,292\n","Non-trainable params: 0\n","-----------------------------------------------------------------------\n"]}],"source":["from pytorch_model_summary import summary\n","print(summary(model, torch.zeros(807, 2).long()))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training and evaluation loop"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def get_accuracy_from_log_probs(log_probs, labels):\n","  probs = torch.exp(log_probs)\n","  predicted_label = torch.argmax(probs, dim = 1)\n","  acc = (predicted_label == labels).float().mean()\n","  return acc"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["def evaluation_loop(model, loss_fn, one_hot_generator, device):\n","    model.eval()\n","    \n","    mean_acc, mean_loss = 0, 0\n","    count = 0\n","\n","    start_time = time.time()\n","    with torch.inference_mode():\n","        for iteration, data_tensor in enumerate(one_hot_generator):\n","            context_tensor = data_tensor[:, 0:2]\n","            target_tensor = data_tensor[:, 2]\n","            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n","            log_probs = model(context_tensor)\n","\n","            mean_loss += loss_fn(log_probs, target_tensor).item()\n","            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n","            count += 1\n","\n","            if(iteration % 500 == 0):\n","                print(f\"Evaluation iteration : {iteration} completed; Evaluation Accuracy : {mean_acc / count}; Evaluation Loss : {mean_loss / count}; Time Taken : {time.time() - start_time}\")\n","                start_time = time.time()\n","    \n","    return mean_loss / count, mean_acc / count"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["def training_loop(model, loss_fn, optimizer, one_hot_generator, device):\n","    model.train()\n","\n","    train_acc, train_loss = 0, 0\n","    count = 0\n","\n","    start_time = time.time()\n","    for iteration, data_tensor in enumerate(one_hot_generator):\n","        # get X, y data\n","        context_tensor = data_tensor[:, 0:2]\n","        target_tensor = data_tensor[:, 2]\n","        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n","\n","        # Forward propagation\n","        log_probs = model(context_tensor)\n","\n","        # calculate the loss and accuracy\n","        acc = get_accuracy_from_log_probs(log_probs, target_tensor)\n","        loss = loss_fn(log_probs, target_tensor)\n","        train_loss += loss.item()\n","        train_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n","\n","        # optimizer zero grad\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        count += 1\n","        if(iteration % 500 == 0):\n","            print(f\"Training Iteration : {iteration} completed; Training accuracy : {train_acc / count}; Training Loss : {train_loss / count}; Time Taken : {time.time() - start_time}\")\n","            start_time = time.time()\n","\n","    return train_loss / count, train_acc / count\n","        "]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["def train(model, train_one_hot_generator, test_one_hot_generator, optimizer, loss_fn, epochs, device):\n","  results = {\n","    \"train_loss\" : [],\n","    \"train_acc\" : [], \n","    \"val_loss\" : [], \n","    \"val_acc\" : []\n","  }\n","  \n","  for epoch in tqdm(range(epochs)):\n","    print(f\"\\n------------------- Epoch: {epoch+1} -------------------\\n\")\n","    train_loss, train_acc = training_loop(model = model, loss_fn=loss_fn, optimizer=optimizer, one_hot_generator=train_one_hot_generator, device = device)\n","    val_loss, val_acc = evaluation_loop(model, loss_fn=loss_fn, one_hot_generator=test_one_hot_generator, device = device)\n","    \n","    print(\n","      f\"Epoch: {epoch+1} | \"\n","      f\"train_loss: {train_loss:.4f} | \"\n","      f\"train_acc: {train_acc:.4f} | \"\n","      f\"val_loss: {val_loss:.4f} | \"\n","      f\"val_acc: {val_acc:.4f}\"\n","    )\n","\n","    results[\"train_loss\"].append(train_loss)\n","    results[\"train_acc\"].append(train_acc)\n","    results[\"val_loss\"].append(val_loss)\n","    results[\"val_acc\"].append(val_acc)\n","  \n","  return results\n"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","EPOCHS = 2\n","\n","\n","# create the generator for the train and test data\n","train_generator = generate_batch(train_corpus, batch_size=4)\n","train_one_hot_generator = one_hot_generator(train_generator)\n","\n","test_generator = generate_batch(test_corpus, batch_size=4)\n","test_one_hot_generator = one_hot_generator(test_generator)\n","\n","# create model\n","VOCAB_SIZE = len(vocab)\n","EMBEDDING_DIM = 300\n","CONTEXT_SIZE = 2\n","nplm_model = TrigramNNmodel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE).to(DEVICE)\n","\n","# optimizer and loss_fn\n","optimizer = torch.optim.Adam(params=nplm_model.parameters(), lr=0.001)\n","loss_function = nn.NLLLoss()"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","------------------- Epoch: 1 -------------------\n","\n","Training Iteration : 0 completed; Training accuracy : 0.0; Training Loss : 10.63282585144043; Time Taken : 0.023766517639160156\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2 [00:12<?, ?it/s]\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.67 GiB (GPU 0; 11.76 GiB total capacity; 6.23 GiB already allocated; 1.28 GiB free; 8.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[1;32m/media/niranjan/Volume_2/Sequencial Modeling/NPLM.ipynb Cell 55\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m train(model \u001b[39m=\u001b[39;49m nplm_model, \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                 train_one_hot_generator\u001b[39m=\u001b[39;49mtrain_one_hot_generator, \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                 test_one_hot_generator\u001b[39m=\u001b[39;49mtest_one_hot_generator, \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 optimizer\u001b[39m=\u001b[39;49moptimizer, \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 loss_fn\u001b[39m=\u001b[39;49mloss_function, \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49mEPOCHS, \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 device\u001b[39m=\u001b[39;49mDEVICE)\n","\u001b[1;32m/media/niranjan/Volume_2/Sequencial Modeling/NPLM.ipynb Cell 55\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_one_hot_generator, test_one_hot_generator, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m------------------- Epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m -------------------\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   train_loss, train_acc \u001b[39m=\u001b[39m training_loop(model \u001b[39m=\u001b[39;49m model, loss_fn\u001b[39m=\u001b[39;49mloss_fn, optimizer\u001b[39m=\u001b[39;49moptimizer, one_hot_generator\u001b[39m=\u001b[39;49mtrain_one_hot_generator, device \u001b[39m=\u001b[39;49m device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   val_loss, val_acc \u001b[39m=\u001b[39m evaluation_loop(model, loss_fn\u001b[39m=\u001b[39mloss_fn, one_hot_generator\u001b[39m=\u001b[39mtest_one_hot_generator, device \u001b[39m=\u001b[39m device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   \u001b[39mprint\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_acc: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   )\n","\u001b[1;32m/media/niranjan/Volume_2/Sequencial Modeling/NPLM.ipynb Cell 55\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, loss_fn, optimizer, one_hot_generator, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m context_tensor, target_tensor \u001b[39m=\u001b[39m context_tensor\u001b[39m.\u001b[39mto(device), target_tensor\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Forward propagation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m log_probs \u001b[39m=\u001b[39m model(context_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# calculate the loss and accuracy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m acc \u001b[39m=\u001b[39m get_accuracy_from_log_probs(log_probs, target_tensor)\n","File \u001b[0;32m~/miniconda3/envs/envpytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32m/media/niranjan/Volume_2/Sequencial Modeling/NPLM.ipynb Cell 55\u001b[0m in \u001b[0;36mTrigramNNmodel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# compute y: log_softmax(W_2.h)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m log_prods \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mlog_softmax(out, dim \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# return log probabilites\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# log_prods : BATCH_SIZE x len(vocab)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/niranjan/Volume_2/Sequencial%20Modeling/NPLM.ipynb#Y112sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m log_prods\n","File \u001b[0;32m~/miniconda3/envs/envpytorch/lib/python3.10/site-packages/torch/nn/functional.py:1930\u001b[0m, in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1928\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39mlog_softmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1929\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1930\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mlog_softmax(dim)\n\u001b[1;32m   1931\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1932\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mlog_softmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.67 GiB (GPU 0; 11.76 GiB total capacity; 6.23 GiB already allocated; 1.28 GiB free; 8.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["results = train(model = nplm_model, \n","                train_one_hot_generator=train_one_hot_generator, \n","                test_one_hot_generator=test_one_hot_generator, \n","                optimizer=optimizer, \n","                loss_fn=loss_function, \n","                epochs=EPOCHS, \n","                device=DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPWDcVVo+PpGBTVli0Mimdi","mount_file_id":"1NwTv4-DLDLVaw5fb9DUUDubmaxvXXYyv","provenance":[]},"kernelspec":{"display_name":"envpytorch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"},"vscode":{"interpreter":{"hash":"5b3fd3eb8806dba3462d2f8dec7ef26f40efa19809f5ce83e35f2c38654d96f6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"2495f504600c4122baf45c8a542e9ef0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b14c905e48db48a599352f98ac994cbe","placeholder":"​","style":"IPY_MODEL_e34510ed0fdf403195aa7f774b4d041e","value":"100%"}},"40cca0631be246b099ac2040409c7a6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c016a590c064c5da40f59c58172e421","max":1490594,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a7370125cdc411cbdf3cac9459d49f2","value":1490594}},"543d7325b4594708acabcd64a8e96701":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56fcc3197b094d4ba627b3f5303312e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d48510d17f074319a1fcdbc48ef4797d","max":1416064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_745a7becf1514ba9be3f65aeae8d31fe","value":1416064}},"5d6a26bf692647348344cbf1f24532ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"609cbbfdf5c44e64b8e5e0fbe264e1a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_543d7325b4594708acabcd64a8e96701","placeholder":"​","style":"IPY_MODEL_b798b3a08edc4674961683f88cd21869","value":" 1416064/1416064 [00:38&lt;00:00, 37305.56it/s]"}},"634e8980ccdf4558b70fa2c8493cbfeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2495f504600c4122baf45c8a542e9ef0","IPY_MODEL_56fcc3197b094d4ba627b3f5303312e3","IPY_MODEL_609cbbfdf5c44e64b8e5e0fbe264e1a8"],"layout":"IPY_MODEL_91d14e6d018b46c3bd2aac9b5cb8c698"}},"6a7370125cdc411cbdf3cac9459d49f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"745a7becf1514ba9be3f65aeae8d31fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"798a3ba041ac4d4bb5807effa8617d98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c016a590c064c5da40f59c58172e421":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f4a575277f6427d984dca5dd25db5d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91d14e6d018b46c3bd2aac9b5cb8c698":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9edc7bff45494ebf9eb67363a1d205f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac9071277e5c472fa456716cdf1d5c12","placeholder":"​","style":"IPY_MODEL_d1accea3dc174e3bbef02f408929b3d4","value":"100%"}},"a8ac993552544d7ca839d9a2ef9ed010":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d6a26bf692647348344cbf1f24532ba","placeholder":"​","style":"IPY_MODEL_8f4a575277f6427d984dca5dd25db5d4","value":" 1490594/1490594 [00:22&lt;00:00, 90964.27it/s]"}},"ac9071277e5c472fa456716cdf1d5c12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b14c905e48db48a599352f98ac994cbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b798b3a08edc4674961683f88cd21869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c301ed80e5ea4f2282febcf1b57030b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9edc7bff45494ebf9eb67363a1d205f1","IPY_MODEL_40cca0631be246b099ac2040409c7a6c","IPY_MODEL_a8ac993552544d7ca839d9a2ef9ed010"],"layout":"IPY_MODEL_798a3ba041ac4d4bb5807effa8617d98"}},"d1accea3dc174e3bbef02f408929b3d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d48510d17f074319a1fcdbc48ef4797d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e34510ed0fdf403195aa7f774b4d041e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
