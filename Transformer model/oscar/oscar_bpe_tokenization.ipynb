{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"16qskPBMkfdCHCr-wmyMjopXK9sQHAc7_","authorship_tag":"ABX9TyPjV3TtlMUo1fPQrLEtVFzr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKJjt-_Ry5CJ","executionInfo":{"status":"ok","timestamp":1672367285937,"user_tz":-345,"elapsed":29,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}},"outputId":"161252eb-d4b9-49ec-ba5e-6c19db5c9ad5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"code","source":["!pip install tokenizers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m5zVQN55pKGo","executionInfo":{"status":"ok","timestamp":1672367292370,"user_tz":-345,"elapsed":6445,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}},"outputId":"6a400104-658b-4fcf-a13d-45a1f17ddada"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tokenizers\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.13.2\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IkBfQvxmpBB7","executionInfo":{"status":"ok","timestamp":1672367293303,"user_tz":-345,"elapsed":940,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}}},"outputs":[],"source":["import os\n","from tokenizers.models import BPE\n","from tokenizers import Tokenizer\n","from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n","from tokenizers.normalizers import NFKC, Sequence\n","from tokenizers.pre_tokenizers import ByteLevel\n","from tokenizers.trainers import BpeTrainer\n"]},{"cell_type":"code","source":["class BPE_token(object):\n","    def __init__(self):\n","        self.tokenizer = Tokenizer(BPE())\n","        self.tokenizer.normalizer = Sequence([\n","            NFKC()\n","        ])\n","        self.tokenizer.pre_tokenizer = ByteLevel()\n","        self.tokenizer.decoder = ByteLevelDecoder()\n","        self.vocab_size = 50000\n","\n","    def bpe_train(self, paths):\n","        trainer = BpeTrainer(vocab_size=self.vocab_size, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n","            \"<s>\",\n","            \"<pad>\",\n","            \"</s>\",\n","            \"<unk>\",\n","            \"<mask>\"\n","        ])\n","        self.tokenizer.train(files = paths, trainer = trainer)\n","\n","    def save_tokenizer(self, location, prefix=None):\n","        if not os.path.exists(location):\n","            os.makedirs(location)\n","        self.tokenizer.model.save(location, prefix)"],"metadata":{"id":"tKWINsxUpE-0","executionInfo":{"status":"ok","timestamp":1672367293303,"user_tz":-345,"elapsed":6,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["oscar_data_path = \"/content/drive/MyDrive/Colab Notebooks/NLP/Language Modelling/Datasets/Oscar_nepali_dataset\""],"metadata":{"id":"idgTfT5upN88","executionInfo":{"status":"ok","timestamp":1672367293304,"user_tz":-345,"elapsed":5,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path \n","paths = [str(x) for x in Path(oscar_data_path).glob(\"*.txt\")]"],"metadata":{"id":"unNi9H94pHNb","executionInfo":{"status":"ok","timestamp":1672367294013,"user_tz":-345,"elapsed":713,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["paths"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8oKx77R9pUuh","executionInfo":{"status":"ok","timestamp":1672367294016,"user_tz":-345,"elapsed":27,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}},"outputId":"437f1c1a-d9b2-4452-e1e2-af243fd32ca3"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/NLP/Language Modelling/Datasets/Oscar_nepali_dataset/ne_dedup.txt']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["tokenizer = BPE_token()# train the tokenizer model\n","tokenizer.bpe_train(paths)"],"metadata":{"id":"VpSw1KW6pYAO","executionInfo":{"status":"ok","timestamp":1672367727550,"user_tz":-345,"elapsed":433554,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["save_path = '/content/drive/MyDrive/Colab Notebooks/NLP/Transformer Models/oscar_gpt2/Tokenizer'\n","tokenizer.save_tokenizer(save_path)"],"metadata":{"id":"o7QaMxDopmNs","executionInfo":{"status":"ok","timestamp":1672367727552,"user_tz":-345,"elapsed":6,"user":{"displayName":"Nirajan Bekoju","userId":"14852213716960576742"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JnbKapQ9p0W1"},"execution_count":null,"outputs":[]}]}